{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d44e76",
   "metadata": {},
   "source": [
    "# Redes Neuronales - TP2\n",
    "## Ej 3\n",
    "\n",
    "Implemente un perceptrón multicapa que aprenda la función lógica XOR de 2 y de 4\n",
    "entradas (utilizando el algoritmo Backpropagation y actualizando en batch). Muestre\n",
    "cómo evoluciona el error durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815fd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee833391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron_multicapa:\n",
    "    def __init__(self, capas, dim_entrada):\n",
    "        \"\"\"\n",
    "        capas: lista con la cantidad de perceptrones por capa (ej: [3,2,1])\n",
    "        dim_entrada: tamaño del vector de entrada (sin bias)\n",
    "        \"\"\"\n",
    "        self.capas = capas\n",
    "        self.dim_entrada = dim_entrada\n",
    "        self.lista_matrices = []\n",
    "        self.lr = None\n",
    "        \n",
    "        # Inicializar matrices de pesos para cada capa\n",
    "        entrada_anterior = dim_entrada\n",
    "        for num_perceptrones in capas:\n",
    "            # Cada matriz tiene shape (num_perceptrones, entrada_anterior + 1) (+1 por bias)\n",
    "            matriz_pesos = np.random.uniform(-1, 1, size=(num_perceptrones, entrada_anterior + 1)) # arranca con pesos aleatorios\n",
    "            self.lista_matrices.append(matriz_pesos)\n",
    "            entrada_anterior = num_perceptrones\n",
    "\n",
    "    def funcion_activacion(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) # función de activación tipo sigmoide\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante\n",
    "        x: vector de entrada (sin bias)\n",
    "        \"\"\"\n",
    "        a = np.concatenate(([1], x))  # Añadir bias a la entrada\n",
    "        for W in self.lista_matrices:  # recorremos todas las matrices de pesos\n",
    "            z = np.dot(W, a)\n",
    "            a = self.funcion_activacion(z)\n",
    "            a = np.concatenate(([1], a))  # Añadir bias para la siguiente capa\n",
    "        return a[1:]  # Devolver salida sin bias\n",
    "    \n",
    "    def predecir(self, X):\n",
    "        \"\"\"\n",
    "        X: matriz de entradas (cada fila es un vector de entrada)\n",
    "        \"\"\"\n",
    "        return np.array([self.forward(x) for x in X])\n",
    "    \n",
    "    def entrenar(self, X, Y, lr=0.01, epochs=1000):\n",
    "        \"\"\"\n",
    "        Entrenamiento del perceptrón multicapa usando backpropagation\n",
    "        X: matriz de entradas (cada fila es un vector de entrada)\n",
    "        Y: matriz de salidas esperadas (cada fila es un vector de salida)\n",
    "        lr: tasa de aprendizaje\n",
    "        epochs: cantidad de iteraciones sobre el conjunto de datos\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(n_samples):\n",
    "                # tomamos las \n",
    "                x = X[i]\n",
    "                y = Y[i]\n",
    "                \n",
    "                # Forward pass\n",
    "                activations = [np.concatenate(([1], x))]  # Lista de activaciones por capa (con bias)\n",
    "                for W in self.lista_matrices:\n",
    "                    z = np.dot(W, activations[-1])\n",
    "                    a = self.funcion_activacion(z)\n",
    "                    activations.append(np.concatenate(([1], a)))  # Añadir bias\n",
    "                \n",
    "                # Backward pass\n",
    "                delta = activations[-1][1:] - y  # Error en la salida (sin bias)\n",
    "                for layer in reversed(range(len(self.lista_matrices))):\n",
    "                    a_prev = activations[layer]\n",
    "                    W = self.lista_matrices[layer]\n",
    "                    \n",
    "                    # Gradiente\n",
    "                    grad = np.outer(delta, a_prev)\n",
    "                    \n",
    "                    # Actualizar pesos\n",
    "                    self.lista_matrices[layer] -= self.lr * grad\n",
    "                    \n",
    "                    if layer > 0:\n",
    "                        # Calcular delta para la capa anterior (sin bias)\n",
    "                        delta = np.dot(W[:, 1:].T, delta) * activations[layer][1:] * (1 - activations[layer][1:])\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.predecir(X) - Y) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa43828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos para la XOR de 2 entradas y 1 salida\n",
    "A = np.array([-1,1,-1,1])\n",
    "B = np.array([-1,-1,1,1])\n",
    "Y1= np.array([-1,1,-1,1])\n",
    "datos_XOR2 = np.column_stack((A, B, Y1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6c7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([-1,1,-1,1,-1,1,-1,1,-1,1,-1,1,-1,1,-1,1])\n",
    "B = np.array([-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1])\n",
    "C = np.array([-1,-1,-1,-1,1,1,1,1,-1,-1,-1,-1,1,1,1,1])\n",
    "D = np.array([-1,-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1,1,1,1])\n",
    "Y1 = np.where(np.logical_xor.reduce([A == 1, B == 1, C == 1, D == 1]), 1, -1)\n",
    "datos_XOR4 = np.column_stack((A, B, C, D, Y1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4589204984767417\n",
      "Epoch 100, Loss: 0.6130755149426634\n",
      "Epoch 200, Loss: 0.612770292018128\n",
      "Epoch 300, Loss: 0.6127963972311867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400, Loss: 0.6128074705392885\n",
      "Epoch 500, Loss: 0.6128127950024782\n",
      "Epoch 600, Loss: 0.6128157202559077\n",
      "Epoch 700, Loss: 0.6128174905894604\n",
      "Epoch 800, Loss: 0.6128186407184384\n",
      "Epoch 900, Loss: 0.6128194291699646\n",
      "Epoch 700, Loss: 0.6128174905894604\n",
      "Epoch 800, Loss: 0.6128186407184384\n",
      "Epoch 900, Loss: 0.6128194291699646\n",
      "Predicciones para XOR 2 entradas:\n",
      "Predicciones para XOR 2 entradas:\n"
     ]
    }
   ],
   "source": [
    "# ahora toca entrenar el perceptron con estos datos\n",
    "test2 = perceptron_multicapa(capas=[2, 1], dim_entrada=2) # para la XOR de 2 entradas y 1 salida\n",
    "test4 = perceptron_multicapa(capas=[4, 1], dim_entrada=4) # para la XOR de 4 entradas y 1 salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc905d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
