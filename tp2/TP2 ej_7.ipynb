{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bca683",
   "metadata": {},
   "source": [
    "Entrene un autoencoder para obtener una representación de baja dimensionalidad de las imágenes de MNIST. Use dichas representaciones para entrenar un perceptrón multicapa como clasificador. ¿Cuál es el tiempo de entrenamiento y la exactitud del clasificador obtenido cuando parte de la representación del autoencoder, en comparación con lo obtenido usando las imágenes originales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7873a2c",
   "metadata": {},
   "source": [
    "En este caso se nos permitió usar Pytorch para la parte de código. La idea es definir la estructura general de la red y luego ir modificandola hasta lograr la mejor performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b65811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Elegir dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Transformación\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Dataset MNIST\n",
    "train_dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# minibatch\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04edbe43",
   "metadata": {},
   "source": [
    "Primero una función que entrena y grafica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd18e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader,\n",
    "                       criterion, optimizer, device,\n",
    "                       num_epochs=50, patience=5, min_delta=1e-4):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"accuracy\": []}\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(model)\n",
    "    print(f\"Parámetros entrenables: {total_params:,}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # Evaluación\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x,y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                test_loss += criterion(outputs, y).item()\n",
    "                _, pred = outputs.max(1)\n",
    "                total += y.size(0)\n",
    "                correct += pred.eq(y).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"accuracy\"].append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss {train_loss:.4f} | \"\n",
    "              f\"Test Loss {test_loss:.4f} | Acc {accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if test_loss < best_loss - min_delta:\n",
    "            best_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"✔ Early stopping activado\")\n",
    "            break\n",
    "\n",
    "    # Gráficos\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train\")\n",
    "    plt.plot(history[\"test_loss\"], label=\"Test\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history[\"accuracy\"], label=\"Accuracy\")\n",
    "    plt.title(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b9484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            x_flat = x.view(x.size(0), -1)     #  aplanar\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_flat)\n",
    "            loss = criterion(outputs, x_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # evaluación\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, _ in test_loader:\n",
    "                x = x.to(device)\n",
    "                x_flat = x.view(x.size(0), -1)\n",
    "                outputs = model(x_flat)\n",
    "                loss = criterion(outputs, x_flat)\n",
    "                test_loss += loss.item()\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa81cc",
   "metadata": {},
   "source": [
    "La primer estructura que quiero hacer es el autoencoder. Vo a elegir una arquitectura simétrica con una reducción del 80% de tamaño de dimensión de entrada, de acuerdo al principio de Pareto, que dice que el 80% de las cosas son causadas por el 20%. Esto lo extrapolo a energía y varianza y listo. \n",
    "\n",
    "Entramos con $28*28$, luego a $28*28*0.5 $y luego a $0.2*28*28$. De ahí sube de vuelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6512c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 392)\n",
    "        self.fc2 = nn.Linear(392, 156)\n",
    "        self.fc3 = nn.Linear(156, 392)\n",
    "        self.fc4 = nn.Linear(392, 28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "model_cnn = Autoencoder().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce4dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 0.1440, Test Loss: 0.0967\n",
      "Epoch [2/100] - Train Loss: 0.0901, Test Loss: 0.0837\n",
      "Epoch [3/100] - Train Loss: 0.0818, Test Loss: 0.0788\n",
      "Epoch [4/100] - Train Loss: 0.0779, Test Loss: 0.0762\n",
      "Epoch [5/100] - Train Loss: 0.0755, Test Loss: 0.0742\n",
      "Epoch [6/100] - Train Loss: 0.0738, Test Loss: 0.0734\n",
      "Epoch [7/100] - Train Loss: 0.0725, Test Loss: 0.0718\n",
      "Epoch [8/100] - Train Loss: 0.0715, Test Loss: 0.0711\n",
      "Epoch [9/100] - Train Loss: 0.0708, Test Loss: 0.0705\n",
      "Epoch [10/100] - Train Loss: 0.0702, Test Loss: 0.0700\n",
      "Epoch [11/100] - Train Loss: 0.0696, Test Loss: 0.0693\n",
      "Epoch [12/100] - Train Loss: 0.0692, Test Loss: 0.0689\n",
      "Epoch [13/100] - Train Loss: 0.0688, Test Loss: 0.0687\n",
      "Epoch [14/100] - Train Loss: 0.0685, Test Loss: 0.0685\n",
      "Epoch [15/100] - Train Loss: 0.0682, Test Loss: 0.0683\n"
     ]
    }
   ],
   "source": [
    "# -------- Entrenamiento --------\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "train_losses, test_losses = train_autoencoder(\n",
    "    model_cnn, train_loader, test_loader,\n",
    "    criterion, optimizer, device,\n",
    "    num_epochs=100\n",
    ")\n",
    "\n",
    "\n",
    "# -------- Gráficos --------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Pérdida del Autoencoder')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------- Visualizar reconstrucciones --------\n",
    "model_cnn.eval()\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        x = x.to(device)\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        outputs = model_cnn(x_flat)\n",
    "        break  # solo un batch\n",
    "\n",
    "# convertir a imágenes\n",
    "x = x.cpu().view(-1, 1, 28, 28)\n",
    "outputs = outputs.cpu().view(-1, 1, 28, 28)\n",
    "\n",
    "# mostrar algunas imágenes\n",
    "n = 8\n",
    "plt.figure(figsize=(15,4))\n",
    "for i in range(n):\n",
    "    # original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x[i][0], cmap=\"gray\")\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # reconstruida\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(outputs[i][0], cmap=\"gray\")\n",
    "    plt.title(\"Reconstruida\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382de07a",
   "metadata": {},
   "source": [
    "1570 parámetros no congelados, la capa de 156 a 10 + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aff970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Loss=0.6624 | Acc=81.28%\n",
      "Epoch 2/30 | Loss=0.3537 | Acc=89.86%\n",
      "Epoch 3/30 | Loss=0.3200 | Acc=90.68%\n",
      "Epoch 4/30 | Loss=0.3073 | Acc=90.92%\n",
      "Epoch 5/30 | Loss=0.2996 | Acc=91.19%\n",
      "Epoch 6/30 | Loss=0.2962 | Acc=91.22%\n",
      "Epoch 7/30 | Loss=0.2921 | Acc=91.29%\n",
      "Epoch 8/30 | Loss=0.2884 | Acc=91.44%\n",
      "Epoch 9/30 | Loss=0.2884 | Acc=91.51%\n",
      "Epoch 10/30 | Loss=0.2855 | Acc=91.53%\n",
      "Epoch 11/30 | Loss=0.2832 | Acc=91.62%\n",
      "Epoch 12/30 | Loss=0.2837 | Acc=91.56%\n",
      "Epoch 13/30 | Loss=0.2823 | Acc=91.63%\n",
      "Epoch 14/30 | Loss=0.2813 | Acc=91.79%\n",
      "Epoch 15/30 | Loss=0.2810 | Acc=91.69%\n",
      "Epoch 16/30 | Loss=0.2799 | Acc=91.83%\n",
      "Epoch 17/30 | Loss=0.2785 | Acc=91.79%\n",
      "Epoch 18/30 | Loss=0.2773 | Acc=91.82%\n",
      "Epoch 19/30 | Loss=0.2772 | Acc=91.85%\n",
      "Epoch 20/30 | Loss=0.2775 | Acc=91.76%\n",
      "Epoch 21/30 | Loss=0.2751 | Acc=91.91%\n",
      "Epoch 22/30 | Loss=0.2738 | Acc=91.90%\n",
      "Epoch 23/30 | Loss=0.2745 | Acc=91.89%\n",
      "Epoch 24/30 | Loss=0.2738 | Acc=91.94%\n",
      "Epoch 25/30 | Loss=0.2735 | Acc=91.97%\n",
      "Epoch 26/30 | Loss=0.2731 | Acc=92.01%\n",
      "Epoch 27/30 | Loss=0.2727 | Acc=91.92%\n",
      "Epoch 28/30 | Loss=0.2736 | Acc=91.98%\n",
      "Epoch 29/30 | Loss=0.2726 | Acc=91.98%\n",
      "Epoch 30/30 | Loss=0.2720 | Acc=92.05%\n",
      "Test Accuracy: 92.33%\n"
     ]
    }
   ],
   "source": [
    "class ClassifierFromEncoder(nn.Module):\n",
    "    def __init__(self, autoencoder, num_classes=10):\n",
    "        super().__init__()\n",
    "        # acá tomamos las capas del otro modelo y las congelamos\n",
    "        self.encoder_fc1 = autoencoder.fc1\n",
    "        self.encoder_fc2 = autoencoder.fc2\n",
    "        for param in self.encoder_fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.encoder_fc2.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = nn.Linear(156, num_classes) # lo más simple posible\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        with torch.no_grad():\n",
    "            x = F.relu(self.encoder_fc1(x))\n",
    "            encoded = F.relu(self.encoder_fc2(x))\n",
    "        logits = self.classifier(encoded)\n",
    "        return logits\n",
    "\n",
    "classifier_model = ClassifierFromEncoder(model_cnn).to(device)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.Adam(classifier_model.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# ------------------------\n",
    "# Entrenamiento Clasificador\n",
    "# ------------------------\n",
    "num_epochs_cls = 100\n",
    "for epoch in range(num_epochs_cls):\n",
    "    classifier_model.train()\n",
    "    running_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer_cls.zero_grad()\n",
    "        outputs = classifier_model(x)\n",
    "        loss = criterion_cls(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "        running_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_cls} | Loss={running_loss/len(train_loader):.4f} | Acc={train_acc:.2f}%\")\n",
    "\n",
    "# ------------------------\n",
    "# Evaluación en Test\n",
    "# ------------------------\n",
    "classifier_model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = classifier_model(x)\n",
    "        _, pred = outputs.max(1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Test Accuracy: {100 * correct/total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
