{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bca683",
   "metadata": {},
   "source": [
    "Entrene una red convolucional para clasificar las imágenes de la base de datos MNIST.\n",
    "\n",
    "¿Cuál es la red convolucional más pequeña que puede conseguir con una exactitud de al menos 90% en el conjunto de evaluación? ¿Cuál es el perceptrón multicapa más pequeño que puede conseguir con la misma exactitud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3b0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7873a2c",
   "metadata": {},
   "source": [
    "En este caso se nos permitió usar Pytorch para la parte de código. La idea es definir la estructura general de la red y luego ir modificandola hasta lograr la mejor performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b65811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert the data to tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa81cc",
   "metadata": {},
   "source": [
    "La primer estructura que quiero hacer es algo muy simple, una red convolucional pegada a una fully connected, bien simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6512c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) # son 6 filtros de 5x5, pero bienen en escala de grises\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # esto reduce la dimensión espacial a la mitad\n",
    "        self.flatten = nn.Flatten() # esto aplana la entrada para la capa lineal\n",
    "        self.linear_relu_stack = nn.Sequential( # solo 1 capa lineal, mínimo posible\n",
    "            nn.Linear(864, 10), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv → ReLU → pool\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ce47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # minibatch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe0be2",
   "metadata": {},
   "source": [
    "El código de abajo entrena hasta un máximo de 20 epochs, con early stippong (si converge la loss). Entrena y testea por cada epoch. también grafica todo para poder entender como varia con epoch, la idea es que podemos comparar la velocidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "661d0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, criterion= nn.CrossEntropyLoss(), optimizer, device,\n",
    "                       num_epochs=100, patience=10, min_delta=1e-4, plot=True):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo PyTorch con early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: instancia de nn.Module\n",
    "        train_loader: DataLoader de entrenamiento\n",
    "        test_loader: DataLoader de validación/prueba\n",
    "        criterion: función de pérdida (ej: nn.CrossEntropyLoss())\n",
    "        optimizer: optimizador (ej: optim.Adam(model.parameters(), lr=0.001))\n",
    "        device: 'cpu' o 'cuda'\n",
    "        num_epochs: cantidad máxima de épocas\n",
    "        patience: cuántas épocas esperar sin mejora\n",
    "        min_delta: mejora mínima en test loss para resetear early stopping\n",
    "        plot: si True, grafica loss y accuracy\n",
    "\n",
    "    Returns:\n",
    "        history: diccionario con 'train_loss', 'test_loss', 'accuracy'\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"El modelo tiene {total_params:,} parámetros entrenables.\")\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"accuracy\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "        # --- Evaluación ---\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        acc = 100 * correct / total\n",
    "\n",
    "        history[\"test_loss\"].append(avg_test_loss)\n",
    "        history[\"accuracy\"].append(acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Test Loss: {avg_test_loss:.4f} | \"\n",
    "              f\"Acc: {acc:.2f}%\")\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        if avg_test_loss < best_loss - min_delta:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping por falta de mejora.\")\n",
    "            break\n",
    "\n",
    "    # --- Graficar ---\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evolución del Loss\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history[\"accuracy\"], label=\"Accuracy\", color=\"green\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evolución de la Accuracy\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ca637",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "history = train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device,\n",
    "    num_epochs=50\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04073281",
   "metadata": {},
   "source": [
    "ahora creo otros modelos y vemos que pasa. la idea es ir mejorando para llegar a la performance de 90%. primero un modelo con más capa de perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5db4fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) # son 6 filtros de 5x5, duplicamos la cantidad de kernels diferentes. -> 26x26x6\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # esto reduce la dimensión espacial a la mitad -> 12x12x6\n",
    "        self.flatten = nn.Flatten() # esto aplana la entrada para la capa lineal\n",
    "        # After conv (28->24) and pool (24->12), the feature map size is 12x12 with 6 channels -> 12*12*6 = 864\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(12 * 12 * 6, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv → ReLU → pool\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93816cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Train Loss: 1.4471 | Test Loss: 1.2297 | Acc: 49.10%\n",
      "Epoch [2/50] | Train Loss: 1.1901 | Test Loss: 1.1814 | Acc: 49.59%\n",
      "Epoch [3/50] | Train Loss: 1.1771 | Test Loss: 1.1879 | Acc: 49.46%\n",
      "Epoch [4/50] | Train Loss: 1.1738 | Test Loss: 1.1922 | Acc: 49.56%\n",
      "Epoch [5/50] | Train Loss: 1.1672 | Test Loss: 1.1910 | Acc: 49.37%\n",
      "Epoch [6/50] | Train Loss: 1.1703 | Test Loss: 1.2024 | Acc: 49.48%\n",
      "Epoch [7/50] | Train Loss: 1.1701 | Test Loss: 1.2098 | Acc: 49.38%\n",
      "Epoch [8/50] | Train Loss: 1.1703 | Test Loss: 1.1947 | Acc: 49.46%\n",
      "Epoch [9/50] | Train Loss: 1.1691 | Test Loss: 1.2162 | Acc: 49.27%\n",
      "Epoch [10/50] | Train Loss: 1.1644 | Test Loss: 1.2136 | Acc: 49.29%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "history = train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device,\n",
    "    num_epochs=50\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) # son 6 filtros de 5x5, pero bienen en escala de grises\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # esto reduce la dimensión espacial a la mitad\n",
    "        self.flatten = nn.Flatten() # esto aplana la entrada para la capa lineal\n",
    "        self.linear_relu_stack = nn.Sequential( # solo 1 capa lineal, mínimo posible\n",
    "            nn.Linear(864, 10), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv → ReLU → pool\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308a033",
   "metadata": {},
   "source": [
    "Ahora comparamos con el perceptrón muilticapa más chico que encontremos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
