{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d44e76",
   "metadata": {},
   "source": [
    "# Redes Neuronales - TP2\n",
    "## Ej 4\n",
    "- a - Implemente una red con aprendizaje Backpropagation que aprenda la siguiente\n",
    "función:\n",
    "\n",
    "$$f (x , y , z)=sin(x)+cos(y)+z$$\n",
    "\n",
    "donde: x e y ∊ [0,2 π] y z ∊[−1,1]. Para ello construya un conjunto de datos de entrenamiento y un conjunto de evaluación. Muestre la evolución del error  e\n",
    "entrenamiento y de evaluación en función de las épocas de entrenamiento.\n",
    "\n",
    "- b - Estudie la evolución de los errores durante el entrenamiento de una red con una capa oculta de 30 neuronas cuando el conjunto de entrenamiento contiene 40 muestras. ¿Que ocurre si el minibatch tiene tamaño 40? ¿Y si tiene tamaño 1?\n",
    "\n",
    "Acá la consigna lo lo explicita pero hay que aplicar algo de minibatch al lagoritmo de aprendizaje.\n",
    "\n",
    "Robando de un post de stackExchange:\n",
    "\n",
    "batch gradient descent you process the entire training set in one iteration. Whereas, in a mini-batch gradient descent you process a small subset of the training set in each iteration.\n",
    "\n",
    "Also compare stochastic gradient descent, where you process a single example from the training set in each iteration.\n",
    "\n",
    "Another way to look at it: they are all examples of the same approach to gradient descent with a batch size of m and a training set of size n. For stochastic gradient descent, m=1. For batch gradient descent, m = n. For mini-batch, m=b and b < n, typically b is small compared to n.\n",
    "\n",
    "Mini-batch adds the question of determining the right size for b, but finding the right b may greatly improve your results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d886a6",
   "metadata": {},
   "source": [
    "Dicho todo esto, la idea es tomar el set de datos y generar batches de entrenamiento. Es otro loop for y una partición de los datos de entrada. hay 2 hiperparams -> la # de minibatches y la # de veces que hacemos el proceso. Para mantener la cantidad de muestras balanceadas, solo se le pueden mostrar 1 vez por batch de minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "815fd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e54023",
   "metadata": {},
   "source": [
    "Vamos a generar las muestras de la función. Voy a barrer X, Y de 0 a 2pi para Z=+-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e28250",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,num=200)*2*np.pi\n",
    "y = x # hasta ahora tenemos los ejes\n",
    "z = [-1,1]\n",
    "f = []\n",
    "tripletas_xyz = []\n",
    "\n",
    "for k in z:\n",
    "    for i in x:\n",
    "        for j in y:\n",
    "            tripletas_xyz.append([i,j,k])\n",
    "\n",
    "    \n",
    "# este es el dataset entero, es gigante. \n",
    "\n",
    "# -> lo pasamos a matriz\n",
    "\n",
    "dataset = np.asmatrix(tripletas_xyz) # listo. \n",
    "\n",
    "# no lo shuffleo acá, es lo va a hacer el entrenador\n",
    "# voy a generar el f\n",
    "\n",
    "f = np.sin(dataset[:,0])+np.cos(dataset[:,1])+dataset[:,2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987957fa",
   "metadata": {},
   "source": [
    "Ahora toca shufflear las filas y hacer dataset de entrenamiento y de testeo. Por comodidad lo hago con sklearn, el resto si será a mano. Como tengo 80mil registros, y quiero batches de números enteros, voy a separar esto en una proporción 70% 30%, que me parece razonable. Luego, los minibatches serán de números como 1,10,100,1000,2000,4000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf330bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(  dataset, f, train_size = 56000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee833391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron_multicapa:\n",
    "    def __init__(self, capas, dim_entrada):\n",
    "        self.capas = capas\n",
    "        self.dim_entrada = dim_entrada\n",
    "        self.lista_matrices = []\n",
    "        self.lr = None\n",
    "        entrada_anterior = dim_entrada\n",
    "        for num_perceptrones in capas:\n",
    "            matriz_pesos = np.random.uniform(-2, 2, size=(num_perceptrones, entrada_anterior + 1)) # inicialización más amplia\n",
    "            self.lista_matrices.append(matriz_pesos)\n",
    "            entrada_anterior = num_perceptrones\n",
    "\n",
    "    def funcion_activacion(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def derivada_activacion(self, a):\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        activaciones = [a] # todos los x de la red cuando se propaga para adelante - es el zs pasado por la función de activación\n",
    "        zs = []# todos los h de la red cuando se propaga para adelante\n",
    "        for W in self.lista_matrices:\n",
    "            a_b = np.concatenate(([1], a))\n",
    "            z = np.dot(W, a_b)\n",
    "            zs.append(z)\n",
    "            a = self.funcion_activacion(z)\n",
    "            activaciones.append(a)\n",
    "        return activaciones, zs\n",
    "    \n",
    "    def predecir(self, X):\n",
    "        salidas = []\n",
    "        for x in X:\n",
    "            a = x\n",
    "            for W in self.lista_matrices:\n",
    "                a = np.concatenate(([1], a))\n",
    "                a = self.funcion_activacion(np.dot(W, a))\n",
    "            salidas.append(a.reshape(-1))  # garantiza que sea vector 1D\n",
    "        return np.array(salidas).reshape(len(X), -1)\n",
    "\n",
    "    def predecir_hard(self, X):\n",
    "        salidas = self.predecir(X)\n",
    "        # Si la salida es entre 0 y 1 (sigmoide), umbral en 0.5\n",
    "        return (salidas >= 0.5).astype(int)\n",
    "\n",
    "    def entrenar(self, X, Y, lr=1.0, epochs=100):\n",
    "        self.lr = lr\n",
    "        n_samples = X.shape[0]\n",
    "        self.error_por_epoch = []\n",
    "        for epoch in range(epochs):\n",
    "            grad_acum = [np.zeros_like(W) for W in self.lista_matrices]\n",
    "            for i in range(n_samples):\n",
    "                x = X[i]\n",
    "                y = Y[i]\n",
    "                activaciones, zs = self.forward(x) # tomo los h y x del paso foward\n",
    "                deltas = [None] * len(self.lista_matrices) # lugar para guardar los deltas\n",
    "                a_out = activaciones[-1] # la activavión de la salida es la última\n",
    "                deltas[-1] = (a_out - y) * self.derivada_activacion(a_out) # el último delta es el de la salida, y se calcula como la salida deseada - la actual por la derivada de la func de act. \n",
    "                for l in range(len(self.lista_matrices)-2, -1, -1): # acá propagamos desde la salida para aras\n",
    "                    W_next = self.lista_matrices[l+1][:,1:]\n",
    "                    delta_next = deltas[l+1]\n",
    "                    a_l = activaciones[l+1]\n",
    "                    deltas[l] = np.dot(W_next.T, delta_next) * self.derivada_activacion(a_l)\n",
    "                for l in range(len(self.lista_matrices)):\n",
    "                    a_prev = np.concatenate(([1], activaciones[l]))\n",
    "                    grad_acum[l] += np.outer(deltas[l], a_prev)\n",
    "            for l in range(len(self.lista_matrices)):\n",
    "                self.lista_matrices[l] -= self.lr * grad_acum[l] / n_samples\n",
    "            if epoch % 100 == 0 or epoch == epochs-1:\n",
    "                pred = self.predecir(X)\n",
    "                loss = np.mean((pred - Y) ** 2) # acá se va guardando el ECM para luego visualizarlo, pero se hace cada tanto, no los miles de puntos. \n",
    "                self.error_por_epoch.append(loss)\n",
    "\n",
    "    def minibatch_training(self, X, Y, tamanio, veces, lr=0.1, epochs=100, X_val=None, Y_val=None, eval_every=1, verbose=False):\n",
    "        \"\"\"\n",
    "        Minibatch training wrapper.\n",
    "        - X, Y: training data (ndarray)\n",
    "        - tamanio: tamaño del minibatch (int)\n",
    "        - veces: número de pasadas (epochs sobre el dataset, int)\n",
    "        - lr, epochs: parámetros pasados a `entrenar` para cada minibatch\n",
    "        - X_val, Y_val: si se pasan, se calcula ECM sobre validación cada `eval_every` batches\n",
    "        - eval_every: frecuencia en batches para evaluar en validación (>=1)\n",
    "        - verbose: si True imprime progreso resumido\n",
    "        \"\"\"\n",
    "        # asegurar arrays numpy en caso de recibir matrices\n",
    "        X = np.asarray(X)\n",
    "        Y = np.asarray(Y)\n",
    "        if X_val is not None and Y_val is not None:\n",
    "            X_val = np.asarray(X_val)\n",
    "            Y_val = np.asarray(Y_val)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        self.train_error_progress = []\n",
    "        self.val_error_progress = []\n",
    "        total_batches_per_pass = int(np.ceil(n_samples / float(tamanio)))\n",
    "        for pass_idx in range(veces): # iteramos todas las veces pedidas\n",
    "            indices = np.random.permutation(n_samples) # hacemos la permutacion para obtener indices randomizados\n",
    "            X_shuffled = X[indices] # hacemos shuffling \n",
    "            Y_shuffled = Y[indices]\n",
    "            for b_idx, start in enumerate(range(0, n_samples, tamanio)):# toma de a batches de \"tamanio\"\n",
    "                end = min(start + tamanio, n_samples)\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                Y_batch = Y_shuffled[start:end]\n",
    "                # Entrenar sobre este minibatch (actualiza pesos acumulativamente)\n",
    "                self.entrenar(X_batch, Y_batch, lr=lr, epochs=epochs)\n",
    "                # Calcular ECM sobre el minibatch (rápido) y guardar progreso\n",
    "                pred_batch = self.predecir(X_batch)\n",
    "                batch_loss = float(np.mean((pred_batch - Y_batch) ** 2))\n",
    "                self.train_error_progress.append(batch_loss)\n",
    "                # Evaluación en conjunto de validación si fue provisto y corresponde por frecuencia\n",
    "                if X_val is not None and Y_val is not None and ((b_idx % eval_every) == 0):\n",
    "                    pred_val = self.predecir(X_val)\n",
    "                    val_loss = float(np.mean((pred_val - Y_val) ** 2))\n",
    "                    self.val_error_progress.append(val_loss)\n",
    "                # Mensaje resumido de progreso\n",
    "                if verbose and (b_idx % max(1, total_batches_per_pass//10) == 0):\n",
    "                    msg = f\"pass {pass_idx+1}/{veces} batch {b_idx+1}/{total_batches_per_pass} train_loss={batch_loss:.6f}\"\n",
    "                    if X_val is not None and Y_val is not None and ((b_idx % eval_every) == 0):\n",
    "                        msg += f\" val_loss={val_loss:.6f}\"\n",
    "                    print(msg)\n",
    "        # devolver los historiales por conveniencia\n",
    "        return {'train': self.train_error_progress, 'val': self.val_error_progress}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c7295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (56000, 3) y_train shape = (56000, 1)\n",
      "X_test shape = (24000, 3) y_test shape = (24000, 1)\n",
      "pass 1/5 batch 1/1400 train_loss=1.662248 val_loss=1.775642\n",
      "pass 1/5 batch 1/1400 train_loss=1.662248 val_loss=1.775642\n",
      "pass 1/5 batch 141/1400 train_loss=1.877625 val_loss=1.313563\n",
      "pass 1/5 batch 141/1400 train_loss=1.877625 val_loss=1.313563\n",
      "pass 1/5 batch 281/1400 train_loss=1.199767 val_loss=1.298995\n",
      "pass 1/5 batch 281/1400 train_loss=1.199767 val_loss=1.298995\n",
      "pass 1/5 batch 421/1400 train_loss=1.496228 val_loss=1.289747\n",
      "pass 1/5 batch 421/1400 train_loss=1.496228 val_loss=1.289747\n",
      "pass 1/5 batch 561/1400 train_loss=1.238502 val_loss=1.283924\n",
      "pass 1/5 batch 561/1400 train_loss=1.238502 val_loss=1.283924\n",
      "pass 1/5 batch 701/1400 train_loss=1.870132 val_loss=1.279499\n",
      "pass 1/5 batch 701/1400 train_loss=1.870132 val_loss=1.279499\n",
      "pass 1/5 batch 841/1400 train_loss=1.059682 val_loss=1.273415\n",
      "pass 1/5 batch 841/1400 train_loss=1.059682 val_loss=1.273415\n",
      "pass 1/5 batch 981/1400 train_loss=1.308789 val_loss=1.267790\n",
      "pass 1/5 batch 981/1400 train_loss=1.308789 val_loss=1.267790\n",
      "pass 1/5 batch 1121/1400 train_loss=1.450174 val_loss=1.264494\n",
      "pass 1/5 batch 1121/1400 train_loss=1.450174 val_loss=1.264494\n",
      "pass 1/5 batch 1261/1400 train_loss=1.581551 val_loss=1.266923\n",
      "pass 1/5 batch 1261/1400 train_loss=1.581551 val_loss=1.266923\n",
      "pass 2/5 batch 1/1400 train_loss=1.423701 val_loss=1.263520\n",
      "pass 2/5 batch 1/1400 train_loss=1.423701 val_loss=1.263520\n",
      "pass 2/5 batch 141/1400 train_loss=2.229110 val_loss=1.260790\n",
      "pass 2/5 batch 141/1400 train_loss=2.229110 val_loss=1.260790\n",
      "pass 2/5 batch 281/1400 train_loss=1.957451 val_loss=1.261465\n",
      "pass 2/5 batch 281/1400 train_loss=1.957451 val_loss=1.261465\n",
      "pass 2/5 batch 421/1400 train_loss=1.898809 val_loss=1.257321\n",
      "pass 2/5 batch 421/1400 train_loss=1.898809 val_loss=1.257321\n",
      "pass 2/5 batch 561/1400 train_loss=0.992604 val_loss=1.257619\n",
      "pass 2/5 batch 561/1400 train_loss=0.992604 val_loss=1.257619\n",
      "pass 2/5 batch 701/1400 train_loss=1.033540 val_loss=1.254888\n",
      "pass 2/5 batch 701/1400 train_loss=1.033540 val_loss=1.254888\n",
      "pass 2/5 batch 841/1400 train_loss=1.273817 val_loss=1.252483\n",
      "pass 2/5 batch 841/1400 train_loss=1.273817 val_loss=1.252483\n",
      "pass 2/5 batch 981/1400 train_loss=1.878267 val_loss=1.258967\n",
      "pass 2/5 batch 981/1400 train_loss=1.878267 val_loss=1.258967\n",
      "pass 2/5 batch 1121/1400 train_loss=0.849096 val_loss=1.253139\n",
      "pass 2/5 batch 1121/1400 train_loss=0.849096 val_loss=1.253139\n",
      "pass 2/5 batch 1261/1400 train_loss=1.129605 val_loss=1.259370\n",
      "pass 2/5 batch 1261/1400 train_loss=1.129605 val_loss=1.259370\n",
      "pass 3/5 batch 1/1400 train_loss=1.187206 val_loss=1.249686\n",
      "pass 3/5 batch 1/1400 train_loss=1.187206 val_loss=1.249686\n",
      "pass 3/5 batch 141/1400 train_loss=0.626412 val_loss=1.248813\n",
      "pass 3/5 batch 141/1400 train_loss=0.626412 val_loss=1.248813\n",
      "pass 3/5 batch 281/1400 train_loss=2.096062 val_loss=1.249365\n",
      "pass 3/5 batch 281/1400 train_loss=2.096062 val_loss=1.249365\n",
      "pass 3/5 batch 421/1400 train_loss=1.208881 val_loss=1.251208\n",
      "pass 3/5 batch 421/1400 train_loss=1.208881 val_loss=1.251208\n",
      "pass 3/5 batch 561/1400 train_loss=1.404256 val_loss=1.247353\n",
      "pass 3/5 batch 561/1400 train_loss=1.404256 val_loss=1.247353\n",
      "pass 3/5 batch 701/1400 train_loss=1.169634 val_loss=1.254785\n",
      "pass 3/5 batch 701/1400 train_loss=1.169634 val_loss=1.254785\n",
      "pass 3/5 batch 841/1400 train_loss=1.284481 val_loss=1.245508\n",
      "pass 3/5 batch 841/1400 train_loss=1.284481 val_loss=1.245508\n",
      "pass 3/5 batch 981/1400 train_loss=1.038696 val_loss=1.244213\n",
      "pass 3/5 batch 981/1400 train_loss=1.038696 val_loss=1.244213\n",
      "pass 3/5 batch 1121/1400 train_loss=1.313761 val_loss=1.241229\n",
      "pass 3/5 batch 1261/1400 train_loss=0.857604 val_loss=1.241856\n",
      "pass 4/5 batch 1/1400 train_loss=1.756438 val_loss=1.240859\n",
      "pass 4/5 batch 141/1400 train_loss=1.551337 val_loss=1.239641\n",
      "pass 4/5 batch 281/1400 train_loss=1.121277 val_loss=1.241060\n",
      "pass 4/5 batch 421/1400 train_loss=1.088490 val_loss=1.239191\n",
      "pass 4/5 batch 561/1400 train_loss=1.128139 val_loss=1.238180\n",
      "pass 4/5 batch 701/1400 train_loss=1.338672 val_loss=1.238268\n",
      "pass 4/5 batch 841/1400 train_loss=1.199629 val_loss=1.239688\n"
     ]
    }
   ],
   "source": [
    "# --- Entrenamiento para la función f(x,y,z) usando X_train / y_train ---\n",
    "# Convertir a ndarray y asegurar formas correctas (X: n x 3, y: n x 1)\n",
    "X_train_arr = np.asarray(X_train, dtype=float)\n",
    "y_train_arr = np.asarray(y_train, dtype=float).reshape(-1, 1)\n",
    "X_test_arr = np.asarray(X_test, dtype=float)\n",
    "y_test_arr = np.asarray(y_test, dtype=float).reshape(-1, 1)\n",
    "\n",
    "print('X_train shape =', X_train_arr.shape, 'y_train shape =', y_train_arr.shape)\n",
    "print('X_test shape =', X_test_arr.shape, 'y_test shape =', y_test_arr.shape)\n",
    "\n",
    "# hay que ajustar la salida para que esté entre 0 y 1, no -3 y 3\n",
    "y_train_arr = (y_train_arr + 3) / 6\n",
    "y_test_arr = (y_test_arr + 3) / 6\n",
    "\n",
    "# Crear un modelo para 3 entradas (ejemplo: 30 neuronas ocultas)\n",
    "model_f = perceptron_multicapa(capas=[30, 1], dim_entrada=3)\n",
    "\n",
    "# Entrenamiento con minibatches: tamanio de batch = 40 (configurable)\n",
    "# Pasamos X_test/y_test para evaluar progreso en validación, verbose para ver avance resumido\n",
    "hist = model_f.minibatch_training(X_train_arr, y_train_arr, tamanio=40, veces=5, lr=0.5, epochs=5, X_val=X_test_arr, Y_val=y_test_arr, eval_every=100, verbose=True)\n",
    "\n",
    "print('Entrenamiento de model_f completado.')\n",
    "print('Últimos 5 train losses:', hist['train'][-5:])\n",
    "print('Últimos 5 val losses:', hist['val'][-5:])\n",
    "\n",
    "# Graficar progreso (train y validation)\n",
    "plt.figure(figsize=(8,4))\n",
    "x_train = np.arange(len(hist['train']))\n",
    "# mapear val a indices aproximados a lo largo del eje de train para visual comparativa\n",
    "if len(hist['val'])>0:\n",
    "    x_val = np.linspace(0, len(hist['train'])-1, len(hist['val']))\n",
    "    plt.plot(x_train, hist['train'], label='train (minibatch ECM)')\n",
    "    plt.plot(x_val, hist['val'], label='val (ECM)')\n",
    "else:\n",
    "    plt.plot(x_train, hist['train'], label='train (minibatch ECM)')\n",
    "plt.xlabel('Batch index')\n",
    "plt.ylabel('ECM')\n",
    "plt.title('Progreso de ECM durante minibatch training (train vs val)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos de progreso y comparación predicción vs real\n",
    "# Helpers de escala (lineal entre espacio original y [0,1])\n",
    "# Si cambias la transformación, actualizá estas funciones.\n",
    "def scale_to_01(y):\n",
    "    \"\"\"Escala y (vector) del rango original [-3,3] a [0,1].\"\"\"\n",
    "    y = np.asarray(y).reshape(-1)\n",
    "    return (y + 3.0) / 6.0\n",
    "\n",
    "\n",
    "def inverse_scale(y01):\n",
    "    \"\"\"Vuelve de [0,1] al rango original [-3,3].\"\"\"\n",
    "    y01 = np.asarray(y01).reshape(-1)\n",
    "    return y01 * 6.0 - 3.0\n",
    "\n",
    "\n",
    "def plot_train_val(hist, titulo='ECM - train vs val'):\n",
    "    \"\"\"Plotea la historia de ECM. Hist contiene ECM calculados sobre targets escalados [0,1].\n",
    "    Para mostrar en la escala original se multiplica la MSE por el factor de escala^2\n",
    "    (si y_orig = a * y_scaled + b, entonces MSE_orig = a^2 * MSE_scaled).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    train = np.array(hist.get('train', []))\n",
    "    val = np.array(hist.get('val', []))\n",
    "    # factor para pasar MSE de escala [0,1] a original [-3,3]: a = 6 -> a^2 = 36\n",
    "    factor = 6.0 ** 2\n",
    "    train_orig = train * factor\n",
    "    val_orig = val * factor\n",
    "    x_train = np.arange(len(train))\n",
    "    plt.plot(x_train, train_orig, label='train (MSE en escala original)')\n",
    "    if val.size > 0:\n",
    "        x_val = np.linspace(0, len(train)-1, len(val)) if len(train)>0 else np.arange(len(val))\n",
    "        plt.plot(x_val, val_orig, label='val (MSE en escala original)')\n",
    "    plt.xlabel('Batch index')\n",
    "    plt.ylabel('ECM (MSE) [escala original]')\n",
    "    plt.title(titulo)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred_vs_true(X, Y, model, n_samples=500, random_seed=0):\n",
    "    \"\"\"Muestra dos gráficos en la escala original ([-3,3]):\n",
    "    - scatter: pred vs true (línea y=x)\n",
    "    - series: valores reales vs predicciones para un subconjunto de índices\n",
    "    Asume que Y recibido está en la misma escala que se usó para entrenar (es decir, [0,1]).\n",
    "    \"\"\"\n",
    "    X_arr = np.asarray(X)\n",
    "    Y_arr = np.asarray(Y).reshape(-1)\n",
    "    preds = model.predecir(X_arr).reshape(-1)\n",
    "\n",
    "    # Convertir ambas series a escala original\n",
    "    Y_un = inverse_scale(Y_arr)\n",
    "    preds_un = inverse_scale(preds)\n",
    "\n",
    "    N = len(Y_un)\n",
    "    if N == 0:\n",
    "        print('No hay muestras para graficar')\n",
    "        return\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    if n_samples is None or n_samples >= N:\n",
    "        idx = np.arange(N)\n",
    "    else:\n",
    "        idx = rng.choice(N, size=n_samples, replace=False)\n",
    "\n",
    "    # scatter pred vs true (en escala original)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(Y_un[idx], preds_un[idx], alpha=0.6, s=10)\n",
    "    mn = min(Y_un[idx].min(), preds_un[idx].min())\n",
    "    mx = max(Y_un[idx].max(), preds_un[idx].max())\n",
    "    plt.plot([mn, mx], [mn, mx], 'r--', label='y = y_pred')\n",
    "    plt.xlabel('True y (original scale)')\n",
    "    plt.ylabel('Predicted y (original scale)')\n",
    "    plt.title('Predicción vs Real (scatter) - escala original')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-3.1, 3.1)\n",
    "    plt.show()\n",
    "\n",
    "    # series plot for same indices sorted\n",
    "    idx_sorted = np.sort(idx)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(idx_sorted, Y_un[idx_sorted], 'o-', label='True y (original)', markersize=4)\n",
    "    plt.plot(idx_sorted, preds_un[idx_sorted], 'x--', label='Predicted y (original)', markersize=4)\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('y (original scale)')\n",
    "    plt.title('True vs Predicted (subset) - escala original')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-3.1, 3.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_prediction_for_xyz(xyz, model):\n",
    "    \"\"\"Recibe xyz = [x,y,z] y muestra la predicción del modelo en la escala original junto al valor real.\"\"\"\n",
    "    arr = np.asarray(xyz, dtype=float).reshape(1, -1)\n",
    "    pred_scaled = model.predecir(arr).reshape(-1)[0]\n",
    "    pred_orig = float(inverse_scale(pred_scaled))\n",
    "    real = float(np.sin(xyz[0]) + np.cos(xyz[1]) + xyz[2])\n",
    "    print(f'Entrada (x,y,z): {xyz}')\n",
    "    print(f'Predicción (escala original): {pred_orig:.6f}')\n",
    "    print(f'Valor real (escala original): {real:.6f}')\n",
    "    return pred_orig\n",
    "\n",
    "\n",
    "# Uso: trazamos train/val si existe el historial 'hist' retornado por minibatch_training\n",
    "try:\n",
    "    plot_train_val(hist, 'ECM por minibatch - train vs val (escala original)')\n",
    "except NameError:\n",
    "    print('Variable hist no encontrada. Ejecutá la celda de entrenamiento antes de graficar.')\n",
    "\n",
    "# Uso: comparar pred/true en test set (se asume que y_test_arr está escalado a [0,1])\n",
    "try:\n",
    "    plot_pred_vs_true(X_test_arr, y_test_arr, model_f, n_samples=1000)\n",
    "except NameError:\n",
    "    print('Asegurate de haber entrenado model_f y de tener X_test_arr / y_test_arr en memoria.')\n",
    "\n",
    "# Ejemplo: pedir predicción para una muestra concreta (descomentar y editar si querés probar)\n",
    "# ejemplo_xyz = [0.1, 1.2, -1]\n",
    "# show_prediction_for_xyz(ejemplo_xyz, model_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee73449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Experimentos: Inciso (a) y (b) ===\n",
    "# - (a) Entrenar en el dataset completo y mostrar evolución de error y predicciones\n",
    "# - (b) Tomar un conjunto de entrenamiento con 40 muestras y comparar minibatch size = 40 vs = 1\n",
    "\n",
    "import time\n",
    "\n",
    "# Comprobaciones básicas\n",
    "try:\n",
    "    X_train_arr\n",
    "    y_train_arr\n",
    "    X_test_arr\n",
    "    y_test_arr\n",
    "except NameError:\n",
    "    raise NameError('Ejecutá antes las celdas que generan y escalan X_train_arr / y_train_arr / X_test_arr / y_test_arr')\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# ---- Inciso (a): entrenar con el dataset grande (ya escalado a [0,1]) ----\n",
    "print('\\n--- Inciso (a): entrenamiento en dataset grande (ejemplo) ---')\n",
    "start = time.time()\n",
    "model_a = perceptron_multicapa(capas=[30,1], dim_entrada=3)\n",
    "# parámetros moderados, rápido de correr: 5 pases, minibatch 40\n",
    "# los epochs deben ser 1 porque queremos mostrar el minibatch solo 1 vez, sino es literal multiplkicar el delta W por la cantidad de veces que se lo mostramos, muy tonto de mí parte haber probado . \n",
    "hist_a = model_a.minibatch_training(X_train_arr, y_train_arr, tamanio=40, veces=5, lr=0.5, epochs=1, X_val=X_test_arr, Y_val=y_test_arr, eval_every=50, verbose=True)\n",
    "print(f'Tiempo entrenamiento (a): {time.time()-start:.2f}s')\n",
    "print('Últimos 5 train losses (scaled):', hist_a['train'][-5:])\n",
    "if len(hist_a['val'])>0:\n",
    "    print('Últimos 5 val losses (scaled):', hist_a['val'][-5:])\n",
    "\n",
    "# Graficar (se muestran en escala original mediante las funciones que añadimos)\n",
    "plot_train_val(hist_a, 'Inciso (a) - MSE por minibatch (escala original)')\n",
    "plot_pred_vs_true(X_test_arr, y_test_arr, model_a, n_samples=1000)\n",
    "\n",
    "# Ejemplo de predicción para una muestra concreta\n",
    "example_xyz = [0.5, 1.0, -1]\n",
    "print('\\nEjemplo:')\n",
    "show_prediction_for_xyz(example_xyz, model_a)\n",
    "\n",
    "# ---- Inciso (b): usar solo 40 muestras de entrenamiento y comparar minibatch sizes ----\n",
    "print('\\n--- Inciso (b): comparar minibatch size = 40 vs = 1 (con solo 40 muestras de entrenamiento) ---')\n",
    "# seleccionar 40 muestras aleatorias del conjunto de entrenamiento escalado\n",
    "n_small = 40\n",
    "N_train = X_train_arr.shape[0]\n",
    "idx_small = rng.choice(N_train, size=n_small, replace=False)\n",
    "X_small = X_train_arr[idx_small]\n",
    "y_small = y_train_arr[idx_small]\n",
    "\n",
    "# Mostrar algunos valores reales (desescalados) para verificar rango\n",
    "print('Algunas Y (original scale) del subset de 40:')\n",
    "print(inverse_scale(y_small[:6].reshape(-1)))\n",
    "\n",
    "# Experimento 1: minibatch = 40 (batch gradient descent w.r.t dataset_small)\n",
    "model_b1 = perceptron_multicapa(capas=[30,1], dim_entrada=3)\n",
    "start = time.time()\n",
    "# usamos muchas pasadas para ver sobreajuste/variación, entrenar con epochs=1 por minibatch\n",
    "hist_b1 = model_b1.minibatch_training(X_small, y_small, tamanio=40, veces=200, lr=0.1, epochs=1, X_val=X_test_arr, Y_val=y_test_arr, eval_every=20, verbose=False)\n",
    "print(f'Tiempo experimento b1 (batch=40): {time.time()-start:.2f}s')\n",
    "print('Len train history b1:', len(hist_b1['train']), 'Len val history b1:', len(hist_b1['val']))\n",
    "\n",
    "# Graficar\n",
    "plot_train_val(hist_b1, 'Inciso (b) - batch size 40 (MSE escala original)')\n",
    "plot_pred_vs_true(X_test_arr, y_test_arr, model_b1, n_samples=1000)\n",
    "\n",
    "# Evaluación numérica final (MSE en escala original)\n",
    "def mse_original(model, X, y_scaled):\n",
    "    preds = model.predecir(np.asarray(X)).reshape(-1)\n",
    "    preds_orig = inverse_scale(preds)\n",
    "    y_orig = inverse_scale(np.asarray(y_scaled).reshape(-1))\n",
    "    return float(np.mean((preds_orig - y_orig)**2))\n",
    "\n",
    "mse_b1_train = mse_original(model_b1, X_small, y_small)\n",
    "mse_b1_test = mse_original(model_b1, X_test_arr, y_test_arr)\n",
    "print(f'MSE (original scale) para b1 - train (subset 40): {mse_b1_train:.6f}, test: {mse_b1_test:.6f}')\n",
    "\n",
    "# Experimento 2: minibatch = 1 (stochastic)\n",
    "model_b2 = perceptron_multicapa(capas=[30,1], dim_entrada=3)\n",
    "start = time.time()\n",
    "hist_b2 = model_b2.minibatch_training(X_small, y_small, tamanio=1, veces=200, lr=0.1, epochs=1, X_val=X_test_arr, Y_val=y_test_arr, eval_every=200, verbose=False)\n",
    "print(f'Tiempo experimento b2 (batch=1): {time.time()-start:.2f}s')\n",
    "print('Len train history b2:', len(hist_b2['train']), 'Len val history b2:', len(hist_b2['val']))\n",
    "\n",
    "plot_train_val(hist_b2, 'Inciso (b) - batch size 1 (MSE escala original)')\n",
    "plot_pred_vs_true(X_test_arr, y_test_arr, model_b2, n_samples=1000)\n",
    "\n",
    "mse_b2_train = mse_original(model_b2, X_small, y_small)\n",
    "mse_b2_test = mse_original(model_b2, X_test_arr, y_test_arr)\n",
    "print(f'MSE (original scale) para b2 - train (subset 40): {mse_b2_train:.6f}, test: {mse_b2_test:.6f}')\n",
    "\n",
    "# Resumen comparativo\n",
    "print('\\nResumen comparativo (MSE en escala original):')\n",
    "print(f'  batch=40 -> train(subset40)={mse_b1_train:.6f}, test={mse_b1_test:.6f}')\n",
    "print(f'  batch=1  -> train(subset40)={mse_b2_train:.6f}, test={mse_b2_test:.6f}')\n",
    "\n",
    "print('\\nObservaciones:')\n",
    "print('- Con batch=40 (entrenamiento por batches del tamaño del dataset pequeño) suele converger de forma más estable pero puede sobreajustar si damos muchas pasadas.')\n",
    "print('- Con batch=1 (estocástico) la trayectoria del error es más ruidosa; puede generalizar distinto según la tasa de aprendizaje y la cantidad de pasos.')\n",
    "\n",
    "# Fin de celda de experimentos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
