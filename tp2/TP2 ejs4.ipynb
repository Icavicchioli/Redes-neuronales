{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d44e76",
   "metadata": {},
   "source": [
    "# Redes Neuronales - TP2\n",
    "## Ej 4\n",
    "- a - Implemente una red con aprendizaje Backpropagation que aprenda la siguiente\n",
    "función:\n",
    "\n",
    "$$f (x , y , z)=sin(x)+cos(y)+z$$\n",
    "\n",
    "donde: x e y ∊ [0,2 π] y z ∊[−1,1]. Para ello construya un conjunto de datos de entrenamiento y un conjunto de evaluación. Muestre la evolución del error  e\n",
    "entrenamiento y de evaluación en función de las épocas de entrenamiento.\n",
    "\n",
    "- b - Estudie la evolución de los errores durante el entrenamiento de una red con una capa oculta de 30 neuronas cuando el conjunto de entrenamiento contiene 40 muestras. ¿Que ocurre si el minibatch tiene tamaño 40? ¿Y si tiene tamaño 1?\n",
    "\n",
    "Acá la consigna lo lo explicita pero hay que aplicar algo de minibatch al lagoritmo de aprendizaje.\n",
    "\n",
    "Robando de un post de stackExchange:\n",
    "\n",
    "batch gradient descent you process the entire training set in one iteration. Whereas, in a mini-batch gradient descent you process a small subset of the training set in each iteration.\n",
    "\n",
    "Also compare stochastic gradient descent, where you process a single example from the training set in each iteration.\n",
    "\n",
    "Another way to look at it: they are all examples of the same approach to gradient descent with a batch size of m and a training set of size n. For stochastic gradient descent, m=1. For batch gradient descent, m = n. For mini-batch, m=b and b < n, typically b is small compared to n.\n",
    "\n",
    "Mini-batch adds the question of determining the right size for b, but finding the right b may greatly improve your results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d886a6",
   "metadata": {},
   "source": [
    "Dicho todo esto, la idea es tomar el set de datos y generar batches de entrenamiento. Es otro loop for y una partición de los datos de entrada. hay 2 hiperparams -> la # de minibatches y la # de veces que hacemos el proceso. Para mantener la cantidad de muestras balanceadas, solo se le pueden mostrar 1 vez por batch de minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "815fd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e54023",
   "metadata": {},
   "source": [
    "Vamos a generar las muestras de la función. Voy a barrer X, Y de 0 a 2pi para Z=+-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0e28250",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,num=200)*2*np.pi\n",
    "y = x # hasta ahora tenemos los ejes\n",
    "z = [-1,1]\n",
    "f = []\n",
    "tripletas_xyz = []\n",
    "\n",
    "for k in z:\n",
    "    for i in x:\n",
    "        for j in y:\n",
    "            tripletas_xyz.append([i,j,k])\n",
    "\n",
    "    \n",
    "# este es el dataset entero, es gigante. \n",
    "\n",
    "# -> lo pasamos a matriz\n",
    "\n",
    "dataset = np.asmatrix(tripletas_xyz) # listo. \n",
    "\n",
    "# no lo shuffleo acá, es lo va a hacer el entrenador\n",
    "# voy a generar el f\n",
    "\n",
    "f = np.sin(dataset[:,0])+np.cos(dataset[:,1])+dataset[:,2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987957fa",
   "metadata": {},
   "source": [
    "Ahora toca shufflear las filas y hacer dataset de entrenamiento y de testeo. Por comodidad lo hago con sklearn, el resto si será a mano. Como tengo 80mil registros, y quiero batches de números enteros, voy a separar esto en una proporción 70% 30%, que me parece razonable. Luego, los minibatches serán de números como 1,10,100,1000,2000,4000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf330bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(  dataset, f, train_size = 56000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee833391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron_multicapa:\n",
    "    def __init__(self, capas, dim_entrada):\n",
    "        self.capas = capas\n",
    "        self.dim_entrada = dim_entrada\n",
    "        self.lista_matrices = []\n",
    "        self.lr = None\n",
    "        entrada_anterior = dim_entrada\n",
    "        for num_perceptrones in capas:\n",
    "            matriz_pesos = np.random.uniform(-2, 2, size=(num_perceptrones, entrada_anterior + 1)) # inicialización más amplia\n",
    "            self.lista_matrices.append(matriz_pesos)\n",
    "            entrada_anterior = num_perceptrones\n",
    "\n",
    "    def funcion_activacion(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def derivada_activacion(self, a):\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        activaciones = [a] # todos los x de la red cuando se propaga para adelante - es el zs pasado por la función de activación\n",
    "        zs = []# todos los h de la red cuando se propaga para adelante\n",
    "        for W in self.lista_matrices:\n",
    "            a_b = np.concatenate(([1], a))\n",
    "            z = np.dot(W, a_b)\n",
    "            zs.append(z)\n",
    "            a = self.funcion_activacion(z)\n",
    "            activaciones.append(a)\n",
    "        return activaciones, zs\n",
    "    \n",
    "    def predecir(self, X):\n",
    "        salidas = []\n",
    "        for x in X:\n",
    "            a = x\n",
    "            for W in self.lista_matrices:\n",
    "                a = np.concatenate(([1], a))\n",
    "                a = self.funcion_activacion(np.dot(W, a))\n",
    "            salidas.append(a.reshape(-1))  # garantiza que sea vector 1D\n",
    "        return np.array(salidas).reshape(len(X), -1)\n",
    "\n",
    "    def predecir_hard(self, X):\n",
    "        salidas = self.predecir(X)\n",
    "        # Si la salida es entre 0 y 1 (sigmoide), umbral en 0.5\n",
    "        return (salidas >= 0.5).astype(int)\n",
    "\n",
    "    def entrenar(self, X, Y, lr=1.0, epochs=20000):\n",
    "        self.lr = lr\n",
    "        n_samples = X.shape[0]\n",
    "        self.error_por_epoch = []\n",
    "        for epoch in range(epochs):\n",
    "            grad_acum = [np.zeros_like(W) for W in self.lista_matrices]\n",
    "            for i in range(n_samples):\n",
    "                x = X[i]\n",
    "                y = Y[i]\n",
    "                activaciones, zs = self.forward(x) # tomo los h y x del paso foward\n",
    "                deltas = [None] * len(self.lista_matrices) # lugar para guardar los deltas\n",
    "                a_out = activaciones[-1] # la activavión de la salida es la última\n",
    "                deltas[-1] = (a_out - y) * self.derivada_activacion(a_out) # el último delta es el de la salida, y se calcula como la salida deseada - la actual por la derivada de la func de act. \n",
    "                for l in range(len(self.lista_matrices)-2, -1, -1): # acá propagamos desde la salida para aras\n",
    "                    W_next = self.lista_matrices[l+1][:,1:]\n",
    "                    delta_next = deltas[l+1]\n",
    "                    a_l = activaciones[l+1]\n",
    "                    deltas[l] = np.dot(W_next.T, delta_next) * self.derivada_activacion(a_l)\n",
    "                for l in range(len(self.lista_matrices)):\n",
    "                    a_prev = np.concatenate(([1], activaciones[l]))\n",
    "                    grad_acum[l] += np.outer(deltas[l], a_prev)\n",
    "            for l in range(len(self.lista_matrices)):\n",
    "                self.lista_matrices[l] -= self.lr * grad_acum[l] / n_samples\n",
    "            if epoch % 100 == 0 or epoch == epochs-1:\n",
    "                pred = self.predecir(X)\n",
    "                loss = np.mean((pred - Y) ** 2) # acá se va guardando el ECM para luego visualizarlo, pero se hace cada tanto, no los miles de puntos. \n",
    "                self.error_por_epoch.append(loss)\n",
    "\n",
    "    def minibatch_training(self, X, Y, tamanio, veces, lr=0.1, epochs=20000):\n",
    "        # X: matriz de entradas, Y: vector columna de targets\n",
    "        n_samples = X.shape[0]\n",
    "        for _ in range(veces):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            Y_shuffled = Y[indices]\n",
    "            for start in range(0, n_samples, tamanio):\n",
    "                end = min(start + tamanio, n_samples)\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                Y_batch = Y_shuffled[start:end]\n",
    "                self.entrenar(X_batch, Y_batch, lr=lr, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfa43828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos para la XOR de 2 entradas y 1 salida (corregido)\n",
    "A = np.array([0, 0, 1, 1])\n",
    "B = np.array([0, 1, 0, 1])\n",
    "Y12 = np.array([0, 1, 1, 0])\n",
    "datos_XOR2 = np.column_stack((A, B, Y12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa6c7295",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m X2 = datos_XOR2[:, \u001b[32m0\u001b[39m:\u001b[32m2\u001b[39m]\n\u001b[32m      7\u001b[39m Y2 = datos_XOR2[:, \u001b[32m2\u001b[39m].reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# targets 0/1 para sigmoide\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtest2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminibatch_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtamanio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mveces\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mperceptron_multicapa.minibatch_training\u001b[39m\u001b[34m(self, X, Y, tamanio, veces, lr, epochs)\u001b[39m\n\u001b[32m     83\u001b[39m batchesXY = np.vsplit(data,tamanio) \u001b[38;5;66;03m# separo por filas\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batchesXY:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28mself\u001b[39m.entrenar(X=batch[:,\u001b[32m0\u001b[39m:\u001b[32m3\u001b[39m],Y=\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m,lr=lr,epochs=epochs)\n",
      "\u001b[31mIndexError\u001b[39m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "# ahora toca entrenar el perceptron con estos datos\n",
    "test2 = perceptron_multicapa(capas=[4,1], dim_entrada=2) # para la XOR de 2 entradas y 1 salida\n",
    "test4 = perceptron_multicapa(capas=[4, 1], dim_entrada=4) # para la XOR de 4 entradas y 1 salida\n",
    "\n",
    "# entrenamos ambos modelos con sus respectivos datos y targets \n",
    "X2 = datos_XOR2[:, 0:2]\n",
    "Y2 = datos_XOR2[:, 2].reshape(-1, 1)  # targets 0/1 para sigmoide\n",
    "\n",
    "\n",
    "test2.minibatch_training(X2, Y2, tamanio = 1, veces = 10, lr=0.1, epochs=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77380caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar frontera de decisión para XOR de 2 entradas\n",
    "h = 0.01\n",
    "x_min, x_max = X2[:,0].min() - 0.1, X2[:,0].max() + 0.1\n",
    "y_min, y_max = X2[:,1].min() - 0.1, X2[:,1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = test2.predecir(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=[0,0.5,1], alpha=0.3, cmap=\"bwr\")\n",
    "plt.scatter(X2[:,0], X2[:,1], c=Y2[:,0], cmap=\"bwr\", edgecolors=\"k\")\n",
    "plt.title(\"Frontera de decisión XOR (2 entradas)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e01a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar predicciones hard y soft para los 4 puntos de la XOR de 2 entradas\n",
    "print(\"Entradas\\tTarget\\tSalida sigmoide\\tPredicción (umbral 0.5)\")\n",
    "preds = test2.predecir(X2)\n",
    "for x, y, p in zip(X2, Y2, preds):\n",
    "    print(f\"{x}\\t{int(y[0])}\\t{p[0]:.3f}\\t\\t{int(p[0]>=0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_error(modelo, titulo):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(modelo.error_por_epoch)) * 100, modelo.error_por_epoch, marker='o')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('ECM')\n",
    "    plt.title(titulo)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Graficar error para XOR 2 entradas\n",
    "graficar_error(test2, \"Evolución del error (ECM) - XOR 2 entradas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef14515",
   "metadata": {},
   "source": [
    "la xor de 4 cae más rápido xq tiene 16 muestras para 4 dimensiones, mucbo menos proporiconalmente que 4 muestras para 2. diría que está más vacio el espacio R4 que el R2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
