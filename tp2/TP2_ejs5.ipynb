{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79d44e76",
      "metadata": {
        "id": "79d44e76"
      },
      "source": [
        "# Redes Neuronales - TP2\n",
        "## Ej 5\n",
        "\n",
        "Siguiendo el trabajo de Hinton y Salakhutdinov (2006), entrene una máquina restringida\n",
        "de Boltzmann con imágenes de la base de datos MNIST. Muestre el error de\n",
        "recontruccion durante el entrenamiento, y ejemplos de cada uno de los dígitos\n",
        "reconstruidos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b0b95d",
      "metadata": {},
      "source": [
        "La MRB se puede pensar como una red feedfoward de 4 capas, en vez de pensarla como una de 2 capas con pesos simétricos. $v$ es la capa visible y $h$ es la oculta, y tenemos un $\\hat{v}$ y $\\hat{h}$, que son las estimaciones. \n",
        "\n",
        "Las fórmulas que hay que seguir son las siguientes:\n",
        "\n",
        "$$\n",
        "m_i = \\text{pixel}_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_i \\sim  \\mathcal{N}(m_i , 1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "P_r (j) = g(\\sum _i w_{ij} v_i + b_j) \\quad \\quad g (x) = \\frac{1}{1+e^{-x}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h(j) = \n",
        "\\begin{cases}\n",
        "1 \\quad \\text{con probabilidad} \\quad P_r \n",
        "\\\\\n",
        "0 \\quad  \\text{con probabilidad}\\quad 1-P_r \n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{guardar} \\quad (v_i , h_j)_{data}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e778c1",
      "metadata": {},
      "source": [
        "Luego se repite pero para obtener las estimaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f56d151",
      "metadata": {},
      "source": [
        "$$\n",
        "m_i = \\sum _j w_{ij} v_i + b_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{v}_i \\sim  \\mathcal{N}(m_i , 1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "P_r (j) = g(\\sum _i w_{ij} v_i + b_j) \\quad \\quad g (x) = \\frac{1}{1+e^{-x}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{h}(j) = \n",
        "\\begin{cases}\n",
        "1 \\quad \\text{con probabilidad} \\quad P_r \n",
        "\\\\\n",
        "0 \\quad  \\text{con probabilidad}\\quad 1-P_r \n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{guardar} \\quad (v_i , h_j)_{reconstruccion}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5604ab05",
      "metadata": {},
      "source": [
        "Al final se hace\n",
        "\n",
        "$$\n",
        "\\Delta w_{ij}= \\eta ( \\left\\langle v_i h_j \\right\\rangle - \\left\\langle \\hat{v_i} \\hat{h_j} \\right\\rangle)\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_i= \\eta ( \\left\\langle v_i \\right\\rangle - \\left\\langle \\hat{v_i} \\right\\rangle)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Delta w_{ij}= \\eta ( \\left\\langle v_i h_j \\right\\rangle - \\left\\langle \\hat{v_i} \\hat{h_j} \\right\\rangle)\n",
        "$$\n",
        "$$\n",
        "b_j= \\eta ( \\left\\langle  h_j \\right\\rangle - \\left\\langle  \\hat{h_j} \\right\\rangle)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a63bb448",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ae4c5441",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1/2 - recon_error: 1.013876\n",
            "Epoch   2/2 - recon_error: 1.009487\n",
            "Finished, epoch errors: [np.float32(1.0138762), np.float32(1.0094873)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x)) # es la logistica pero toma forma de sigmoide\n",
        "\n",
        "\n",
        "class RBM:\n",
        "    \"\"\"\n",
        "    RBM con visibles gaussianas (sigma=1) y ocultas binarias.\n",
        "\n",
        "    Métodos separados y pequeñas correcciones para evitar la adición accidental de ruido\n",
        "    doble y para usar las probabilidades correctas al calcular gradientes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_visible, n_hidden, lr=0.1, rng=None):\n",
        "        self.n_visible = int(n_visible)\n",
        "        self.n_hidden = int(n_hidden)\n",
        "        self.lr = float(lr)\n",
        "        self.rng = np.random.RandomState(None) if rng is None else rng\n",
        "\n",
        "        # pesos: forma (n_visible, n_hidden)\n",
        "        self.W = 0.01 * self.rng.randn(self.n_visible, self.n_hidden).astype(np.float32)\n",
        "        # sesgos visibles y ocultos\n",
        "        self.b = np.zeros(self.n_visible, dtype=np.float32)\n",
        "        self.c = np.zeros(self.n_hidden, dtype=np.float32)\n",
        "\n",
        "    def v_to_h(self, pixeles, sample=True, sample_visible=False):\n",
        "        \"\"\"Paso v -> h.\n",
        "        Args:\n",
        "          pixeles: array (batch, n_visible) con las medias m_i (por ejemplo los pixeles en [0,1])\n",
        "          sample: si True, devuelve muestras binarias de h además de las probabilidades\n",
        "          sample_visible: si True, primero muestrea visibles v ~ N(m,1); si False usa m directamente\n",
        "        Returns:\n",
        "          h_prob, h_samp\n",
        "        \"\"\"\n",
        "        m = np.asarray(pixeles, dtype=np.float32)  # media = pixel_i\n",
        "        if sample_visible:\n",
        "            v = m + self.rng.randn(*m.shape).astype(np.float32)\n",
        "        else:\n",
        "            v = m\n",
        "\n",
        "        pre = np.dot(v, self.W) + self.c\n",
        "        h_prob = _sigmoid(pre)\n",
        "        h_samp = None\n",
        "        if sample:\n",
        "            h_samp = (self.rng.rand(*h_prob.shape) < h_prob).astype(np.float32)\n",
        "        return h_prob, h_samp\n",
        "\n",
        "    def h_to_v(self, h, sample=True):\n",
        "        \"\"\"Paso h -> v.\n",
        "        Args:\n",
        "          h: array (batch, n_hidden) -- puede ser probabilidades o muestras\n",
        "          sample: si True, devuelve muestras gaussianas v ~ N(mean, 1); si False solo la media\n",
        "        Returns:\n",
        "          v_mean, v_samp\n",
        "        \"\"\"\n",
        "        h = np.asarray(h, dtype=np.float32)\n",
        "        v_mean = np.dot(h, self.W.T) + self.b\n",
        "        v_samp = None\n",
        "        if sample:\n",
        "            v_samp = v_mean + self.rng.randn(*v_mean.shape).astype(np.float32)\n",
        "        return v_mean, v_samp\n",
        "\n",
        "    def gradiente(self, v0, h0_prob, v1, h1_prob):\n",
        "        \"\"\"CD-1: calcula dW, db, dc usando probabilidades (h0_prob, h1_prob).\n",
        "        Args:\n",
        "          v0: batch de datos (N, n_visible)\n",
        "          h0_prob: P(h|v0) (N, n_hidden)\n",
        "          v1: reconstrucción de v (muestra) (N, n_visible)\n",
        "          h1_prob: P(h|v1) (N, n_hidden)\n",
        "        Returns:\n",
        "          dW, db, dc (ya multiplicados por lr)\n",
        "        \"\"\"\n",
        "        v0 = np.asarray(v0, dtype=np.float32)\n",
        "        v1 = np.asarray(v1, dtype=np.float32)\n",
        "        h0_prob = np.asarray(h0_prob, dtype=np.float32)\n",
        "        h1_prob = np.asarray(h1_prob, dtype=np.float32)\n",
        "\n",
        "        batch = float(v0.shape[0])\n",
        "        pos_assoc = np.dot(v0.T, h0_prob) / batch\n",
        "        neg_assoc = np.dot(v1.T, h1_prob) / batch\n",
        "\n",
        "        dW = self.lr * (pos_assoc - neg_assoc)\n",
        "        db = self.lr * (v0.mean(axis=0) - v1.mean(axis=0))\n",
        "        dc = self.lr * (h0_prob.mean(axis=0) - h1_prob.mean(axis=0))\n",
        "        return dW, db, dc\n",
        "\n",
        "    def reconstruct(self, v):\n",
        "        \"\"\"Reconstrucción determinística: v -> h_prob -> v_mean.\"\"\"\n",
        "        h_prob, _ = self.v_to_h(v, sample=False, sample_visible=False)\n",
        "        v_mean, _ = self.h_to_v(h_prob, sample=False)\n",
        "        return v_mean\n",
        "\n",
        "    def train_loop(self, data, max_epochs=50, batch_size=64, tol=1e-4, verbose=True):\n",
        "        \"\"\"Superloop de entrenamiento que itera por épocas y batches.\n",
        "        Por defecto hace CD-1 usando:\n",
        "          - h0_prob = P(h|v0)\n",
        "          - h0_samp ~ Bernoulli(h0_prob)\n",
        "          - v1_samp ~ N(dot(h0_samp,W^T)+b, 1)\n",
        "          - h1_prob = P(h|v1_samp)\n",
        "        Y usa (v0, h0_prob) y (v1_samp, h1_prob) para calcular gradientes.\n",
        "        \"\"\"\n",
        "        X = np.asarray(data, dtype=np.float32)\n",
        "        N = X.shape[0]\n",
        "        n_batches = max(1, N // batch_size)\n",
        "\n",
        "        epoch_errors = []\n",
        "        prev_epoch_err = None\n",
        "\n",
        "        for epoch in range(1, max_epochs + 1):\n",
        "            perm = self.rng.permutation(N)\n",
        "            epoch_err = 0.0\n",
        "            for b in range(n_batches):\n",
        "                idx = perm[b * batch_size:(b + 1) * batch_size]\n",
        "                v0 = X[idx]\n",
        "\n",
        "                # Paso 1: v -> h (obtener probabilidades y muestras)\n",
        "                h0_prob, h0_samp = self.v_to_h(v0, sample=True, sample_visible=False)\n",
        "\n",
        "                # Paso 2: h -> v (reconstrucción)\n",
        "                v1_mean, v1_samp = self.h_to_v(h0_samp, sample=True)\n",
        "\n",
        "                # Paso 3: v(recon) -> h (prob negative)\n",
        "                h1_prob, _ = self.v_to_h(v1_samp, sample=False, sample_visible=False)\n",
        "\n",
        "                # Paso 4: calcular gradientes usando probabilidades para las ocultas\n",
        "                dW, db, dc = self.gradiente(v0, h0_prob, v1_samp, h1_prob)\n",
        "\n",
        "                # Aplicar actualizaciones\n",
        "                self.W += dW\n",
        "                self.b += db\n",
        "                self.c += dc\n",
        "\n",
        "                # Acumular error del batch (MSE entre v0 y v1_mean)\n",
        "                batch_err = np.mean((v0 - v1_mean) ** 2)\n",
        "                epoch_err += batch_err\n",
        "\n",
        "            epoch_err /= float(n_batches)\n",
        "            epoch_errors.append(epoch_err)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch:3d}/{max_epochs} - recon_error: {epoch_err:.6f}\")\n",
        "\n",
        "            # Criterio de convergencia: cambio pequeño en el error promedio por época\n",
        "            if prev_epoch_err is not None and abs(prev_epoch_err - epoch_err) < tol:\n",
        "                if verbose:\n",
        "                    print(\"Converged (tol reached). Stopping training.\")\n",
        "                break\n",
        "            prev_epoch_err = epoch_err\n",
        "\n",
        "        return epoch_errors\n",
        "\n",
        "\n",
        "# Prueba rápida de humo: solo si se ejecuta como script\n",
        "if __name__ == \"__main__\":\n",
        "    rbm = RBM(28 * 28, 128, lr=0.01)\n",
        "    X = np.random.randn(100, 28 * 28).astype(np.float32)\n",
        "    errs = rbm.train_loop(X, max_epochs=2, batch_size=32, verbose=True)\n",
        "    print('Finished, epoch errors:', errs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07c166e",
      "metadata": {},
      "source": [
        "## Entrenamiento con MNIST\n",
        "\n",
        "En estas celdas cargamos MNIST, preprocesamos las imágenes (normalización y aplanado),\n",
        "entrenamos la RBM usando `train_loop` implementado más arriba y luego mostramos\n",
        "el error de reconstrucción por época y algunas reconstrucciones de ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d9a1b0ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1/25 - recon_error: 0.068635\n",
            "Epoch   2/25 - recon_error: 0.055880\n",
            "Epoch   2/25 - recon_error: 0.055880\n",
            "Epoch   3/25 - recon_error: 0.053194\n",
            "Epoch   3/25 - recon_error: 0.053194\n",
            "Epoch   4/25 - recon_error: 0.052009\n",
            "Epoch   4/25 - recon_error: 0.052009\n",
            "Epoch   5/25 - recon_error: 0.051037\n",
            "Epoch   5/25 - recon_error: 0.051037\n",
            "Epoch   6/25 - recon_error: 0.050183\n",
            "Epoch   6/25 - recon_error: 0.050183\n",
            "Epoch   7/25 - recon_error: 0.049457\n",
            "Epoch   7/25 - recon_error: 0.049457\n",
            "Epoch   8/25 - recon_error: 0.048881\n",
            "Epoch   8/25 - recon_error: 0.048881\n",
            "Epoch   9/25 - recon_error: 0.048304\n",
            "Epoch   9/25 - recon_error: 0.048304\n",
            "Epoch  10/25 - recon_error: 0.047848\n",
            "Epoch  10/25 - recon_error: 0.047848\n",
            "Epoch  11/25 - recon_error: 0.047421\n",
            "Epoch  11/25 - recon_error: 0.047421\n",
            "Epoch  12/25 - recon_error: 0.047045\n",
            "Epoch  12/25 - recon_error: 0.047045\n",
            "Epoch  13/25 - recon_error: 0.046698\n",
            "Epoch  13/25 - recon_error: 0.046698\n",
            "Epoch  14/25 - recon_error: 0.046451\n",
            "Epoch  14/25 - recon_error: 0.046451\n",
            "Epoch  15/25 - recon_error: 0.046169\n",
            "Epoch  15/25 - recon_error: 0.046169\n",
            "Epoch  16/25 - recon_error: 0.045921\n",
            "Epoch  16/25 - recon_error: 0.045921\n",
            "Epoch  17/25 - recon_error: 0.045688\n",
            "Epoch  17/25 - recon_error: 0.045688\n",
            "Epoch  18/25 - recon_error: 0.045505\n",
            "Epoch  18/25 - recon_error: 0.045505\n",
            "Epoch  19/25 - recon_error: 0.045269\n",
            "Epoch  19/25 - recon_error: 0.045269\n",
            "Epoch  20/25 - recon_error: 0.045101\n",
            "Epoch  20/25 - recon_error: 0.045101\n",
            "Epoch  21/25 - recon_error: 0.044915\n",
            "Epoch  21/25 - recon_error: 0.044915\n",
            "Epoch  22/25 - recon_error: 0.044744\n",
            "Epoch  22/25 - recon_error: 0.044744\n",
            "Epoch  23/25 - recon_error: 0.044604\n",
            "Epoch  23/25 - recon_error: 0.044604\n",
            "Epoch  24/25 - recon_error: 0.044479\n",
            "Epoch  24/25 - recon_error: 0.044479\n",
            "Epoch  25/25 - recon_error: 0.044331\n",
            "Epoch  25/25 - recon_error: 0.044331\n"
          ]
        }
      ],
      "source": [
        "# Preparar MNIST y entrenar la RBM\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar MNIST\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "# Preprocesamiento:\n",
        "# - aplanar\n",
        "# - normalizar: dejamos los pixeles en rango [0,1] y usamos la interpretación de visibles gaussianas\n",
        "#   con media igual al pixel (m_i = pixel_i). Para respetar eso, simplemente usamos los valores\n",
        "#   en [0,1] como medias cuando alimentamos v_to_h en la clase.\n",
        "\n",
        "train_X = train_X.astype(np.float32) / 255.0\n",
        "test_X = test_X.astype(np.float32) / 255.0\n",
        "\n",
        "X = train_X.reshape(-1, 28 * 28)\n",
        "\n",
        "# Instanciar RBM\n",
        "rbm = RBM(28 * 28, 256, lr=0.01)\n",
        "\n",
        "# Entrenar\n",
        "errs = rbm.train_loop(X, max_epochs=25, batch_size=128, tol=1e-5, verbose=True)\n",
        "\n",
        "# Guardar errores y modelo en memoria\n",
        "train_errors = errs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fab8d244",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_recon_and_error(rbm, X, train_errors, n_show=10, title_prefix=None):\n",
        "    \"\"\"Muestra la curva de error por época y una rejilla de `n_show` originales vs reconstrucciones.\n",
        "\n",
        "    Args:\n",
        "      rbm: instancia entrenada de RBM\n",
        "      X: matriz (N, n_visible) usada para obtener los `n_show` primeros ejemplos para visualizar\n",
        "      train_errors: lista de floats con el error por época\n",
        "      n_show: cuántas imágenes mostrar\n",
        "      title_prefix: texto opcional para anteponer al título (p. ej. clave del experimento)\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # Plot error\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(train_errors, '-o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recon Error (MSE)')\n",
        "    title = 'Recon Error por Época'\n",
        "    if title_prefix:\n",
        "        title = f\"{title_prefix} - {title}\"\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Mostrar algunas reconstrucciones (primeras n_show del conjunto X)\n",
        "    n_show = int(n_show)\n",
        "    orig = np.asarray(X, dtype=np.float32)[:n_show]\n",
        "    recons = rbm.reconstruct(orig)\n",
        "\n",
        "    # Clip para visualizar mejor en [0,1]\n",
        "    recons = np.clip(recons, 0.0, 1.0)\n",
        "    orig_disp = np.clip(orig, 0.0, 1.0)\n",
        "\n",
        "    plt.figure(figsize=(n_show * 2, 4))\n",
        "    for i in range(n_show):\n",
        "        ax = plt.subplot(2, n_show, i + 1)\n",
        "        plt.imshow(orig_disp[i].reshape(28,28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            ax.set_title('Originals')\n",
        "\n",
        "        ax = plt.subplot(2, n_show, n_show + i + 1)\n",
        "        plt.imshow(recons[i].reshape(28,28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            ax.set_title('Reconstructions')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af85bc4d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training h784_e5 ---\n",
            "Epoch   1/5 - recon_error: 5.091648\n",
            "Epoch   1/5 - recon_error: 5.091648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ignac\\AppData\\Local\\Temp\\ipykernel_8284\\145431814.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-x)) # es la logistica pero toma forma de sigmoide\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   2/5 - recon_error: 396389856.000000\n"
          ]
        }
      ],
      "source": [
        "# Experimentos: entrenar 9 modelos (3 tamaños ocultos x 3 números de épocas)\n",
        "hidden_sizes = [28*28,500, 300, 100,50]\n",
        "epochs_list = [5, 10, 20,50,100]\n",
        "results = {}\n",
        "n_show = 10\n",
        "\n",
        "for n_hidden in hidden_sizes:\n",
        "    for epochs in epochs_list:\n",
        "        key = f\"h{n_hidden}_e{epochs}\"\n",
        "        print(f\"--- Training {key} ---\")\n",
        "        rbm = RBM(28 * 28, n_hidden, lr=0.01)\n",
        "        errs = rbm.train_loop(X, max_epochs=epochs, batch_size=128, tol=1e-5, verbose=True)\n",
        "        results[key] = {'errors': [float(e) for e in errs]}\n",
        "        show_recon_and_error(rbm, X, errs, n_show=n_show, title_prefix=key)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
