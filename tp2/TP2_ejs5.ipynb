{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "79d44e76",
      "metadata": {
        "id": "79d44e76"
      },
      "source": [
        "# Redes Neuronales - TP2\n",
        "## Ej 5\n",
        "\n",
        "Siguiendo el trabajo de Hinton y Salakhutdinov (2006), entrene una máquina restringida\n",
        "de Boltzmann con imágenes de la base de datos MNIST. Muestre el error de\n",
        "recontruccion durante el entrenamiento, y ejemplos de cada uno de los dígitos\n",
        "reconstruidos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b0b95d",
      "metadata": {},
      "source": [
        "La MRB se puede pensar como una red feedfoward de 4 capas, en vez de pensarla como una de 2 capas con pesos simétricos. $v$ es la capa visible y $h$ es la oculta, y tenemos un $\\hat{v}$ y $\\hat{h}$, que son las estimaciones. \n",
        "\n",
        "Las fórmulas que hay que seguir son las siguientes:\n",
        "\n",
        "$$\n",
        "m_i = \\text{pixel}_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_i \\sim  \\mathcal{N}(m_i , 1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "P_r (j) = g(\\sum _i w_{ij} v_i + b_j) \\quad \\quad g (x) = \\frac{1}{1+e^{-x}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "h(j) = \n",
        "\\begin{cases}\n",
        "1 \\quad \\text{con probabilidad} \\quad P_r \n",
        "\\\\\n",
        "0 \\quad  \\text{con probabilidad}\\quad 1-P_r \n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{guardar} \\quad (v_i , h_j)_{data}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e778c1",
      "metadata": {},
      "source": [
        "Luego se repite pero para obtener las estimaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f56d151",
      "metadata": {},
      "source": [
        "$$\n",
        "m_i = \\sum _j w_{ij} v_i + b_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{v}_i \\sim  \\mathcal{N}(m_i , 1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "P_r (j) = g(\\sum _i w_{ij} v_i + b_j) \\quad \\quad g (x) = \\frac{1}{1+e^{-x}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{h}(j) = \n",
        "\\begin{cases}\n",
        "1 \\quad \\text{con probabilidad} \\quad P_r \n",
        "\\\\\n",
        "0 \\quad  \\text{con probabilidad}\\quad 1-P_r \n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{guardar} \\quad (v_i , h_j)_{reconstruccion}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5604ab05",
      "metadata": {},
      "source": [
        "Al final se hace\n",
        "\n",
        "$$\n",
        "\\Delta w_{ij}= \\eta ( \\left\\langle v_i h_j \\right\\rangle - \\left\\langle \\hat{v_i} \\hat{h_j} \\right\\rangle)\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_i= \\eta ( \\left\\langle v_i \\right\\rangle - \\left\\langle \\hat{v_i} \\right\\rangle)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Delta w_{ij}= \\eta ( \\left\\langle v_i h_j \\right\\rangle - \\left\\langle \\hat{v_i} \\hat{h_j} \\right\\rangle)\n",
        "$$\n",
        "$$\n",
        "b_j= \\eta ( \\left\\langle  h_j \\right\\rangle - \\left\\langle  \\hat{h_j} \\right\\rangle)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a63bb448",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4c5441",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "class RBM:\n",
        "    \"\"\"\n",
        "    RBM con visibles gaussianas (sigma=1) y ocultas binarias.\n",
        "\n",
        "    He separado los pasos en métodos pequeños para que puedas completarlos o\n",
        "    modificar su comportamiento más fácilmente:\n",
        "      - v_to_h(v, sample=True): cálculo de P(h|v) y opcionalmente muestreo\n",
        "      - h_to_v(h, sample=True): cálculo de la media de v|h y opcionalmente muestreo\n",
        "      - compute_gradients(v0, h0_prob, v1, h1_prob): calcular dW, db, dc (por defecto CD-1)\n",
        "      - train_loop(data, ...): superloop que itera por épocas/batches y para cuando converge\n",
        "\n",
        "    Las implementaciones por defecto realizan CD-1; puedes reimplementar cualquiera\n",
        "    de los métodos si prefieres otro comportamiento.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_visible, n_hidden, lr=0.1, rng=None):\n",
        "        self.n_visible = int(n_visible)\n",
        "        self.n_hidden = int(n_hidden)\n",
        "        self.lr = float(lr)\n",
        "        self.rng = np.random.RandomState(None) if rng is None else rng\n",
        "\n",
        "        # pesos: forma (n_visible, n_hidden)\n",
        "        self.W = 0.01 * self.rng.randn(self.n_visible, self.n_hidden).astype(np.float32)\n",
        "        # sesgos visibles y ocultos\n",
        "        self.b = np.zeros(self.n_visible, dtype=np.float32)\n",
        "        self.c = np.zeros(self.n_hidden, dtype=np.float32)\n",
        "\n",
        "    def v_to_h(self, pixeles, sample=True):\n",
        "        \"\"\"Paso v -> h.\n",
        "        Entrada:\n",
        "          v: array (batch, n_visible)\n",
        "          sample: si True, devuelve también muestras binarias de h; si False solo probs\n",
        "        Salida:\n",
        "          h_prob: P(h=1|v) (batch, n_hidden)\n",
        "          h_samp: muestras binarias (batch, n_hidden) o None si sample=False\n",
        "\n",
        "        Nota: por defecto implementado como en las fórmulas: P(h_j=1|v)=sigm(v W + c)\n",
        "        \"\"\"\n",
        "        m = np.asarray(pixeles, dtype=np.float32)  # media = pixel_i\n",
        "        # sample visibles gaussianos con varianza 1\n",
        "        v = m + self.rng.randn(*m.shape).astype(np.float32) # la gaussiana tiene varianza 1 y media m. \n",
        "\n",
        "        P_r = np.dot(v, self.W) + self.c  # es la activación de la capa de ocultas, pero las usamos como probabilidades para la bernoulli pq son binarias\n",
        "        h_prob = _sigmoid(P_r)\n",
        "        h_samp = None\n",
        "        if sample: # esto es para que genere las muestras binarias de h, que al final las queremos para entrenar\n",
        "            h_samp = (self.rng.rand(*h_prob.shape) < h_prob).astype(np.float32)\n",
        "        return h_prob, h_samp\n",
        "\n",
        "    def h_to_v(self, h, sample=True):\n",
        "        \"\"\"Paso h -> v.\n",
        "        Entrada:\n",
        "          h: array (batch, n_hidden) -- puede ser probabilidades o muestras\n",
        "          sample: si True, devuelve muestras gaussianas v ~ N(mean, 1); si False solo la media\n",
        "        Salida:\n",
        "          v_mean: media de la distribución (batch, n_visible)\n",
        "          v_samp: muestras (batch, n_visible) o None si sample=False\n",
        "\n",
        "        Nota: por defecto asumimos var=1: mean = h W^T + b\n",
        "        \"\"\"\n",
        "        h = np.asarray(h, dtype=np.float32)\n",
        "        m = np.dot(h, self.W.T) + self.b\n",
        "        v = m + self.rng.randn(*m.shape).astype(np.float32) # esto es para muestrear v con varianza 1 y media m\n",
        "        v_samp = None\n",
        "        if sample: # esto es para que genere las muestras gaussianas de v, que al final las queremos para entrenar\n",
        "            v_samp = v + self.rng.randn(*v.shape).astype(np.float32)\n",
        "        return v, v_samp\n",
        "\n",
        "    def gradiente(self, v0, h0_samp, v1, h1_samp):\n",
        "        \"\"\"Calcula los gradientes dW, db, dc dados los estadísticos positivos (v0,h0_prob)\n",
        "        y negativos (v1, h1_prob).\n",
        "\n",
        "        Por defecto aplica la regla CD-1 mostrada en el notebook:\n",
        "          dW = lr * ( <v h>_data - <v h>_recon )\n",
        "          db = lr * ( <v>_data - <v>_recon )\n",
        "          dc = lr * ( <h>_data - <h>_recon )\n",
        "\n",
        "        Devuelve: dW, db, dc (ya escalados por lr)\n",
        "        \"\"\"\n",
        "        # Asegurar arrays y dimensiones\n",
        "        v0 = np.asarray(v0, dtype=np.float32)\n",
        "        v1 = np.asarray(v1, dtype=np.float32)\n",
        "        h0_samp = np.asarray(h0_samp, dtype=np.float32)\n",
        "        h1_samp = np.asarray(h1_samp, dtype=np.float32)\n",
        "\n",
        "        batch = float(v0.shape[0]) # la dimensión del batch\n",
        "        pos_assoc = np.dot(v0.T, h0_samp) / batch # <v h>_data\n",
        "        neg_assoc = np.dot(v1.T, h1_samp) / batch # <v h>_recon\n",
        "\n",
        "    # es practicamente lo que está en als ecuaciones\n",
        "\n",
        "        dW = self.lr * (pos_assoc - neg_assoc)\n",
        "        db = self.lr * (v0.mean(axis=0) - v1.mean(axis=0))\n",
        "        dc = self.lr * (h0_samp.mean(axis=0) - h1_samp.mean(axis=0))\n",
        "        return dW, db, dc\n",
        "\n",
        "    def reconstruct(self, v):\n",
        "        \"\"\"Reconstrucción determinística: v -> h_prob -> v_mean.\"\"\"\n",
        "        h_prob, _ = self.v_to_h(v, sample=False)\n",
        "        v_mean, _ = self.h_to_v(h_prob, sample=False)\n",
        "        return v_mean\n",
        "\n",
        "    def train_loop(self, data, max_epochs=50, batch_size=64, tol=1e-4, verbose=True):\n",
        "        \"\"\"Superloop de entrenamiento que itera por épocas y batches.\n",
        "\n",
        "        Logística:\n",
        "          - data: array (N, n_visible) o iterador/ генератор de minibatches\n",
        "          - max_epochs: número máximo de épocas\n",
        "          - batch_size: tamaño de minibatch si `data` es un array\n",
        "          - tol: criterio de parada si la variación del error por época es menor que tol\n",
        "\n",
        "        Comportamiento: por cada minibatch realiza los pasos v->h, h->v, calcula gradientes\n",
        "        usando `compute_gradients` y actualiza parámetros. Devuelve lista de errores por época.\n",
        "        \"\"\"\n",
        "        # Aceptar `data` como array numpy\n",
        "        X = np.asarray(data, dtype=np.float32)\n",
        "        N = X.shape[0]\n",
        "        n_batches = max(1, N // batch_size)\n",
        "\n",
        "        epoch_errors = []\n",
        "        prev_epoch_err = None\n",
        "\n",
        "        for epoch in range(1, max_epochs + 1):\n",
        "            perm = self.rng.permutation(N)\n",
        "            epoch_err = 0.0\n",
        "            for b in range(n_batches):\n",
        "                idx = perm[b * batch_size:(b + 1) * batch_size]\n",
        "                v0 = X[idx]\n",
        "\n",
        "                # Paso 1: v -> h (obtener probabilidades y muestras)\n",
        "                h0_prob, h0_samp = self.v_to_h(v0)\n",
        "\n",
        "                # v0 con h0 sería el vector de data\n",
        "\n",
        "                # Paso 2: h -> v (reconstrucción)\n",
        "                v1_mean, v1_samp = self.h_to_v(h0_samp)\n",
        "\n",
        "                # Paso 3: v(recon) -> h (prob negative)\n",
        "                h1_prob, h1_samp = self.v_to_h(v1_samp)\n",
        "\n",
        "                # Paso 4: calcular gradientes\n",
        "                dW, db, dc = self.gradiente(v0, h1_samp, v1_samp, h1_samp)  # acá uso las samples pq la capa intermedia es binaria\n",
        "\n",
        "                # Aplicar actualizaciones\n",
        "                self.W += dW\n",
        "                self.b += db\n",
        "                self.c += dc\n",
        "\n",
        "                # Acumular error del batch (MSE entre v0 y v1_mean)\n",
        "                batch_err = np.mean((v0 - v1_mean) ** 2)\n",
        "                epoch_err += batch_err\n",
        "\n",
        "            epoch_err /= float(n_batches)\n",
        "            epoch_errors.append(epoch_err)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch:3d}/{max_epochs} - recon_error: {epoch_err:.6f}\")\n",
        "\n",
        "            # Criterio de convergencia: cambio pequeño en el error promedio por época\n",
        "            if prev_epoch_err is not None and abs(prev_epoch_err - epoch_err) < tol:\n",
        "                if verbose:\n",
        "                    print(\"Converged (tol reached). Stopping training.\")\n",
        "                break\n",
        "            prev_epoch_err = epoch_err\n",
        "\n",
        "        return epoch_errors\n",
        "\n",
        "\n",
        "# Ejemplo de uso rápido (prueba de humo)\n",
        "if __name__ == \"__main__\":\n",
        "    rbm = RBM(28 * 28, 128, lr=0.01)\n",
        "    X = np.random.randn(500, 28 * 28).astype(np.float32)\n",
        "    errs = rbm.train_loop(X, max_epochs=3, batch_size=128, verbose=True)\n",
        "    print('Finished, epoch errors:', errs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
