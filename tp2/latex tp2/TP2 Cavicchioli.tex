% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the ``article'' class.
% See ``book'', ``report'', ``letter'' for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}

% para los cuadritos en links
%\usepackage[linkbordercolor={0 0 1}, citebordercolor={0 1 0}, urlbordercolor={1 0 0}]{hyperref}
\usepackage[colorlinks=true, linkcolor=black, citecolor=green, urlcolor=red]{hyperref} % solo resalta
\usepackage[spanish]{babel}



%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The ``real'' document content comes below...

\title{Trabajo práctico 1\\ Redes Neuronales y Aprendizaje Profundo}
\author{Ignacio Ezequiel Cavicchioli\\Padrón 109428\\icavicchioli@fi.uba.ar}
\date{10/9/2025} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed



\begin{document}
\maketitle

\tableofcontents



\section{Introducción}

Este documento presenta el desarrollo de las consignas del trabajo práctico N°2 de la materia de \textbf{Redes Neuronales y Aprendizaje Profundo}.  El código correspondiente fue realizado en  \textit{Jupyter notebooks}, \textit{Python}, adjuntados a la entrega en formato PDF. Toda imagen o implementación requeridas para el análisis se explicitarán en el presente archivo, por lo que la lectura del código en sí queda a discreción del lector. La teoría relevante será presentada y discutida en la sección pertinente.


\newpage

\section{Ejercicio 1}

\subsection{Consignas}

Implemente un perceptrón simple que aprenda la función lógica AND y la función lógica OR, de 2 y de 4 entradas. Muestre la evolución del error durante el entrenamiento. Para el caso de 2 dimensiones, grafique la recta discriminadora y todos los vectores de entrada de la red.

\subsection{Desarrollo}

Lo primero que se hizo fue generar el dataset para entrenar los perceptrones, que no es más que la tabla de verdad de la función lógica que se quiere aprender.

Las figuras \ref{fig:and2err}, \ref{fig:and4},\ref{fig:or2err} y \ref{fig:or4} muestran el error en función de la iteración (de entrenamiento) para los perceptrones que emulan las funciones AND y OR de 2 y 4 entradas. Como se discutió en la teórica, el perceptrón es capaz de aprender estas funciones lógicas simples, por lo que el error final del entrenamiento es cero.

La figura \ref{fig:orfront} muestra la frontera de decisión del perceptrón que aprendió la función OR, y la \ref{fig:frontAND} muestra la frontera para la compuerta AND. Los puntos del mismo color son de la misma clase (0 o 1), y son adecuadamente segregados por la frontera. Contrario a SVM, el perceptrón simple no tiene la distancia desde la frontera hasta las muestras en su función de costo, por lo que la recta que separa las clases no es única.

Para encontrar la fórmula de la recta que hace de frontera de decisión se puede partir de la expresión de la sumatoria de los $X$ con sus respectivos pesos y el bias, y suponer que $Y=0$, lo que significa que estás parado sobre la frontera, tu muestra no es ni de una clase ni de la otra.

$$
X1 \cdot w1 + X2 \cdot w2 + b = 0
$$

Si suponemos que $X1$ es nuestra abscisa que vamos a barrer y $X2$ es la ordenada, la recta toma la forma de:

$$
X2 = \frac{w1 \cdot X1 + b}{-w2}
$$

Y entonces $m=-w1/w2 $ y b = $-b/w2$

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej1/AND2err}
	\caption[]{Error de entrenamiento en el tiempo para compuerta AND de 2 entradas}
	\label{fig:and2err}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej1/AND4}
	\caption[]{Error de entrenamiento en el tiempo para compuerta AND de 4 entradas}
	\label{fig:and4}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej1/OR2err}
	\caption[]{Error de entrenamiento en el tiempo para compuerta OR de 2 entradas}
	\label{fig:or2err}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej1/OR4}
	\caption[]{Error de entrenamiento en el tiempo para compuerta OR de 4 entradas}
	\label{fig:or4}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej1/ANDFRONT}
	\caption[]{Frontera encontrada para compuerta AND de 2 entradas}
	\label{fig:frontAND}
\end{figure}



\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej1/ORFRONT}
	\caption[]{Frontera encontrada para compuerta OR de 2 entradas}
	\label{fig:orfront}
\end{figure}


\subsection{Análisis}

Los perceptrones lograron aprender sus funciones lógicas designadas sin error y en una cantidad finita de iteraciones del algoritmo. Esto es el resultado esperado, se sabe que puede aprender este tipo particular de problemas.

Una parte interesante del ejercicio fue el armado del algoritmo de gradiente descendiente, regla por la cual el perceptrón va moviendo su frontera de decisión. Los gráficos muestran un error decreciente en el tiempo, que a veces aumenta espuriamente para luego ser corregido, porque el gradiente descendiente va actualizando los pesos en función del error, intentando moverse hacia fronteras que predigan correctamente.

La obtención de la recta que hace de frontera en 2D solo requirió del desarrollo de fórmulas hecho en la sección previa.

En líneas generales, el ejercicio sirvió como una buena introducción a los perceptrones y el concepto de gradiente descendiente.


\clearpage

\section{Ejercicio 2}

\subsection{Consignas}

Determine numéricamente cómo varía la capacidad del perceptrón simple en función del número de patrones enseñados.


\subsection{Desarrollo}

La capacidad de un perceptrón está definida en la p.111 (expresión 5.63) del libro ``Introduction to the theory of neural computation''. La idea es básicamente determinar la cantidad de patrones aleatorios (con etiqueta aleatoria) para el cual existe una frontera. En el libro se demuestra que la capacidad de almacenamiento máxima del perceptrón converge a $p_{max} = 2N$, con $p$ la cantidad de patrones y N, el número de pesos sinápticos que tiene el perceptrón (o cantidad de unidades de entrada), con $N \to \inf$.

Para corroborar experimentalmente esta aserción, se entrenó un perceptrón de un N particular con cada vez más patrones y se registró el resultado, para luego graficarlo.

En pocas palabras, el código genera conjuntos de patrones aleatorios y entrena un perceptrón simple para clasificarlos. Repite el experimento múltiples veces\footnote{Para un cierto $p$ se hacen varios experimentos y se promedia, por eso hay una cantidad de unidades de entrada y de repeticiones} y calcula con qué frecuencia el perceptrón logra aprender sin errores (es decir, converge). Luego grafica la probabilidad de éxito en función de $\alpha = \frac{p}{N}$, donde $p$ es la cantidad de patrones y $N$ la dimensión de entrada, mostrando así la capacidad del perceptrón. Se espera que la recta vertical $p/N = 2$ interseque la gráfica en $p=1/2$ (como en la literatura citada).


los resultados para $N=30$ se pueden ver en la figura \ref{fig:capacidad}, y para $N=10$, en la \ref{fig:N10}. No se hizo con más unidades de entrada porque condiciona mucho el tiempo de ejecución y la cantidad de epochs necesarias para lograr un buen resultado.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej2/capacidad}
	\caption[]{Ensayo de capacidad $N=30$}
	\label{fig:capacidad}
\end{figure}

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej2/N10}
	\caption[]{Ensayo de capacidad $N=10$}
	\label{fig:N10}
\end{figure}

\newpage

\subsection{Análisis}

La figura \ref{fig:capacidad} muestra que la capacidad de memorizar (el eje Y) del perceptrón es perfecta (vale 1) y, a medida que aumenta la cantidad de patrones, decrece hasta hacerse cero. El cruce por el punto $p/N =2$ se da aproximadamente en $p\simeq 0.5$, resultado coincidente con la teoría del libro mencionado.

La transición, o flanco, no es perfectamente vertical, sino que decrece casi como una función sigmoidea. Esto es debido al uso de valores de $N$ relativamente pequeños y el promediado empleado. A forma comparativa se incluye la imagen \ref{fig:N10}, de $N=10$. En esta se ve que la capacidad tarda más en converge a cero (en $N=30$ se hace cero en $2.5$, y en $N=10$, en $3.5$), y la intersección del límite teórico es más cercana a $0.6$ y no $0.5$.

Resumiendo, aún con las limitaciones en la cantidad de unidades de entrada, se empieza a divisar una tendencia que parece coincidir con la teoría.



\clearpage

\section{Ejercicio 3}

\subsection{Consignas}

Implemente un perceptrón multicapa que aprenda la función lógica XOR de 2 y de 4 entradas (utilizando el algoritmo Backpropagation y actualizando en batch). Muestre cómo evoluciona el error durante el entrenamiento.

\subsection{Desarrollo}

En este inciso se entrenó un MLP para que aprendan la XOR, pero se hizo con 2, 4 y 10 neuronas en la capa oculta. La idea general fue crear las tablas de verdad de las funciones XOR de 2 y 4 entradas y entrenar MLP´s, registrando el error en función de las epochs.

No se usa 1 solo perceptrón porque su funcionamiento es incompatible con la función XOR. El problema fundamental es que no existen pesos que permitan aprender perfectamente el problema. Voy a intentar una pequeña demostración basándome en las fórmulas básicas del perceptrón y suponiendo una toma de decisión por alguna función que categorice 0 y 1.

La explicación de a continuación debería ayudar a entender que la limitación se puede hasta pensar de forma geométrica, no es dependiente del entrenamiento o función de activación\footnote{Excepto en el caso de usar una función de activación no monótona. Se me ocurrió definir, por elegir valores simples, una función que vale 0 para todo valor fuera del intervalo $(\tfrac{1}{10}, \tfrac{3}{8})$ y 1 dentro de él. Con esta nueva función, el perceptrón podría aprender la función XOR, ya que existen, por ejemplo, $\alpha = \tfrac{1}{2}$ que cumplen: para $(0,0)\!\to\!h=0\!\to\!y=0$, para $(1,0)$ y $(0,1)\!\to\!h=\tfrac{1}{4}\!\to\!y=1$, y para $(1,1)\!\to\!h=\tfrac{1}{2}\!\to\!y=0$. Esta función no permite resolver la AND o la OR, fue revisado en un visor de geometría 3D.}.


$$
y = \begin{cases}
	1 \; \text{si} \; h > 1/2 \\
	0 \; \text{si} \;h \leq 1/2
\end{cases}
$$

donde $h$ es la salida del perceptrón simple:

$$h = \alpha_1 x_1 + \alpha_2 x_2$$

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		x1 & x2 & h &y\\
		\hline
		0 & 0 & $\le1/2$&0\\
		\hline
		0 & 1 & $ > 1/2$&1\\
		\hline
		1 & 0 & $ > 1/2$&1\\
		\hline
		1 & 1 & $\le1/2$&0\\
		\hline
	\end{tabular}
\end{table}


Resolviendo el sistema de ecuaciones para $(0,0)$:

$$
0 \cdot \alpha_1 + 0 \cdot \alpha_2 = h \le 1/2
$$
Para $(0,1)$:
$$
0 \cdot \alpha_1 + 1 \cdot \alpha_2 = h \ge 1/2 \Rightarrow \alpha_2 \ge 1/2
$$
Para $(1,0)$:
$$
1 \cdot \alpha_1 + 0 \cdot \alpha_2 = h \ge 1/2 \Rightarrow \alpha_1 \ge  1/2
$$
Para $(1,1)$:
$$
1 \cdot \alpha_1 + 1 \cdot \alpha_2 = h \le 1/2 \Rightarrow \alpha_1 + \alpha_2 \le  1/2
$$

Sumando las dos desigualdades obtenemos
$$
\alpha_1 + \alpha_2 \ge 1,
$$
Lo cual contradice \(\alpha_1 + \alpha_2 \le \tfrac12\). Por lo tanto, no existen \(\alpha_1,\alpha_2\) que satisfagan todas las condiciones: el perceptrón simple no puede aprender la función XOR.
Ahora, cuando se agrega otro perceptrón en paralelo y se le da uno que decide en base a esta capa oculta, la XOR se vuelve ``aprendible''.



\subsubsection{2 Neuronas en capa oculta}

En este primer caso se utilizó un perceptrón multicapa con una capa oculta de dos neuronas, con el objetivo de que aprenda la función XOR.
Se trabajó tanto con la XOR de dos entradas como con la de cuatro entradas, manteniendo la misma arquitectura a fin de observar las diferencias en la capacidad de representación del modelo.

Para la XOR de dos entradas, la red logró converger correctamente, alcanzando un error cercano a cero y reproduciendo exactamente la tabla de verdad de la función.
En las Figuras~\ref{fig:xor2err} y~\ref{fig:xor2front} se muestra la evolución del error por época y la frontera de decisión obtenida, respectivamente, donde puede verse una separación clara entre las clases.


En contraste, al aplicar la misma arquitectura a la XOR de cuatro entradas, la red no consiguió reducir el error a cero, incluso habiendo hasta triplicado la cantidad de epochs. (Figura~\ref{fig:xor4err}).

Esto apunta a que dos neuronas ocultas no son suficientes para modelar la complejidad de la XOR de cuatro entradas, y se supone que se debe principalmente al incremento exponencial en cantidad de combinaciones posibles. La red ya no dispone de capacidad suficiente como para memorizar los patrones, siendo esto una manifestación práctica de la capacidad teórica analizada en el ejercicio anterior.


\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{"../imgs/ej3/2 neur int/xor2_err"}
	\caption{Error por época - XOR de 2 entradas - MLP 2 neuronas en capa oculta}
	\label{fig:xor2err}
\end{figure}

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{"../imgs/ej3/2 neur int/xor2_front"}
	\caption{Frontera de decisión - XOR de 2 entradas - MLP 2 neuronas en capa oculta}
	\label{fig:xor2front}
\end{figure}

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{"../imgs/ej3/2 neur int/xor4_err"}
	\caption{Error por época - XOR de 4 entradas - MLP 2 neuronas en capa oculta}
	\label{fig:xor4err}
\end{figure}

\begin{table}[h!]
	\centering
	\begin{tabular}{cccc}
		\hline
		\textbf{Entradas} & \textbf{Target} & \textbf{Salida sigmoide} & \textbf{Predicción (umbral 0.5)} \\
		\hline
		$[0\ 0]$ & 0 & 0.082 & 0 \\
		$[0\ 1]$ & 1 & 0.927 & 1 \\
		$[1\ 0]$ & 1 & 0.927 & 1 \\
		$[1\ 1]$ & 0 & 0.077 & 0 \\
		\hline
	\end{tabular}
	\caption{Resultados del perceptrón con dos entradas  - MLP 2 neuronas en capa oculta}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{cccc}
		\hline
		\textbf{Entradas} & \textbf{Target} & \textbf{Salida sigmoide} & \textbf{Predicción (umbral 0.5)} \\
		\hline
		$[-1\ -1\ -1\ -1]$ & 0 & 0.725 & 1 \\
		$[1\ -1\ -1\ -1]$ & 1 & 0.837 & 1 \\
		$[-1\ 1\ -1\ -1]$ & 1 & 0.701 & 1 \\
		$[1\ 1\ -1\ -1]$ & 0 & 0.036 & 0 \\
		$[-1\ -1\ 1\ -1]$ & 1 & 0.708 & 1 \\
		$[1\ -1\ 1\ -1]$ & 0 & 0.048 & 0 \\
		$[-1\ 1\ 1\ -1]$ & 0 & 0.045 & 0 \\
		$[1\ 1\ 1\ -1]$ & 0 & 0.009 & 0 \\
		$[-1\ -1\ -1\ 1]$ & 1 & 0.879 & 1 \\
		$[1\ -1\ -1\ 1]$ & 0 & 0.063 & 0 \\
		$[-1\ 1\ -1\ 1]$ & 0 & 0.030 & 0 \\
		$[1\ 1\ -1\ 1]$ & 0 & 0.029 & 0 \\
		$[-1\ -1\ 1\ 1]$ & 0 & 0.055 & 0 \\
		$[1\ -1\ 1\ 1]$ & 0 & 0.036 & 0 \\
		$[-1\ 1\ 1\ 1]$ & 0 & 0.011 & 0 \\
		$[1\ 1\ 1\ 1]$ & 0 & 0.027 & 0 \\
		\hline
	\end{tabular}
	\caption{Resultados del perceptrón con cuatro entradas  - MLP 2 neuronas en capa oculta}
\end{table}

\clearpage

\subsubsection{4 Neuronas en capa oculta}

En este caso se incrementó el número de neuronas en la capa oculta a cuatro, con el objetivo de mejorar la capacidad de representación del MLP.
Para la XOR de dos entradas, la red continúa convergiendo correctamente, alcanzando un error nulo y generando fronteras de decisión curvas (Figuras~\ref{fig:ecm2} y~\ref{fig:front2}). Esta curvatura parece ser causada por la complejidad de la red; mientras más unidades tiene la capa oculta, más grados de libertad tienen las fronteras.
Esto se refleja también en la tabla de resultados, donde todas las predicciones coinciden con los valores esperados.

Para la XOR de cuatro entradas, la red aún no logra converger a un error nulo (Figura~\ref{fig:ecm4}).
En la tabla correspondiente se puede ver que varias predicciones todavía difieren del target esperado. Se sigue confundiendo en la primer entrada.


\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{"../imgs/ej3/4 neur int/ecm2"}
	\caption{Error por época - XOR de 2 entradas - MLP 4 neuronas en capa oculta}
	\label{fig:ecm2}
\end{figure}

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{"../imgs/ej3/4 neur int/front2"}
	\caption{Frontera de decisión - XOR de 2 entradas - MLP 4 neuronas en capa oculta}
	\label{fig:front2}
\end{figure}

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{"../imgs/ej3/4 neur int/ecm4"}
	\caption{Error por época - XOR de 4 entradas - MLP 4 neuronas en capa oculta}
	\label{fig:ecm4}
\end{figure}



\begin{table}[h!]
	\centering
	\begin{tabular}{cccc}
		\hline
		\textbf{Entradas} & \textbf{Target} & \textbf{Salida sigmoide} & \textbf{Predicción (umbral 0.5)} \\
		\hline
		$[0\ 0]$ & 0 & 0.059 & 0 \\
		$[0\ 1]$ & 1 & 0.902 & 1 \\
		$[1\ 0]$ & 1 & 0.946 & 1 \\
		$[1\ 1]$ & 0 & 0.102 & 0 \\
		\hline
	\end{tabular}
	\caption{Resultados del perceptrón con dos entradas - MLP 4 neuronas en capa oculta}
\end{table}


\begin{table}[h!]
	\centering
	\begin{tabular}{cccc}
		\hline
		\textbf{Entradas} & \textbf{Target} & \textbf{Salida sigmoide} & \textbf{Predicción (umbral 0.5)} \\
		\hline
		$[-1\ -1\ -1\ -1]$ & 0 & 0.727 & 1 \\
		$[1\ -1\ -1\ -1]$ & 1 & 0.879 & 1 \\
		$[-1\ 1\ -1\ -1]$ & 1 & 0.697 & 1 \\
		$[1\ 1\ -1\ -1]$ & 0 & 0.033 & 0 \\
		$[-1\ -1\ 1\ -1]$ & 1 & 0.863 & 1 \\
		$[1\ -1\ 1\ -1]$ & 0 & 0.054 & 0 \\
		$[-1\ 1\ 1\ -1]$ & 0 & 0.039 & 0 \\
		$[1\ 1\ 1\ -1]$ & 0 & 0.010 & 0 \\
		$[-1\ -1\ -1\ 1]$ & 1 & 0.696 & 1 \\
		$[1\ -1\ -1\ 1]$ & 0 & 0.028 & 0 \\
		$[-1\ 1\ -1\ 1]$ & 0 & 0.058 & 0 \\
		$[1\ 1\ -1\ 1]$ & 0 & 0.003 & 0 \\
		$[-1\ -1\ 1\ 1]$ & 0 & 0.034 & 0 \\
		$[1\ -1\ 1\ 1]$ & 0 & 0.007 & 0 \\
		$[-1\ 1\ 1\ 1]$ & 0 & 0.003 & 0 \\
		$[1\ 1\ 1\ 1]$ & 0 & 0.003 & 0 \\
		\hline
	\end{tabular}
	\caption{Resultados del perceptrón con cuatro entradas - MLP 4 neuronas en capa oculta}
\end{table}

\clearpage

\subsubsection{10 Neuronas en capa oculta}

Al aumentar el número de neuronas en la capa oculta a diez, se observa un cambio significativo en el comportamiento del MLP.

Para la XOR de dos entradas, la red sigue convergiendo sin problemas (Figura~\ref{fig:2ecm10} y \ref{fig:front10}). Las fronteras de decisión siguen siendo curvas, cambiando en cada ejecución, pudiendo ser más o menos recta. Acá ya se entiende que la red tiene más que suficiente capacidad para aprender todos los patrones, y su espacio de soluciones posibles es grande y variado (relacionado con la cantidad de grados de libertad que le da la capa oculta de 10 unidades), por eso la gran variedad de fronteras.

Para la XOR de cuatro entradas, a diferencia de los casos anteriores con 2 o 4 neuronas, ahora la red logra aprender correctamente todos los patrones, alcanzando un error nulo (Figura~\ref{fig:4ecm10}). Esto confirma la sospecha de que las dificultades de aprendizaje de la XOR de 4 entradas en los experimentos anteriores estaba relacionada con la capacidad de la red, que se encontraba limitada por la cantidad de neuronas. De alguna manera, el aumento de grados de libertad hace que los subespacios de parámetros que son compatibles con el set de datos se vuelvan cada vez más accesibles, hasta que es lo suficientemente accesible como para que siempre haya error nulo post-aprendizaje.

El cuadro \ref{cuadroXOR4_010} muestra la tabla de verdad enseñada y aprendida. Nótese que, contrario a los casos para menos neuronas, la red aprendió bien la lógica, y esto se nota en que las salidas de la sigmoidea son o muy cercanas a 0, o muy cercanas a 1, indicando poca ``confusión'' (siempre está muy seguro de su decisión).

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej3/10neur/2ecm}
	\caption{Error por época - XOR de 2 entradas - MLP 10 neuronas en capa oculta}
	\label{fig:2ecm10}
\end{figure}

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej3/10neur/front}
	\caption{Frontera de decisión - XOR de 2 entradas - MLP 10 neuronas en capa oculta}
	\label{fig:front10}
\end{figure}

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej3/10neur/4ecm}
	\caption{Error por época - XOR de 4 entradas - MLP 10 neuronas en capa oculta}
	\label{fig:4ecm10}
\end{figure}





\begin{table}[h!]
	\centering
	\begin{tabular}{cccc}
		\hline
		\textbf{Entradas} & \textbf{Target} & \textbf{Salida sigmoide} & \textbf{Predicción (umbral 0.5)} \\
		\hline
		$[0\ 0]$ & 0 & 0.051 & 0 \\
		$[0\ 1]$ & 1 & 0.948 & 1 \\
		$[1\ 0]$ & 1 & 0.942 & 1 \\
		$[1\ 1]$ & 0 & 0.057 & 0 \\
		\hline
	\end{tabular}
	\caption{Resultados del perceptrón con dos entradas y 10 neuronas ocultas.}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{cccc}
		\hline
		\textbf{Entradas} & \textbf{Target} & \textbf{Salida sigmoide} & \textbf{Predicción (umbral 0.5)} \\
		\hline
		$[-1\ -1\ -1\ -1]$ & 0 & 0.113 & 0 \\
		$[1\ -1\ -1\ -1]$ & 1 & 0.950 & 1 \\
		$[-1\ 1\ -1\ -1]$ & 1 & 0.931 & 1 \\
		$[1\ 1\ -1\ -1]$ & 0 & 0.028 & 0 \\
		$[-1\ -1\ 1\ -1]$ & 1 & 0.912 & 1 \\
		$[1\ -1\ 1\ -1]$ & 0 & 0.018 & 0 \\
		$[-1\ 1\ 1\ -1]$ & 0 & 0.062 & 0 \\
		$[1\ 1\ 1\ -1]$ & 0 & 0.003 & 0 \\
		$[-1\ -1\ -1\ 1]$ & 1 & 0.921 & 1 \\
		$[1\ -1\ -1\ 1]$ & 0 & 0.042 & 0 \\
		$[-1\ 1\ -1\ 1]$ & 0 & 0.017 & 0 \\
		$[1\ 1\ -1\ 1]$ & 0 & 0.017 & 0 \\
		$[-1\ -1\ 1\ 1]$ & 0 & 0.012 & 0 \\
		$[1\ -1\ 1\ 1]$ & 0 & 0.010 & 0 \\
		$[-1\ 1\ 1\ 1]$ & 0 & 0.006 & 0 \\
		$[1\ 1\ 1\ 1]$ & 0 & 0.052 & 0 \\
		\hline
	\end{tabular}
	\caption{Resultados del perceptrón con cuatro entradas y 10 neuronas ocultas.}
	\label{cuadroXOR4_010}
\end{table}



\subsection{Análisis}

El análisis ya se desarrolló casi completamente en cada subsección. Lo que se puede afirmar es que la función XOR de 2 entradas es relativamente simple de aprender para un MLP (3 perceptrones mínimo; 2 en la capa oculta), pero no tanto la XOR de 4 entradas, que requirió de una capa oculta bastante más grande (10 neuronas). Esto demuestra que siguen existiendo limitaciones a la cantidad de patrones que puede aprender una cierta red de perceptrones, y que parecería aumentar con la cantidad de neuronas agregadas.

Algo que no se mencionó es que los errores tienen forma de exponenciales decrecientes (aunque a veces deformes), que parecería normal para estos casos. Algunos entrenamientos presentaron como puntos silla, lo que indicaría que el algoritmo se encontró con una región de gradiente muy pequeño, como la figura \ref{fig:xor2err}, que se quedó varias epochs en 0.25 de ECM.

otro tema interesante para mencionar es que, como el método de gradiente usado no es estocástico, el resultado final del aprendizaje es determinístico. Esto trae como consecuencia que, si el espacio de soluciones de error mínimo es pequeño, sería difícil encontrar justo la condición inicial que caiga en el (más bien, no se encontró para menos de 10 neuronas). Claramente, el uso de más neuronas da más grados de libertad a la red, y más capacidad, y todo esto parecería ser acompañado por un incremento en accesibilidad para las regiones que son solución al problema.
Volviendo al punto anterior, es posible que haya soluciones en espacios de menor dimensión que el de la red de 10 neuronas en la capa oculta, pero no fueron accedidos por la regla de aprendizaje. Opino que existe la posibilidad de que la función XOR de 4 entradas se pueda aprender con menos neuronas  pero con el uso de alguna regla que meta estocacidad en el sistema, como el uso de un gradiente estocástico. Este punto se retoma en el inciso de \textit{simulated annealing}, donde se comprueba esta hipótesis. También sería interesante saber si un perceptrón simple de 4 entradas con función de activación no monótona podría aprenderlo (intuyo que si).

\newpage

\section{Ejercicio 4}

\subsection{Consignas}

a - Implemente una red con aprendizaje Backpropagation que aprenda la siguiente función:
$$
f (x , y , z)= \sin(x)+\cos(y)+z
$$

Donde: $x$ e $y$ $\in [0,2 \pi]$ y $z \in [-1,1]$.

Para ello construya un conjunto de datos de entrenamiento y un conjunto de evaluación. Muestre la evolución del error de entrenamiento y de evaluación en función de las épocas de entrenamiento.

b - Estudie la evolución de los errores durante el entrenamiento de una red con una capa oculta de 30 neuronas cuando el conjunto de entrenamiento contiene 40 muestras. ¿Que ocurre si el \textit{minibatch} tiene tamaño 40? ¿Y si tiene tamaño 1?

\subsection{Desarrollo}

El conjunto de datos se generó sin mayores problemas, fue solo evaluar la función $f(x,y,z)$ en muchos puntos.

Los datasets de entrenamiento y testeo se separaron 70/30 (56000 muestras de entrenamiento) y se entrenó variando el tamaño de \textit{minibatch}. El uso de \textit{minibatches} implica la utilización de un gradiente estocástico, que es más rápido (menos datos, menos cuentas) y ayuda con la convergencia ya que la aleatoriedad introducida por no usar todas las muestras causa que no siempre se caiga en las mismas cuencas de la función de costo.

En esta implementación se separaron los datos en tantos batches como corresponde según el tamaño, y todos se mostraron 10 veces en un orden aleatorio (pero nunca seguidos, todos se ostraban antes de volver a seleccionar uno ya mostrado). Se intentó balancear la cantidad de ``veces'' (en el código, ``veces'' sería la cantida de veces que se muestran todos los \textit{minibatches}) para que sea justo entre modelos de tamaños de \textit{minibatch} diferentes.

Los MLPs son de 30 neuronas en una sola capa oculta. Se mantuvieron las mismas condiciones para los 4 experimentos, para no sesgar los resultados. La validación se hizo con todo el set de testeo.

\newpage

\subsection{Inciso a}

\subsubsection{\textit{minibatch} de 40 muestras}
Las figuras \ref{fig:batch40ecm} y \ref{fig:batch40ajustefuncion} corresponden a un \textit{minibatch} de 40 muestras.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/batch40ECM}
	\caption{ECM de entrenamiento y validación - \textit{minibatch} de 40 muestras, 10 veces, 1 epoch}
	\label{fig:batch40ecm}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/batch40AjusteFuncion}
	\caption{Predicción del modelo en base a (X,Y,Z) - \textit{minibatch} de 40 muestras, 10 veces, 1 epoch}
	\label{fig:batch40ajustefuncion}
\end{figure}

\newpage


\subsubsection{\textit{minibatch} de 100 muestras}

Las figuras \ref{fig:batch100ecm} y \ref{fig:batch100ajustefuncion} corresponden a un \textit{minibatch} de 100 muestras. L
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/batch100ECM}
	\caption{ECM de entrenamiento y validación - \textit{minibatch} de 100 muestras}
	\label{fig:batch100ecm}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/batch100AjusteFuncion}
	\caption{Predicción del modelo en base a (X,Y,Z) - \textit{minibatch} de 100 muestras}
	\label{fig:batch100ajustefuncion}
\end{figure}

\newpage
\subsubsection{\textit{minibatch} de 1000 muestras}

Las figuras \ref{fig:batch1000ecm1} y \ref{fig:batch1000ajustefuncion1} corresponden a un \textit{minibatch} de 1000 muestras.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/batch1000ECM1}
	\caption{ECM de entrenamiento y validación - \textit{minibatch} de 1000 muestras}
	\label{fig:batch1000ecm1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/batch1000AjusteFuncion1}
	\caption{Predicción del modelo en base a (X,Y,Z) - \textit{minibatch} de 1000 muestras}
	\label{fig:batch1000ajustefuncion1}
\end{figure}
\newpage
\subsubsection{Análisis}

El incremento del tamaño de \textit{minibatch} manteniendo la cantidad de iteraciones constante hizo que se tardara mucho más en entrenar para los \textit{minibatches} de 1000 que para el de 100 y el de 40. Claramente se entrenó con más iteraciones que las necesarias, evidente en el hecho de que el ECM converge para la iteración 4000 en los 3 casos.

El ECm de entrenamiento es más ruidoso con el \textit{minibatch} más chico, lo que tiene sentido porque es el que tiene el gradiente más estocástico, de menos muestras.

Los 3 casos mostrados predicen con una forma aproximadamente igual, aunque considero que el mejor predictor es el de \textit{minibatch} de 1000; parece el menos disperso de los 3 a lo largo de toda la recta. Los 3 casos presentan una aparente saturación en los extremos de la recta, que recuerda a una sigmoidea, y es muy probable que sea causado por la misma función de activación.


\subsection{Inciso b}

\subsubsection{\textit{minibatch} de 40 muestras}

Las figuras \ref{fig:batch40y40muestrasnoconverge} y \ref{fig:batch40y40muestrasnoconverge1} muestran el resultado de entrenar un MLP con \textit{minibatch} de 40 muestras en base a solo 40 muestras de entrenamiento. El entrenamiento se ejecutó por la misma cantidad de tiempo que el de \textit{minibatch} de 1 muestra, pero este no convergió a una solución adecuada, como se aprecia en la predicción.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/batch40y40muestras_noConverge}
	\caption{ECM de entrenamiento y validación - no hubo convergencia}
	\label{fig:batch40y40muestrasnoconverge}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/batch40y40muestras_noConverge1}
	\caption{Predicción del modelo en base a (X,Y,Z)}
	\label{fig:batch40y40muestrasnoconverge1}
\end{figure}

Las figuras \ref{fig:batch40y40muestrasconverge} y \ref{fig:batch40y40muestrasconverge1} son el resultado de tomar el modelo de las imágenes de antes pero darle más iteraciones de aprendizaje. Ahora si se observa convergencia en los EC; y la predicción parecería menos dispersa.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/batch40y40muestras_Converge}
	\caption{}
	\label{fig:batch40y40muestrasconverge}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/batch40y40muestras_Converge1}
	\caption{}
	\label{fig:batch40y40muestrasconverge1}
\end{figure}

\clearpage

\subsubsection{\textit{minibatch} de 1 muestras}

Las figuras \ref{fig:batch40y40muestrasnoconverge} y \ref{fig:batch40y40muestrasnoconverge1} muestran el resultado de entrenar un MLP con \textit{minibatch} de 1 muestra en base a solo 40 muestras de entrenamiento.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/batch1y40muestras_noConverge2}
	\caption{ECM de entrenamiento y validación - Hubo convergencia}
	\label{fig:batch1y40muestrasnoconverge2}
\end{figure}

\begin{figure}[h!]s
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/batch1y40muestras_noConverge3}
	\caption{Predicción del modelo en base a (X,Y,Z)}
	\label{fig:batch1y40muestrasnoconverge3}
\end{figure}

\subsubsection{Análisis}

Los resultados presentan dos puntos interesantes de destacar. Por un lado, limitar el set de entrenamiento a 40 muestras hace que no sea representativo de la dinámica que se quiere aprender/modelar/predecir y, por ende, el modelo va a tener mucha dificultad prediciendo con exactitud. Los 3 modelos parecen generalizar, es decir, para las varias ternas de entrada parece dar una salida diferentes, pero no es cercana a la deseada. Por otro lado, el modelo entrenado con \textit{minibatch} de 1 tiene mejor predicción que los otros, lo que puede deberse a que las muestras fueron más representativas (porque son tomadas aleatoriamente) y/o un \textit{minibatch} de 40 con 40 muestras ya no es estocástico (usa las mismas 40 siempre) y entonces no se goza de los beneficios de un gradiente estocástico. Estos son varios pero el principal es que ayuda a la convergencia a mejores mínimos de la función de error.


\subsection{Análisis global}

Como resumen de los 2 análisis previos, se desea resaltar que el \textit{minibatch} (indirectamente el gradiente estocástico) parecería aumentar la velocidad del proceso de aprendizaje, y produce resultados similarmente a los obtenidos con tamaños de lotes mayores. Esto es indispensable para la optimización en datasets grandes. Otro beneficio del uso de \textit{minibatches} es que cada ejecución es única (por el orden de las muestras), por lo que el camino que recorre en la minimización varía con la ejecución, permitiendo evitar mínimos locales que posiblemente trabarían un aprendizaje por gradiente determinístico.

Otro punto es que, aunque la cantidad de muestras es importante, y condiciona la capacidad de predicción del modelo, también es imperativo elegir el método de aprendizaje correcto. Se pueden tener resultados muy diferentes según el tamaño de \textit{minibatch}, como se notó en el segundo inciso del ejercicio.

En líneas generales, la consigna permitió entender que el resultado y proceso de entrenamiento no depende puramente de los datos (aunque si importan), sino que además de como se separan y usan dentro del proceso.


\clearpage

\section{Ejercicio 5}

\subsection{Consignas}

Siguiendo el trabajo de Hinton y Salakhutdinov (2006), entrene una máquina restringida de Boltzmann con imágenes de la base de datos MNIST. Muestre el error de reconstrucción durante el entrenamiento, y ejemplos de cada uno de los dígitos reconstruidos.
\subsection{Desarrollo}

\subsection{Análisis}

\clearpage

\section{Ejercicio 6}

\subsection{Consignas}

Entrene una red convolucional para clasificar las imágenes de la base de datos MNIST.
¿Cuál es la red convolucional más pequeña que puede conseguir con una exactitud de al menos 90\% en el conjunto de evaluación? ¿Cuál es el perceptrón multicapa más pequeño que puede conseguir con la misma exactitud?

\subsection{Desarrollo}

\subsection{Análisis}


\clearpage

\section{Ejercicio 7}

\subsection{Consignas}

Entrene un autoencoder para obtener una representación de baja dimensionalidad de las imágenes de MNIST. Use dichas representaciones para entrenar un perceptrón multicapa como clasificador. ¿Cuál es el tiempo de entrenamiento y la exactitud del clasificador obtenido cuando parte de la representación del autoencoder, en comparación con lo obtenido usando las imágenes originales?

\subsection{Desarrollo}



entrenamiento final:
Epoch 1/30 | Loss=0.6624 | Acc=81.28%
Epoch 2/30 | Loss=0.3537 | Acc=89.86%
Epoch 3/30 | Loss=0.3200 | Acc=90.68%
Epoch 4/30 | Loss=0.3073 | Acc=90.92%
Epoch 5/30 | Loss=0.2996 | Acc=91.19%
Epoch 6/30 | Loss=0.2962 | Acc=91.22%
Epoch 7/30 | Loss=0.2921 | Acc=91.29%
Epoch 8/30 | Loss=0.2884 | Acc=91.44%
Epoch 9/30 | Loss=0.2884 | Acc=91.51%
Epoch 10/30 | Loss=0.2855 | Acc=91.53%
Epoch 11/30 | Loss=0.2832 | Acc=91.62%
Epoch 12/30 | Loss=0.2837 | Acc=91.56%
Epoch 13/30 | Loss=0.2823 | Acc=91.63%
Epoch 14/30 | Loss=0.2813 | Acc=91.79%
Epoch 15/30 | Loss=0.2810 | Acc=91.69%
Epoch 16/30 | Loss=0.2799 | Acc=91.83%
Epoch 17/30 | Loss=0.2785 | Acc=91.79%
Epoch 18/30 | Loss=0.2773 | Acc=91.82%
Epoch 19/30 | Loss=0.2772 | Acc=91.85%
Epoch 20/30 | Loss=0.2775 | Acc=91.76%
Epoch 21/30 | Loss=0.2751 | Acc=91.91%
Epoch 22/30 | Loss=0.2738 | Acc=91.90%
Epoch 23/30 | Loss=0.2745 | Acc=91.89%
Epoch 24/30 | Loss=0.2738 | Acc=91.94%
Epoch 25/30 | Loss=0.2735 | Acc=91.97%
...
Epoch 28/30 | Loss=0.2736 | Acc=91.98%
Epoch 29/30 | Loss=0.2726 | Acc=91.98%
Epoch 30/30 | Loss=0.2720 | Acc=92.05%
Test Accuracy: 92.33%



\subsection{Análisis}



\clearpage

\section{Ejercicio 8}

\subsection{Consignas}

Encontrar un perceptrón multicapa que resuelva una XOR de 2 entradas mediante
\textit{simulated annealing}. Graficar el error a lo largo del proceso de aprendizaje.

\subsection{Desarrollo}

Para este ejercicio se reusó el código del ejercicio 3, cambiando la regla de aprendizaje a aquella del \textit{simulated annealing}.

El algoritmo inicia con un set de pesos $w$, actualizados como $w(n+1)=w(n) + \mathcal{N}(0, \sigma ^2 )$. Es decir, el paso, o $\delta w$ es aleatorio, y la varianza de la gaussiana determina que tan grande es ese paso.

Luego de obtener un set de parámetros $w(n+1)$, se calcula el $\delta E = E(w(n+1)) - E(w(n))$, que es lo mismo que decir que se calcula el cambio del error después del cambio de parámetros.

Ahora viene lo interesante: si el error decrece ($\delta E < 0$) entonces se admite el cambio de parámetros y $w(n) = w(n+1)$. En cambio, si el error empeora ($\delta E > 0$), el cambio se admite con una probabilidad $e ^\frac{-\delta E}{T}$, con $T$ la temperatura del sistema en ese instante.

Emula el proceso físico de templar y revenir una pieza de metal.

Como detalle final, la temperatura se debe decrecer para que el algoritmo vaya convergiendo (si es muy alta tal vez pasea por el espacio de estados sin límite). Esto generalmente se hace de forma exponencial como $T(n+1) = \alpha T(n)$.

En el desarrollo hecho se usó un decaimiento de temperatura del tiempo exponencial y un $\sigma$ constante. A continuación se detallan los varios casos simulados.

\subsubsection{XOR de 2 entradas - 2 neuronas en capa oculta}
$\alpha = 0.995$
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot001}
	\caption{Frontera de decisión de MLP de 2 neuronas en capa oculta}
	\label{fig:screenshot001}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot002}
	\caption{ECM y temperatura por interación de MLP de 2 neuronas en capa oculta}
	\label{fig:screenshot002}
\end{figure}

La XOR de 2 entradas fue aprendida perfectamente por una red de 2 neuronas en la capa oculta, igual que en la consigna 3. La imagen \ref muestra la frontera de decisión{fig:screenshot001}, también recta como en el caso de gradiente descendiente. En el gráfico~\ref{fig:screenshot002} se puede ver el ECM y temperatura. hay cierta estocacidad con el ECM (como es de esperar por la temperatura), pero termina disminuyendo hasta converger en 0.

\subsubsection{XOR de 2 entradas - 4 neuronas en capa oculta}
$\alpha = 0.995$
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot003}
	\caption{Frontera de decisión de MLP de 4 neuronas en capa oculta}
	\label{fig:screenshot003}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot004}
	\caption{ECM y temperatura por interación de MLP de 4 neuronas en capa oculta}
	\label{fig:screenshot004}
\end{figure}

La figura \ref{fig:screenshot003} muestra la frontera de decisión, y la \ref{fig:screenshot004} contiene el ECM y temperatura. La red aprendió el problema a la perfección. Su ECM es decreciente y la forntera es congruente con lo esperado y ya visto.

\subsubsection{XOR de 2 entradas - 10 neuronas en capa oculta}
$\alpha = 0.995$
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot005}
	\caption{Frontera de decisión de MLP de 10 neuronas en capa oculta}
	\label{fig:screenshot005}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot006}
	\caption{ECM y temperatura por interación de MLP de 10 neuronas en capa oculta}
	\label{fig:screenshot006}
\end{figure}

La figura \ref{fig:screenshot005} muestra la frontera de decisión, y la \ref{fig:screenshot006} contiene el ECM y temperatura. la frontera de decisión es curva como en el caso de 10 neuronas con gradiente descendiente y el ECM disminuye como se esperaba.

\subsubsection{XOR de 2 entradas - 2 neuronas en capa oculta}
$\alpha = 0.98$
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot007}
	\caption{ECM y temperatura -Decay rate de 0.98 - caso 1}
	\label{fig:screenshot007}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot008}
	\caption{ECM y temperatura - Decay rate de 0.98 - caso 2}
	\label{fig:screenshot008}
\end{figure}

Para este caso (figuras \ref{fig:screenshot007} y \ref{fig:screenshot008}), em uno la red aprendió el problema aún cuando la regla de la actualización de la temperatura era la de una exponencial rápida. En el otro no lo aprendió. Esto claramente es algo estocástico.

\subsubsection{XOR de 2 entradas - 2 neuronas en capa oculta}
$\alpha = 0.999$
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot009}
	\caption{ECM y temperatura - Decay rate de 0.999}
	\label{fig:screenshot009}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot010}
	\caption{ECM y temperatura - Decay rate de 0.999}
	\label{fig:screenshot010}
\end{figure}

Para este caso (figuras \ref{fig:screenshot009} y \ref{fig:screenshot010}), la red claramente pasó demasiado tiempo con alta temperatura, cayendo en un mínimo no global. esto se ejecutó varias veces con el mismo resultado, aunque no es definitivo.

\subsubsection{XOR de 4 entradas - 4 neuronas en capa oculta}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot011}
	\caption{}
	\label{fig:screenshot011}
\end{figure}

No aprendió

\subsubsection{XOR de 4 entradas - 5 neuronas en capa oculta}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot012}
	\caption{}
	\label{fig:screenshot012}
\end{figure}

aprendió

\subsubsection{XOR de 4 entradas - 6 neuronas en capa oculta}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{../imgs/ej8/screenshot013}
	\caption{}
	\label{fig:screenshot013}
\end{figure}

Sobre estas últimas, puede ser que le sea más fácil encontrar la solución pq el esacio de soluciones es más chico y este algoritmo se lo permite encontrar.

\subsection{Análisis}

\clearpage


\section{Conclusiones}

\end{document}
