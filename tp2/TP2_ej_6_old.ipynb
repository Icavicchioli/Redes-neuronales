{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bca683",
   "metadata": {},
   "source": [
    "Entrene una red convolucional para clasificar las imágenes de la base de datos MNIST.\n",
    "\n",
    "¿Cuál es la red convolucional más pequeña que puede conseguir con una exactitud de al menos 90% en el conjunto de evaluación? ¿Cuál es el perceptrón multicapa más pequeño que puede conseguir con la misma exactitud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3b0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7873a2c",
   "metadata": {},
   "source": [
    "En este caso se nos permitió usar Pytorch para la parte de código. La idea es definir la estructura general de la red y luego ir modificandola hasta lograr la mejor performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b65811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert the data to tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa81cc",
   "metadata": {},
   "source": [
    "La primer estructura que quiero hacer es algo muy simple, una red convolucional pegada a una fully connected, bien simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6512c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5) # son 8 filtros de 5x5\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # esto reduce la dimensión espacial a la mitad\n",
    "        self.flatten = nn.Flatten() # esto aplana la entrada para la capa lineal\n",
    "        self.fc = nn.Linear(12*12*8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ce47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # minibatch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe0be2",
   "metadata": {},
   "source": [
    "El código de abajo entrena hasta un máximo de 20 epochs, con early stippong (si converge la loss). Entrena y testea por cada epoch. también grafica todo para poder entender como varia con epoch, la idea es que podemos comparar la velocidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661d0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, device,\n",
    "                       num_epochs=100, patience=10, min_delta=1e-4, plot=True, criterion= nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo PyTorch con early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: instancia de nn.Module\n",
    "        train_loader: DataLoader de entrenamiento\n",
    "        test_loader: DataLoader de validación/prueba\n",
    "        criterion: función de pérdida (ej: nn.CrossEntropyLoss())\n",
    "        optimizer: optimizador (ej: optim.Adam(model.parameters(), lr=0.001))\n",
    "        device: 'cpu' o 'cuda'\n",
    "        num_epochs: cantidad máxima de épocas\n",
    "        patience: cuántas épocas esperar sin mejora\n",
    "        min_delta: mejora mínima en test loss para resetear early stopping\n",
    "        plot: si True, grafica loss y accuracy\n",
    "\n",
    "    Returns:\n",
    "        history: diccionario con 'train_loss', 'test_loss', 'accuracy'\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"El modelo tiene {total_params:,} parámetros entrenables.\")\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"accuracy\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "        # --- Evaluación ---\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        acc = 100 * correct / total\n",
    "\n",
    "        history[\"test_loss\"].append(avg_test_loss)\n",
    "        history[\"accuracy\"].append(acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Test Loss: {avg_test_loss:.4f} | \"\n",
    "              f\"Acc: {acc:.2f}%\")\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        if avg_test_loss < best_loss - min_delta:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping por falta de mejora.\")\n",
    "            break\n",
    "\n",
    "    # --- Graficar ---\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evolución del Loss\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history[\"accuracy\"], label=\"Accuracy\", color=\"green\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Evolución de la Accuracy\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ca637",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to() received an invalid combination of arguments - got (Adam), but expected one of:\n * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m      2\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m history = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, train_loader, test_loader, optimizer, device, num_epochs, patience, min_delta, plot, criterion)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_evaluate\u001b[39m(model, train_loader, test_loader, optimizer, device,\n\u001b[32m      7\u001b[39m                        num_epochs=\u001b[32m100\u001b[39m, patience=\u001b[32m10\u001b[39m, min_delta=\u001b[32m1e-4\u001b[39m, plot=\u001b[38;5;28;01mTrue\u001b[39;00m, criterion= nn.CrossEntropyLoss()):\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Entrena y evalúa un modelo PyTorch con early stopping.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m        history: diccionario con 'train_loss', 'test_loss', 'accuracy'\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     best_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     29\u001b[39m     epochs_no_improve = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\q\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1314\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1229\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move and/or cast the parameters and buffers.\u001b[39;00m\n\u001b[32m   1230\u001b[39m \n\u001b[32m   1231\u001b[39m \u001b[33;03m    This can be called as\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1312\u001b[39m \n\u001b[32m   1313\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     device, dtype, non_blocking, convert_to_format = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_parse_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (dtype.is_floating_point \u001b[38;5;129;01mor\u001b[39;00m dtype.is_complex):\n",
      "\u001b[31mTypeError\u001b[39m: to() received an invalid combination of arguments - got (Adam), but expected one of:\n * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "history = train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device,\n",
    "    num_epochs=50\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04073281",
   "metadata": {},
   "source": [
    "ahora creo otros modelos y vemos que pasa. la idea es ir mejorando para llegar a la performance de 90%. primero un modelo con más capa de perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) # son 6 filtros de 5x5, duplicamos la cantidad de kernels diferentes. -> 26x26x6\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # esto reduce la dimensión espacial a la mitad -> 12x12x6\n",
    "        self.flatten = nn.Flatten() # esto aplana la entrada para la capa lineal\n",
    "        # After conv (28->24) and pool (24->12), the feature map size is 12x12 with 6 channels -> 12*12*6 = 864\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(12 * 12 * 6, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv → ReLU → pool\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93816cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Train Loss: 1.4471 | Test Loss: 1.2297 | Acc: 49.10%\n",
      "Epoch [2/50] | Train Loss: 1.1901 | Test Loss: 1.1814 | Acc: 49.59%\n",
      "Epoch [3/50] | Train Loss: 1.1771 | Test Loss: 1.1879 | Acc: 49.46%\n",
      "Epoch [4/50] | Train Loss: 1.1738 | Test Loss: 1.1922 | Acc: 49.56%\n",
      "Epoch [5/50] | Train Loss: 1.1672 | Test Loss: 1.1910 | Acc: 49.37%\n",
      "Epoch [6/50] | Train Loss: 1.1703 | Test Loss: 1.2024 | Acc: 49.48%\n",
      "Epoch [7/50] | Train Loss: 1.1701 | Test Loss: 1.2098 | Acc: 49.38%\n",
      "Epoch [8/50] | Train Loss: 1.1703 | Test Loss: 1.1947 | Acc: 49.46%\n",
      "Epoch [9/50] | Train Loss: 1.1691 | Test Loss: 1.2162 | Acc: 49.27%\n",
      "Epoch [10/50] | Train Loss: 1.1644 | Test Loss: 1.2136 | Acc: 49.29%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "history = train_and_evaluate(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device,\n",
    "    num_epochs=50\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) # son 6 filtros de 5x5, pero bienen en escala de grises\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # esto reduce la dimensión espacial a la mitad\n",
    "        self.flatten = nn.Flatten() # esto aplana la entrada para la capa lineal\n",
    "        self.linear_relu_stack = nn.Sequential( # solo 1 capa lineal, mínimo posible\n",
    "            nn.Linear(864, 10), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # conv → ReLU → pool\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308a033",
   "metadata": {},
   "source": [
    "Ahora comparamos con el perceptrón muilticapa más chico que encontremos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
