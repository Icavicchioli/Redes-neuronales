{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bca683",
   "metadata": {},
   "source": [
    "Entrene un autoencoder para obtener una representaci√≥n de baja dimensionalidad de las im√°genes de MNIST. Use dichas representaciones para entrenar un perceptr√≥n multicapa como clasificador. ¬øCu√°l es el tiempo de entrenamiento y la exactitud del clasificador obtenido cuando parte de la representaci√≥n del autoencoder, en comparaci√≥n con lo obtenido usando las im√°genes originales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7873a2c",
   "metadata": {},
   "source": [
    "En este caso se nos permiti√≥ usar Pytorch para la parte de c√≥digo. La idea es definir la estructura general de la red y luego ir modificandola hasta lograr la mejor performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b65811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Elegir dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Transformaci√≥n\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Dataset MNIST\n",
    "train_dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# minibatch\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04edbe43",
   "metadata": {},
   "source": [
    "Primero una funci√≥n que entrena y grafica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fd18e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader,\n",
    "                       criterion, optimizer, device,\n",
    "                       num_epochs=50, patience=5, min_delta=1e-4):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    history = {\"train_loss\": [], \"test_loss\": [], \"accuracy\": []}\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(model)\n",
    "    print(f\"Par√°metros entrenables: {total_params:,}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # Evaluaci√≥n\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x,y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                test_loss += criterion(outputs, y).item()\n",
    "                _, pred = outputs.max(1)\n",
    "                total += y.size(0)\n",
    "                correct += pred.eq(y).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"accuracy\"].append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss {train_loss:.4f} | \"\n",
    "              f\"Test Loss {test_loss:.4f} | Acc {accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if test_loss < best_loss - min_delta:\n",
    "            best_loss = test_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"‚úî Early stopping activado\")\n",
    "            break\n",
    "\n",
    "    # Gr√°ficos\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train\")\n",
    "    plt.plot(history[\"test_loss\"], label=\"Test\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history[\"accuracy\"], label=\"Accuracy\")\n",
    "    plt.title(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b9484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            x_flat = x.view(x.size(0), -1)     # üîπ aplanar\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_flat)\n",
    "            loss = criterion(outputs, x_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # evaluaci√≥n\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, _ in test_loader:\n",
    "                x = x.to(device)\n",
    "                x_flat = x.view(x.size(0), -1)\n",
    "                outputs = model(x_flat)\n",
    "                loss = criterion(outputs, x_flat)\n",
    "                test_loss += loss.item()\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa81cc",
   "metadata": {},
   "source": [
    "La primer estructura que quiero hacer es el autoencoder. Vo a elegir una arquitectura sim√©trica con una reducci√≥n del 80% de tama√±o de dimensi√≥n de entrada, de acuerdo al principio de Pareto, que dice que el 80% de las cosas son causadas por el 20%. Esto lo extrapolo a energ√≠a y varianza y listo. \n",
    "\n",
    "Entramos con $28*28$, luego a $28*28*0.5 $y luego a $0.2*28*28$. De ah√≠ sube de vuelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6512c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 392)\n",
    "        self.fc2 = nn.Linear(392, 156)\n",
    "        self.fc3 = nn.Linear(156, 392)\n",
    "        self.fc4 = nn.Linear(392, 28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "model_cnn = Autoencoder().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce4dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] - Train Loss: 0.0693, Test Loss: 0.0691\n"
     ]
    }
   ],
   "source": [
    "# -------- Entrenamiento --------\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "train_losses, test_losses = train_autoencoder(\n",
    "    model_cnn, train_loader, test_loader,\n",
    "    criterion, optimizer, device,\n",
    "    num_epochs=30\n",
    ")\n",
    "\n",
    "\n",
    "# -------- Gr√°ficos --------\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('P√©rdida del Autoencoder')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------- Visualizar reconstrucciones --------\n",
    "model_cnn.eval()\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        x = x.to(device)\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        outputs = model_cnn(x_flat)\n",
    "        break  # solo un batch\n",
    "\n",
    "# convertir a im√°genes\n",
    "x = x.cpu().view(-1, 1, 28, 28)\n",
    "outputs = outputs.cpu().view(-1, 1, 28, 28)\n",
    "\n",
    "# mostrar algunas im√°genes\n",
    "n = 8\n",
    "plt.figure(figsize=(15,4))\n",
    "for i in range(n):\n",
    "    # original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x[i][0], cmap=\"gray\")\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # reconstruida\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(outputs[i][0], cmap=\"gray\")\n",
    "    plt.title(\"Reconstruida\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382de07a",
   "metadata": {},
   "source": [
    "1570 par√°metros no congelados, la capa de 156 a 10 + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aff970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss=0.6868 | Acc=80.41%\n",
      "Epoch 2/10 | Loss=0.3539 | Acc=89.82%\n",
      "Epoch 3/10 | Loss=0.3204 | Acc=90.53%\n",
      "Epoch 4/10 | Loss=0.3070 | Acc=90.93%\n",
      "Epoch 5/10 | Loss=0.2998 | Acc=91.19%\n",
      "Epoch 6/10 | Loss=0.2950 | Acc=91.32%\n",
      "Epoch 7/10 | Loss=0.2916 | Acc=91.31%\n",
      "Epoch 8/10 | Loss=0.2891 | Acc=91.43%\n",
      "Epoch 9/10 | Loss=0.2872 | Acc=91.48%\n",
      "Epoch 10/10 | Loss=0.2845 | Acc=91.59%\n",
      "Test Accuracy: 92.13%\n"
     ]
    }
   ],
   "source": [
    "class ClassifierFromEncoder(nn.Module):\n",
    "    def __init__(self, autoencoder, num_classes=10):\n",
    "        super().__init__()\n",
    "        # ac√° tomamos las capas del otro modelo y las congelamos\n",
    "        self.encoder_fc1 = autoencoder.fc1\n",
    "        self.encoder_fc2 = autoencoder.fc2\n",
    "        for param in self.encoder_fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.encoder_fc2.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = nn.Linear(156, num_classes) # lo m√°s simple posible\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        with torch.no_grad():\n",
    "            x = F.relu(self.encoder_fc1(x))\n",
    "            encoded = F.relu(self.encoder_fc2(x))\n",
    "        logits = self.classifier(encoded)\n",
    "        return logits\n",
    "\n",
    "classifier_model = ClassifierFromEncoder(model_cnn).to(device)\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.Adam(classifier_model.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# ------------------------\n",
    "# Entrenamiento Clasificador\n",
    "# ------------------------\n",
    "num_epochs_cls = 30\n",
    "for epoch in range(num_epochs_cls):\n",
    "    classifier_model.train()\n",
    "    running_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer_cls.zero_grad()\n",
    "        outputs = classifier_model(x)\n",
    "        loss = criterion_cls(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "        running_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_cls} | Loss={running_loss/len(train_loader):.4f} | Acc={train_acc:.2f}%\")\n",
    "\n",
    "# ------------------------\n",
    "# Evaluaci√≥n en Test\n",
    "# ------------------------\n",
    "classifier_model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = classifier_model(x)\n",
    "        _, pred = outputs.max(1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Test Accuracy: {100 * correct/total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
