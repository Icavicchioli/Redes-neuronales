% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the ``article'' class.
% See ``book'', ``report'', ``letter'' for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{siunitx}
% para los cuadritos en links
%\usepackage[linkbordercolor={0 0 1}, citebordercolor={0 1 0}, urlbordercolor={1 0 0}]{hyperref}
\usepackage[colorlinks=true, linkcolor=black, citecolor=green, urlcolor=red]{hyperref} % solo resalta
\usepackage[spanish]{babel}

\usepackage{booktabs}
\usepackage{array}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The ``real'' document content comes below...



\title{
	\vspace{-2cm} % Ajusta este valor para subir/bajar todo el bloque
	\centering
	\LARGE \textbf{Monografía Final} \\[0.8cm]
	\Huge \textbf{Mi título} \\[1.5cm]
	\large
	\textbf{Universidad de Buenos Aires} \\
	Facultad de Ingeniería \\[0.5cm]
	\normalsize
	\begin{tabular}{r l}
		\textbf{Alumno:} & Ignacio Ezequiel Cavicchioli \\
		\textbf{Padrón:} & 109428 \\
		\textbf{Email:} & icavicchioli@fi.uba.ar
	\end{tabular}
}
\author{} % Dejamos vacío porque el autor ya está en el título
\date{\vspace{0.5cm} \today} % Fecha actual

\begin{document}
\maketitle

%\tableofcontents

\begin{abstract}
	\noindent
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren,

	\vspace{0.5em}
	\noindent
	\textbf{Palabras clave:} Lorem ipsum dolor sit amet.
\end{abstract}

\vspace{1cm}

% TABLA DE CONTENIDOS
\tableofcontents
\newpage

% CONTENIDO PRINCIPAL
\section{Introducción}
\label{sec:introduccion}

Las redes neuronales en sus múltiples formas constituyen sistemas no lineales con un amplio alcance de aplicación en campos como la biología, neurociencia y, al que se avoca este trabajo, aprendizaje automático (\textit{machine learning}, ML). En particular, nos interesa centrarnos en las aplicaciones de las redes neuronales en el campo de control automático. Esta doctrina se encarga del diseño sistemas para regular, guiar o estabilizar procesos de manera autónoma, mediante la realimentación y corrección continua de errores.

La denominada ``caja de herramientas'' de aquellos en el área de control está compuesta por ciertos artefactos matemáticos que permiten encarar estos problemas, como la linealización de un sistema, el control PID, realimentación de estados, \textit{loop-shaping}, observadores, etc. Lo que no se ha tocado en las materias de control son las estrategias no lineales. En líneas generales, todos los sistemas reales exhiben cierto grado de no linearidad (citar CONTROL SYSTEM DESIGN Goodwin  Graebe Salgado, p551, cap19), lo que implica que las estrategias de control lineales son válidas siempre que las no linealidades sean despreciables. Análogamente, las herramientas de identificación de sistemas basadas en la linealización de un sistema fallaran en modelar las no linealidades de estos (Como PCA vs los \textit{autoencoders}).

Este trabajo va a trabajar sobre el uso de redes neuronales en la doctrina de control automático, como identificacores de sistemas, controladores

\textbf{VER SI QUERËS METER ALGO DEL SOM QUE SALIÖ MAL Y DE OBERVADORES Y DE GAIN SCHEDULING}

\section{Sistema elegido}

Que
tamaños
Porque
lo bueno
lo malo
la matemática -> espacio de estados no lineal y linealizado, transferencia (lineal).
el simulink


El sistema elegido está compuesto por 2 tanques de agua de dimensiones diferentes. Tanto la tubería que une los tanques como la que sale tienen cierto coeficiente hidráulico asociado a su geometría. Los valores se eligieron para que el sistema tenga sentido físico aunque no es un requisito. Se podría pensar que el sistema actúa como una cisterna amortiguadora de fluctuaciones en el caudal seguido de un reservorio que ajusta el caudal de salida.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/tanques}
	\caption{Sistema elegido}
	\label{fig:tanques}
\end{figure}

La figura \ref{fig:tanques} muestra el sistema recién descripto, agregando los caudales de entrada y salida $u$ e $y$. Ambos caudales son muestreados a \SI{1}{\Hz}, que debería ser más que suficiente para este tipo de dinámicas lentas.

Las razones por la cuales se eligió este sistema son:
\begin{itemize}
	\item Simplicidad: Es un sistema de 2 estados, simple de modelar, linealizar, simular y controlar. Las redes quese prueben deberían poder con este problema.
	\item No linealidad: Como se va a ver en el inciso matemático, el sistema no es lineal, que sería un requisito si se está intentando evaluar la capacidad de copiar no linealidades de las redes neuronales.
	\item Realismo: Se prefirió elegir un sistema que sea fácil de entender pero real, no una abstracción de un sistema más complejo.
\end{itemize}


\subsection{Modelo matemático}

El modelado de este sistema hace uso de varias leyes físicas de la hidráulica. Primero se plantea que el líquido es incompresible, por lo que el volumen solo varía si los caudales de entrada y salida no son iguales.

\begin{equation}
	\frac{dV}{dt} = \sum q_{in} - \sum q_{out}
	\label{eq:1}
\end{equation}


Además, el volumen es función del área del tanque y su superficie (en este caso que el área es independiente del nivel de agua). Podemos derivar respecto del tiempo.

\begin{equation}
	V(t) = A \cdot h(t) \overset{d/dt}{\longrightarrow } \frac{dV}{dt} = A \cdot \frac{dh(t)}{dt}
	\label{eq:2}
\end{equation}

Luego se aplica la ley de caudal, que relaciona el caudal entre 2 puntos con la diferencia de nivel entre ellos mismos.

\begin{equation}
	q = k\,\sqrt{2g}\,\sqrt{\Delta h}
	\label{eq:3}
\end{equation}

El caudal entre los tanques resulta:

\begin{equation}
	q_{12}(t) = k_{12}\,\sqrt{2g}\,\sqrt{h_1(t) -h_2(t)}
	\label{eq:4}
\end{equation}

El caudal de salida del segundo tanque, que también es la salida $y$, es:
\begin{equation}
	q_{2}(t) = y(t) = k_{2}\,\sqrt{2g}\,\sqrt{h_2(t)}
		\label{eq:5}
\end{equation}


Ahora, se plantea un balance en cada tanque igualando \eqref{eq:1} y \eqref{eq:2}.

\begin{equation}
	A_1 \cdot \frac{dh_1}{dt} = u(t) - q_{12}(t)
	\label{eq:6}
\end{equation}

\begin{equation}
	A_2 \cdot \frac{dh_2}{dt} = q_{12}(t) - q_{2}(t) =  q_{12}(t) - y(t)
	\label{eq:7}
\end{equation}

sustituyendo con \eqref{eq:4} :

\begin{equation}
	A_1 \cdot \frac{dh_1}{dt} = u(t) - k_{12}\,\sqrt{2g}\,\sqrt{h_1(t) -h_2(t)}
	\label{eq:8}
\end{equation}

\begin{equation}
	A_2 \cdot \frac{dh_2}{dt} = k_{12}\,\sqrt{2g}\,\sqrt{h_1(t) -h_2(t)} -  k_{2}\,\sqrt{2g}\,\sqrt{h_2(t)}
	\label{eq:9}
\end{equation}

Con todo esto se plantea el espacio de estados\textbf{ no lineal} y continuo con la forma de a continuación:

\begin{equation}
	\frac{d}{dt}
	\begin{bmatrix}
		h_1
		\\
		h_2
	\end{bmatrix} =
	\begin{bmatrix}
		(u - k_{12}\,\sqrt{2g}\,\sqrt{h_1 -h_2})\cdot \frac{1}{A_1}
		\\
		(k_{12}\,\sqrt{2g}\,\sqrt{h_1 -h_2} -  k_{2}\,\sqrt{2g}\,\sqrt{h_2})\cdot \frac{1}{A_2}
	\end{bmatrix}
	\label{eq:ss1}
\end{equation}


\begin{equation}
	y=k_{2}\,\sqrt{2g}\,\sqrt{h_2}
	\label{eq:ss2}
\end{equation}

Las expresiones \eqref{eq:ss1} y \eqref{eq:ss2} se van a linealizar en torno al punto de equilibrio $(x_e,u_e)$ y las constantes físicas indicadas abajo.

\begin{itemize}
	\item $g = 9.81$
	\item $A_1 = 0.342$, $A_2 = 0.126$
	\item $k_{12} = 5 \times 10^{-4}$, $k_2 = 1 \times 10^{-3}$
	\item $h_{1s} = 1.019$, $h_{2s} = 0.204$
	\item $u_s = 0.002$
\end{itemize}

%g   = 9.81;
%A1  = 0.015;
%A2  = 0.02;
%k12 = 5e-4;
%k2  = 1e-3;
%h1s = 1.019;
%h2s = 0.204;
%us  = 0.002;

Las variables de estado elegidas se redefinen como variaciones en torno a ese mismo estado, por lo que de ahora en más las alturas $h_1$ y $h_2$ no son las mismas que en la planta no lineal. El proceso arranca planteando la linealización en sí, que se ve en la ecuación \eqref{eq:12}. La expresión \eqref{eq:13} se cumple por definición del punto $(x_e,u_e)$. En \eqref{eq:14a} y \eqref{eq:14b} se obtiene la matriz A del espacio de estados lineal. En \eqref{eq:15} se obtiene la matriz B, y en \eqref{eq:17}, la C.

Las expresiones \eqref{eq:16} y \eqref{eq:17} constituyen el espacio de estados lineal para la dinámica de los tanques.

\begin{equation}
\overset{\,\circ }{X} = \overset{\,\circ }{\begin{bmatrix}
		h_1
		\\
		h_2
\end{bmatrix}} =f(x_e,u_e) + \frac{df}{d x}|_{(x_e,u_e)} (x-x_e) +  \frac{df}{d u}|_{(x_e,u_e)} (u-u_e)
\label{eq:12}
\end{equation}

\begin{equation}
f(x_e,u_e)=0
\label{eq:13}
\end{equation}

\begin{equation}
\frac{df}{d x}|_{(x_e,u_e)} = \begin{bmatrix}
	\frac{-k_{12} \sqrt{2g}}{A_1 \cdot 2 \sqrt{h_1-h_2}}
	&
	\frac{k_{12} \sqrt{2g}}{A_1 \cdot 2 \sqrt{h_1-h_2}}
	\\
	\frac{k_{12} \sqrt{2g}}{A_2 \cdot 2 \sqrt{h_1-h_2}}
	&
	\frac{-k_{12} \sqrt{2g}}{A_2 \cdot 2 \sqrt{h_1-h_2}} - \frac{k_{2} \sqrt{2g}}{A_2 \cdot 2 \sqrt{h_2}}
\end{bmatrix}
|_{(x_e,u_e)}
\label{eq:14a}
\end{equation}

\begin{equation}
	\frac{df}{d x}|_{(x_e,u_e)} =
	 \simeq
	\begin{bmatrix}
		-0.00359
		&
		0.000359
		\\
		0.00976
		&
		-0.04878
	\end{bmatrix}
	\label{eq:14b}
\end{equation}


\begin{equation}
\frac{df}{d u}|_{(x_e,u_e)} = \begin{bmatrix}
	\frac{1}{A_1}
	\\
	0
\end{bmatrix}|_{(x_e,u_e)}=
\begin{bmatrix}
	2.92
	\\
	0
\end{bmatrix}
\label{eq:15}
\end{equation}


\begin{equation}
\overset{\,\circ }{\begin{bmatrix}
		h_1
		\\
		h_2
\end{bmatrix}} =
	\begin{bmatrix}
	-0.00359
	&
	0.000359
	\\
	0.00976
	&
	-0.04878
\end{bmatrix}
\begin{bmatrix}
	h_1
	\\
	h_2
\end{bmatrix}
+
\begin{bmatrix}
	2.92
	\\
	0
\end{bmatrix} u
\label{eq:16}
\end{equation}

\begin{equation}
y = \begin{bmatrix}
	0 & 0.0049
\end{bmatrix} \begin{bmatrix}
	h_1
	\\
	h_2
\end{bmatrix}
\label{eq:17}
\end{equation}

Además de usar este modelo matemático, se estimó un espacio de estados de misma dimensión a partir del \textit{dataset} de 14400 muestras, para comparar con las redes.

\newpage
\subsection{Simulaciones}

Para esta monografía se decidió hacer uso de \textit{scripts} \textit{MATLAB} y el entorno de \textit{Simulink} debido a la versatilidad que trae en lo que es simulación de sistemas, además que es la herramienta usada en las materias de control automático. La figura \ref{fig:pnolin} muestra el sistema no lineal armado. En pocas palabras, usa las constantes definidas en el \textit{workbench}, la entrada $u$ y los estados integrados para calcular el siguiente estado de la planta.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/P_no_lin}
	\caption{Planta no lineal armada en \textit{simulink}}
	\label{fig:pnolin}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/P_no_lin_muestreada}
	\caption{Muestreo de planta no lineal}
	\label{fig:pnolinmuestreada}
\end{figure}

La figura \ref{fig:pnolinmuestreada} muestra el sistema de \textit{simulink} que emula el muestreo de la planta. De entrada $u$ se usó una secuencia de escalones de amplitudes aleatorias (ruido blanco gaussiano) alrededor del $10 \%$ del punto operativo. Esto se hizo para poder tener una buena variedad de respuestas al escalón, que se espera ayude a que la red aprenda la dinámica.

Como el muestreo se realizó a \SI{1}{\Hz}, se decidió generar un \textit{dataset} de 4 horas (14400 muestras), y otro de 1 hora (3600 muestras). La intención es observar cuanto empeora la \textit{performance} de la red con menos muestras de entrenamiento.








\section{Identificación de sistemas}
\label{subsec:ids}

La primer experiencia de esta monografía consiste en entrenar una red neuronal en base a los datos de la planta elegida, con la finalidad de obtener un sistema no lineal que la emule.

\subsection{Suposiciones y decisiones}

Con el objetivo de asegurar resultados conmensurables, los experimentos se armaron y realizaron siguiendo los siguientes preceptos:

\begin{enumerate}
	\item La única métrica de comparación elegida es el RMSE (ec. \eqref{RMCE}). Se calculará sobre las secuencias de salida de todas las redes entrenadas.

	\begin{equation}
		RMSE = \sqrt{\frac{1}{n} \sum ^n _{i=1} (\hat{x} - x)^2  }
		\label{RMSE}
	\end{equation}

	\item El RMSE se calculará sobre la secuencia de datos de salida pero sin incluir el transitorio inicial. Esto es porque el transitorio inicial no está bien representado en los datos de entrenamiento.
	\item Todos los modelos deberán poder usarse de forma auto-regresiva, es decir, realimentados con sus propias salidas. Esto implica que el entrenamientos se va a tener que realizar hasta que se observe convergencia en la simulación.
	\item La cantidad de \textit{epochs} y la cantidad de intentos de entrenamiento no fueron fijadas, sino que se ajustaron empíricamente,  asegurando el cumplimiento del ítem 2.
	\item Los datos se separaron en \textit{train} y \textit{test} en una proporción de $85\%$ a $/15\%$. La separación se realizó de forma secuencial, es decir, se tomo el primer $85\%$ de las muestras para entrenar y el resto para evaluar, manteniendo cierta semblanza al orden temporal. Se podría argumentar que las muestras de evaluación no necesariamente se van a distribuir idénticamente que las de entrenamiento, pero se priorizó variedad de muestras, y el hecho de que los escalones se generaron de forma aleatoria debería ayudar.
	\item Se aplicó una normalización en media y varianza a los datos previo a entrenar. Se espera que esto mejore la \textit{performance} (a entradas más espaciadas, pesos más separados) y la convergencia en el \textit{fitting}.
\end{enumerate}



Con esto dicho, se procede a los ensayos.

\subsection{MLP de 1 capa oculta}

\subsubsection{Introducción}

El primer \textit{approach} pensado es usar un MLP de 1 capa oculta y entrenarlo en base a los datos de la planta en una configuración auto-regresiva como la de la figura \ref{fig:redautoreg}. Dado lo visto en la materia, se espera que el MLP logre captar las no-linealidades de la planta con una capacidad que aumenta con la cantidad de neuronas en su capa oculta. En palabras más simples, se espera que los modelos más chicos tengan peor desempeño que los grandes.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/red_autoreg}
	\caption{Modelo de red}
	\label{fig:redautoreg}
\end{figure}


El vector de datos de entrada $\mathbf{x}(n)$ elegido consta de 10 muestras previas de $y$ y la referencia actual $u$.La salida esperada de la red es la siguiente muestra, efectivamente entrenando un predictor del siguiente estado.

\[
\mathbf{x}(n) =
\big[
y(n),\; y(n-1),\; \dots,\; y(n-9),\; u(n)
\big]
\]

\[
\hat{y}(n+1) = f\big(
y(n),\; y(n-1),\; \dots,\; y(n-9),\; u(n)
\big)
\]

La razón por la cual se eligieron 10 muestras previas de $y$ es porque se considera que, siendo un proceso lento, la red se beneficiaria de poder ver múltiples estados previos, notando como varía la salida en el tiempo. Si se usaran pocas muestras previas, el vector de entrada sería un apilamiento de números casi idénticos para los escalones de entrada pequeños.


\subsubsection{Entrenamiento}

Como se adelantó previamente, se decidió crear con 2 \textit{datasets} así que los experimentos van a verse duplicados. Para esta parte se decidió entrenar redes de 20, 10, 5 y 2 preceptrones en la capa oculta.

Algunos puntos interesantes a destacar del entrenamiento son:

\begin{itemize}
	\item Se entrenó por una cantidad fija de \textit{epochs}, permitiendo interrumpir cuando el gradiente era menor a $1e-9$ (que nunca sucedió).
	\item El proceso de entrenamiento fue manejado por la función \textit{train}\footnote{\href{https://la.mathworks.com/help/deeplearning/ref/network.train.html}{Documentación oficial}} del \textit{Deep learning toolbox} de \textit{MATLAB}.
	\item En general, los modelos pequeños requirieron de más intentos de entrenamiento para llegar a un sistema que cumpliera con los requisitos planteados. El parámetro modificado fue cantidad de \textit{epochs}.
	\item Hubo casos en los que los sistemas obtenidos tenían un estado estacionario oscilante, invariante respecto la acción de control. Esto fue considerado un espurio llamativo, aunque lamentablemente no se guardaron imágenes.
	\item El modelo lineal estimado se obtuvo usando la herramienta de identificación de sistemas propia de \textit{MATLAB}.
\end{itemize}



\subsubsection{Resultados}

Las figuras en esta sección muestran las señales de interés, incluyendo la entrada $u$, la salida $y$ real, la $y$ del sistema linealizado y la salida de cada red entrenada. La respuesta real (fig. \ref{fig:salida-y}) y las respuestas de las redes neuronales son casi indistinguibles, salvo por los transitorios iniciales. Sin embargo, lo mismo sucede para las respuestas obtenidas de los sistemas lineales.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/entrada u}
	\caption{Entrada $u$}
	\label{fig:entrada-u}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/salida y}
	\caption{Salida $y$ de la planta no lineal}
	\label{fig:salida-y}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/y e ylin}
	\caption{Comparación entre salida no lineal y lineal}
	\label{fig:y-e-ylin}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/y e ylin est}
	\caption{Comparación entre salida no lineal y lineal  del modelo estimado}
	\label{fig:y-e-ylin-est}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 20 14400}
	\caption{Salida del MLP de 20 neuronas en capa oculta - \textit{dataset} de 14400 muestras}
	\label{fig:mlp-20-14400}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 10 14400}
	\caption{Salida del MLP de 10 neuronas en capa oculta - \textit{dataset} de 14400 muestras}
	\label{fig:mlp-10-14400}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 5 14400}
	\caption{Salida del MLP de 5 neuronas en capa oculta - \textit{dataset} de 14400 muestras}
	\label{fig:mlp-5-14400}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 2 14400}
	\caption{Salida del MLP de 2 neuronas en capa oculta - \textit{dataset} de 14400 muestras}
	\label{fig:mlp-2-14400}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 20 3600}
	\caption{Salida del MLP de 20 neuronas en capa oculta - \textit{dataset} de 3600 muestras}
	\label{fig:mlp-20-3600}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 10 3600}
	\caption{Salida del MLP de 10 neuronas en capa oculta - \textit{dataset} de 3600 muestras}
	\label{fig:mlp-10-3600}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 5 3600}
	\caption{Salida del MLP de 5 neuronas en capa oculta - \textit{dataset} de 3600 muestras}
	\label{fig:mlp-5-3600}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/MLP 2 3600}
	\caption{Salida del MLP de 2 neuronas en capa oculta - \textit{dataset} de 3600 muestras}
	\label{fig:mlp-2-3600}
\end{figure}


A continuación se presenta un cuadro con los RMSE de todas las respuestas, calculado usando de referencia la salida del sistema no lineal original. Las secuencias fueron tomadas a partir de la muestra número 2000, evitando los transitorios, tal como se dijo que se iba a hacer.

\begin{table}[h]
	\centering
	\begin{tabular}{l c}
		\hline
		\textbf{Modelo} & \textbf{RMSE} \\
		\hline
		MLP 20--14400             & $2.36 \times 10^{-7}$ \\
		MLP 10--14400             & $3.24 \times 10^{-7}$ \\
		MLP 5--14400              & $1.06 \times 10^{-6}$ \\
		Modelo lineal (estimado)  & $1.80 \times 10^{-6}$ \\
		MLP 5--3600               & $1.95 \times 10^{-6}$ \\
		MLP 2--14400              & $2.01 \times 10^{-6}$ \\
		MLP 20--3600              & $6.71 \times 10^{-6}$ \\
		Modelo lineal (real)      & $6.83 \times 10^{-6}$ \\
		MLP 2 --3600               & $1.88 \times 10^{-5}$ \\
		MLP 10--3600              & $2.46 \times 10^{-5}$ \\
		\hline
	\end{tabular}
	\caption{Comparación del error cuadrático medio (RMSE) entre los distintos modelos evaluados}
	\label{tab:rmse_comparacion}
\end{table}
\clearpage
\subsubsection{Análisis de resultados}

A partir de los resultados expuestos en el cuadro \ref{tab:rmse_comparacion}, se notan tendencias en la \textit{performance} de los varios modelos analizados. Primero, los modelos entrenados en base a más muestras (14400) tienen RMSE inferiores a los entrenados en base al set de 3600 muestras - que es consistente con las hipótesis antes mencionadas de la capacidad de captar no-linealidades.

En segundo lugar, el sistema lineal obtenido matemáticamente (apodado ``real'' en el cuadro), logró mejores ajustes que dos redes por un orden de magnitud. Esto puede deberse a que el modelo lineal es muy bueno o que las redes resultaron \textit{sub-par}. Dicho esto, es inferior que el resto de las redes, incluyendo 2 entrenadas con el \textit{dataset} pequeño, dando a entender que, incluso con pocas muestras se puede lograr un modelo no lineal mejor que uno simple derivado matemáticamente. Esto representa un punto a favor de la identificación del tipo ``caja negra/gris''.

El ``podio'' de los resultados está compuesto de 3 redes neuronales y el modelo lineal estimado, todos entrenadas con el \textit{dataset} grande . La mejor red supera al modelo lineal por $7.6$ veces, y no sorprende que sea la red más grande. Se considera que, al disponer de más muestras, las redes lograron captar las no linealidades del modelo con mayor precisión.

Los resultados confirman que la cantidad de muestras disponibles permite el entrenamiento de modelos más complejos, que llegan a precisiones superiores que sus contrapartes lineales.

Ahora bien, se debe resaltar que ninguno de los modelos obtenidos es inherentemente malo, sino que el tipo de modelo a usar queda determinado por los requerimientos de la aplicación. Los experimentos recién detallados solo indican que una forma de lograr respuestas más similares a la real es por el uso de sistemas no lineales en la identificación.

\newpage
\subsection{Sistema lineal asistido}

\subsubsection{Introducción}

El segundo enfoque ideado surge del anterior. Si se puede lograr un buen desempeño con un modelo lineal, tal vez se pude corregir el error de la estimación lineal por medio de una red neuronal que aprenda el error de estimación. La figura \ref{fig:red_autoreg2} muestra el sistema recién descripto. El hiperparámetro $m$ se va a ajustar para minimizar el error de entrenamiento y evaluación. *


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{imgs/red_autoreg2}
	\caption{Modelo de red}
	\label{fig:red_autoreg2}
\end{figure}

Inicialmente, se espera que este sistema logre mejor precisión que si solo se usara el lineal, pero no se puede afirmar a priori si va a ser mejor que las otras redes.

Se va a usar el modelo lineal identificado por \textit{MATLAB}, y no el obtenido por medio del desarrollo matemático debido a 2 razones:
\begin{itemize}
	\item El modelo identificado dió mejores resultados.
	\item El uso del modelo identificado tiene más sentido en el contexto de esta monografía, que se centra en la identificación de sistemas por medio del entrenamiento de sistemas.
\end{itemize}


\subsubsection{Entrenamiento}

El proceso de \textit{fitting} comenzó generando un conjunto de datos auxiliar, formado por el error de estimación del sistema lineal $e =y - y_{lin}$, la salida lineal $y_{lin}$ y la entrada de los sistemas.
A partir de este conjunto, se construyeron los vectores de entrada a las redes ya especificados (los retardos de la salida lineal y la entrada $u$), mientras que como objetivo se empleó el error de estimación.

Como los resultados del entrenamiento no eran buenos, se decidió revertir la función de entrenamiento a su estado original, con los conjuntos de \textit{train}, \textit{test} y \textit{validation}.

\subsubsection{Resultados}

El cuadro~\ref{tab:resultados_cascada} muestra los experimentos realizados y el RMSE para cada uno. Cada prueba intentó mejorar la anterior de alguna forma, sea reduciendo la dimensión de la entrada o la cantidad de neuronas.

\newpage

\begin{table}[h!]
	\centering
	\begin{tabular}{c p{7cm} c c}
		\toprule
		\textbf{Prueba} & \textbf{Descripción de la red} & \textbf{RMSE} & \textbf{Figuras} \\
		\midrule
		1 &
		MLP de 20 unidades en la capa oculta.
		Usó $y_{\text{lin}}$ desde $n$ hasta $n-9$. &
		$8.71 \times 10^{-6}$ &
		Figs.~\ref{fig:prueba11}, \ref{fig:prueba12} \\

		2 &
		MLP de 20 unidades en la capa oculta.
		Usó únicamente $y_{\text{lin}}(n)$. &
		$9.18 \times 10^{-6}$ &
		Figs.~\ref{fig:prueba21}, \ref{fig:prueba22} \\

		3 &
		MLP de 10 unidades en la capa oculta.
		Usó $y_{\text{lin}}$ desde $n$ hasta $n-4$. &
		$6.21 \times 10^{-6}$ &
		Figs.~\ref{fig:prueba31}, \ref{fig:prueba32} \\

		4 &
		MLP de 5 unidades en la capa oculta.
		Usó $y_{\text{lin}}$ desde $n$ hasta $n-4$. &
		$5.78 \times 10^{-6}$ &
		Figs.~\ref{fig:prueba41}, \ref{fig:prueba42} \\

		5 &
		MLP de 5 unidades en la capa oculta.
		Usó $y_{\text{lin}}$ desde $n$ hasta $n-9$. &
		$5.87 \times 10^{-6}$ &
		Figs.~\ref{fig:prueba51}, \ref{fig:prueba52} \\
		\bottomrule
	\end{tabular}
	\caption{Resultados de las pruebas}
	\label{tab:resultados_cascada}
\end{table}



\subsubsection{Análisis de resultados}

Primero, los resultados son peores que el modelo lineal solo, que recordando, logró un $RMSE = 1.80e-06$. Esto no es el resultado esperado pero está dentro de lo considerado posible.

En segundo lugar, nótese que los mejores sistemas obtenidos son marginálmente mejores que el sistema lineal desarrollado matemáticamente, así que, aunque hubo un empeoramiento de la estimación, no fue lo peor que se obtuvo.

Dicho esto, las gráficas de las salidas de todas las pruebas hechas muestran espurios generados por la red neuronal (no están en la salida lineal), que empeoran el RMSE y constituyen transiciones de estados que la planta original no podría experimentar nunca (como escalones a la salida). Esto claramente es un efecto indeseado introducido por las redes neuronales entrenadas.

Por último, otro punto desfavorable para estos modelos híbridos es que su obtención tomó más tiempo que la de las redes neuronales solas o del modelo lineal estimado.


Todo lo observado apunta a que estos sistemas son inferiores a las alternativas vistas antes tanto en tiempo como en ajuste. No obstante, es probable que con un cambio de enfoque y arquitectura, la idea de estimar el error $y - y_{lin}$ de forma no lineal sea viable.

\newpage
\subsection{Discusión general de resultados}

En luz de los resultados ya expuestos en los incisos previos, se podrían hacer las siguientes afirmaciones:

\begin{itemize}
	\item Las redes neuronales sirven para hacer identificación de sistemas, y pueden lograr mejores ajustes que los modelos lineales.
	\item El tamaño de las redes no es algo de menor importancia ya que parece condicionar la capacidad de ajuste final. Deberá se elegida a conciencia y por medio de ``prueba y error''.
	\item Se observó una relación positiva entre la cantidad de neuronas en la capa oculta y el ajuste a la salida real.
	\item Se notó que el uso de más muestras permite obtener redes neuronales con mejor ajuste.
	\item Parecería preferible elegir un \textit{approach} puramente lineal o no lineal para la identificación que intentar fusionarlos. Esto último toma más tiempo y los resultados obtenidos no lo justifican.
\end{itemize}

Es clave entender que el uso de un sistema no lineal en el área de identificación de sistemas puede o no resultar un exceso. Como se dijo antes, la complejidad del modelo requerido queda completamente determinada por la aplicación. En este caso de 2 tanques de agua, un sistema no lineal es innecesario, pero para procesos ``menos lineales'' podría darse que el error de la estimación lineal es inadmisible, requiriéndose usar técnicas como las usadas.

\newpage

\section{Control de sistemas}
\label{sec:control}

Habiendop cocnluido la sección de

\section{Misceláneos}
\label{subsec:misc}



\section{Conclusiones}
\label{sec:conclusiones}


\section{Referencias}
\begin{itemize}
	\item Material de la materia ``Control Automático''.
	\item Control System Design. Autores: Graham C. Goodwin, Stefan F. Graebe, Mario E. Salgado. Año: 2000
	\item \href{https://en.wikipedia.org/wiki/Neural_network_(machine_learning)}{Neural network (machine learning)}
	\item \href{https://en.wikipedia.org/wiki/Machine_learning}{Machine learning}
	\item \href{https://en.wikipedia.org/wiki/Neural_network}{Neural network}
	\item \href{https://la.mathworks.com/help/deeplearning/ug/introduction-to-neural-network-control-systems.html}{Introduction to neural network control systems}
\end{itemize}

\newpage

\section{Apéndice de imágenes}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba1_1}
	\caption{Comparación de salidas - prueba 1}
	\label{fig:prueba11}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba1_2}
	\caption{Comparación de salidas - prueba 1}
	\label{fig:prueba12}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba2_1}
	\caption{Comparación de salidas - prueba 2}
	\label{fig:prueba21}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba2_2}
	\caption{Comparación de salidas - prueba 2}
	\label{fig:prueba22}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba3_1}
	\caption{Comparación de salidas - prueba 3}
	\label{fig:prueba31}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba3_2}
	\caption{Comparación de salidas - prueba 3}
	\label{fig:prueba32}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba4_1}
	\caption{Comparación de salidas - prueba 4}
	\label{fig:prueba41}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba4_2}
	\caption{Comparación de salidas - prueba 4}
	\label{fig:prueba42}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba5_1}
	\caption{Comparación de salidas - prueba 5}
	\label{fig:prueba51}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/prueba5_2}
	\caption{Comparación de salidas - prueba 5}
	\label{fig:prueba52}
\end{figure}



\end{document}
