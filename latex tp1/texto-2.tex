% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{enumitem}
\usepackage{amsmath}


%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Trabajo práctico 1\\ Redes Neuronales y Aprendizaje Profundo}
\author{Ignacio Ezequiel Cavicchioli\\Padrón 109428\\icavicchioli@fi.uba.ar}
\date{10/9/2025} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle
\section{Introducción}

Este documento presenta el desarrollo de las consignas del trabajo práctico N°1 de la materia de \textbf{Redes Neuronales y Aprendizaje Profundo}.  El código correspondiente fue realizado en  \textit{Jupyter notebooks}, \textit{Python}, adjuntados a la entrega en formato PDF. Toda imagen o implementación requeridas para el análisis se explicitarán en el presente archivo, por lo que la lectura del código en sí queda a discreción del lector. La teoría relevante será presentada y discutida en la sección pertinente.

\section{Ejercicio 1}

\subsection{Consignas}
Entrene una red de Hopfield ‘82 con las imágenes binarias disponibles en el campus. 
\begin{enumerate}[label=\alph*]
\item Verifique si la red aprendió las imágenes enseñadas.
\item Evalúe la evolución de la red al presentarle versiones alteradas de las imágenes
aprendidas: agregado de ruido, elementos borrados o agregados.
\item Evalúe la existencia de estados espurios en la red: patrones inversos y
combinaciones de un número impar de patrones. (Ver Spurious States, en la sección
2.2, Hertz, Krogh \& Palmer, pág. 24).
\item Realice un entrenamiento con las 6 imágenes disponibles. ¿Es capaz la red de
aprender todas las imágenes? Explique.
\end{enumerate}

\subsection{Desarrollo}

Una red de Hopfield es, en esencia, una memoria direccionable por contenido (\textit{content-adressable memory})\footnote{Neural Networks and Physical Systems with Emergent Collective Computational Abilities, Campus, J. J. Hopfield }; permite obtener un estado memorizado completo a partir de un estado incompleto suficientemente parecido. Como se va a mostrar en los varios incisos, la red de Hopfield no es una memoria ideal, pero bajo ciertas condiciones, el error en la recuperación de estados está acotado. 

La red de Hopfield a tratar es una colección de neuronas de McCulloch-Pitts. En pocas palabras, cada neurona tiene como ``entrada'' los estados de todas las otras neuronas menos sí misma. Estos se ponderan por un peso $w_{i,j}$ calculado por aprendizaje Hebbiano, se suman, conformando una $h$, que se pasa por una función de activación, determinando la ``salida'', o estado futuro, de la neurona. En los ejercicios se trabajó con una función de activación del tipo signo, que devuelve $-1$ o $1$ según el signo de $h$. Para $h = 0$ se determinó que la salida toma valor $1$. 

La red es entrenada por medio del aprendizaje de todos los $w_{i,j}$ según la regla de aprendizaje \eqref{eq:regla_aprendizaje}, que se puede generalizar matricialmente como se ve en \eqref{eq:generalizacion_matricial}. A cada imágen le corresponde una única iteración ($\Delta w_{i,j}$), por lo que solo se suma una vez a la matriz $\mathbf{W}$. En términos más informales, la red solo ``ve'' las imágenes 1 vez.  El peso $\eta$ se mantuvo unitario para todo entrenamiento. 

\begin{equation}
\begin{split}
\Delta w_{i,j} &= \eta x_i x_j \\
w_{{i,j}_n} &= w_{{i,j} _{n-1}} + \Delta w_{i,j}
\end{split}
\label{eq:regla_aprendizaje}
\end{equation}

\begin{equation}
\begin{split}
\Delta\mathbf{W} &= \eta \mathbf{x} \mathbf{x}^T \\
\mathbf{W}_n &= \mathbf{W}_{n-1} + \Delta\mathbf{W}
\end{split}
\label{eq:generalizacion_matricial}
\end{equation}

El proceso de aprendizaje equivale a aprender la correlación entre todas las imágenes. Imágenes muy parecidas (por algún criterio, ej.: distancia de Hamming) van a quedar ``cerca'' en la función energética subyacente. Esto trae a colación un aspecto importante para toda memoria, que es la capacidad. Como se va a demostrar más adelante, la capacidad es función de la cantidad de neuronas y correlación ente estados memorizados. 

Para'' ejecutar'' la red neuronal alcanza con calcular todos los estados siguientes de las neuronas. Esto se puede hacer de dos formas:
\begin{itemize}
\item Actualización sincrónica: se computa el siguiente estado de todas las neuronas a la vez.
\item Actualización asincrónica: las neuronas son aleatoriamente actualizadas, sin seguir un orden particular. La idea es que todas se actualicen al finalizar el ciclo. 
\end{itemize}

Para este trabajo se usó una actualización sincrónica, pero la actualización asincrónica tiene beneficios en lo relativo a convergencia de la red. 

Ahora bien, ¿Cómo se parte de una matriz de pesos y se termina con un sistema que recuerda estados? El libro ``\textit{Introduction To The Theory Of Neural Computation}'' provisto por el Campus de la materia explica que los estados memorizados son atractores en el espacio de todos los estados posibles. Una forma de pensarlo es como que los patrones o estados aprendidos son concavidades en una tela y el estado actual es una canica puesta sobre esta tela. Si se deja que la canica se mueva, va a caer en alguna de estas hendiduras de la tela. Como se va a tratar más adelante, existen otros atractores que no son los patrones aprendidos: los inversos de estos, las combinaciones lineales de un número impar de patrones (estados espurios de mezcla) y otros estados espurios que aparecen como consecuencia de la dinámica de la red.


Ahora que se tiene cierta noción sobre las redes de Hopfield se puede pasar al desarrollo de las consignas como tales. La primera pide verificar que la red entrenada haya aprendido las imágenes. Recordando, si los patrones fueron memorizados, serían atractores, y la red se mantendría en ellos aunque se itere, lo cual constituye una forma simple de verificar el aprendizaje. 

Por un tema de las dimensiones de las imágenes, los tres primeros incisos se realizaron con dos redes neuronales de tres imágenes cada una, para luego unificar los tamaños en el cuarto. 



\subsection{Análisis}

\section{Ejercicio 2}

\subsection{Introducción}

\subsection{Resultados}

\subsection{Análisis}

\section{Conclusiones}

Leí tus notebooks y te dejo una devolución organizada:
Lo que está bien en tu análisis

* **Implementación clara**: programaste la regla de Hebb y la dinámica de actualización de Hopfield de manera correcta y modular.
* **Experimentos variados**: probaste con diferentes números de patrones y neuronas, lo que te permitió mostrar la relación entre capacidad y error.
* **Visualización**: los gráficos permiten ver cómo evoluciona la red y en qué punto empieza a fallar la memoria.
* **Discusión inicial**: mencionás la capacidad límite (aprox. `0.14N`) y observás cómo se degrada el rendimiento al aumentar la carga.

Aspectos en los que podrías ahondar

1. **Profundizar en la teoría**

   * Explicar mejor por qué la capacidad máxima se aproxima a `0.14N` (derivación a partir de resultados de Amit, Gutfreund y Sompolinsky).
   * Diferenciar entre *memorizar patrones* y *recuperarlos con ruido* (estabilidad de atractores vs. basins of attraction).

2. **Dinámica de actualización**

   * Comparar **actualización síncrona vs. asíncrona** y sus consecuencias en la convergencia.
   * Mostrar ejemplos donde la red entra en **ciclos** o estados espurios.

3. **Ruido y robustez**

   * Evaluar qué pasa si los patrones iniciales tienen cierto porcentaje de bits cambiados.
   * Graficar probabilidad de recuperación exitosa vs. nivel de ruido inicial.

4. **Estados espurios**

   * Mencionar y, si podés, mostrar ejemplos de **estados espurios mixtos** (combinaciones lineales de patrones almacenados).
   * Discutir qué implican para la capacidad real de la red.

5. **Extensiones posibles**

   * Comentar variantes como Hopfield continuo (con funciones sigmoides), o usar matrices de pesos con aprendizaje estocástico.
   * Mencionar relación con máquinas de Boltzmann y redes modernas de memoria asociativa.

---

Para tu **documento en LaTeX** te convendría estructurarlo así:

1. Introducción breve (qué es una red de Hopfield y para qué sirve).
2. Regla de aprendizaje (con ecuación).
3. Dinámica y convergencia.
4. Experimentos y resultados (capacidad, ruido, errores).
5. Limitaciones y próximos pasos (espurios, generalización).

¿Querés que te arme un **esqueleto en LaTeX** con estas secciones, listo para que pegues tus resultados?

\end{document}
