% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{enumitem}
\usepackage{amsmath}




%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Trabajo práctico 1\\ Redes Neuronales y Aprendizaje Profundo}
\author{Ignacio Ezequiel Cavicchioli\\Padrón 109428\\icavicchioli@fi.uba.ar}
\date{10/9/2025} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle
\section{Introducción}

Este documento presenta el desarrollo de las consignas del trabajo práctico N°1 de la materia de \textbf{Redes Neuronales y Aprendizaje Profundo}.  El código correspondiente fue realizado en  \textit{Jupyter notebooks}, \textit{Python}, adjuntados a la entrega en formato PDF. Toda imagen o implementación requeridas para el análisis se explicitarán en el presente archivo, por lo que la lectura del código en sí queda a discreción del lector. La teoría relevante será presentada y discutida en la sección pertinente.

\section{Ejercicio 1}

\subsection{Consignas}
Entrene una red de Hopfield ‘82 con las imágenes binarias disponibles en el campus. 
\begin{enumerate}[label=\alph*]
\item Verifique si la red aprendió las imágenes enseñadas.
\item Evalúe la evolución de la red al presentarle versiones alteradas de las imágenes
aprendidas: agregado de ruido, elementos borrados o agregados.
\item Evalúe la existencia de estados espurios en la red: patrones inversos y
combinaciones de un número impar de patrones. (Ver Spurious States, en la sección
2.2, Hertz, Krogh \& Palmer, pág. 24).
\item Realice un entrenamiento con las 6 imágenes disponibles. ¿Es capaz la red de
aprender todas las imágenes? Explique.
\end{enumerate}

\subsection{Desarrollo}

Una red de Hopfield es, en esencia, una memoria direccionable por contenido (\textit{content-adressable memory})\footnote{Neural Networks and Physical Systems with Emergent Collective Computational Abilities, Campus, J. J. Hopfield }; permite obtener un estado memorizado completo a partir de un estado incompleto suficientemente parecido. Como se va a mostrar en los varios incisos, la red de Hopfield no es una memoria ideal, pero bajo ciertas condiciones, el error en la recuperación de estados está acotado. 

La red de Hopfield a tratar es una colección de neuronas de McCulloch-Pitts. En pocas palabras, cada neurona tiene como ``entrada'' los estados de todas las otras neuronas menos sí misma. Estos se ponderan por un peso $w_{i,j}$ calculado por aprendizaje Hebbiano, se suman, conformando una $h$, que se pasa por una función de activación, determinando la ``salida'', o estado futuro, de la neurona. En los ejercicios se trabajó con una función de activación del tipo signo, que devuelve $-1$ o $1$ según el signo de $h$. Para $h = 0$ se determinó que la salida toma valor $1$. 

La red es entrenada por medio del aprendizaje de todos los $w_{i,j}$ según la regla de aprendizaje \eqref{eq:regla_aprendizaje}, que se puede generalizar matricialmente como se ve en \eqref{eq:generalizacion_matricial}. A cada imágen le corresponde una única iteración ($\Delta w_{i,j}$), por lo que solo se suma una vez a la matriz $\mathbf{W}$. En términos más informales, la red solo ``ve'' las imágenes 1 vez.  El peso $\eta$ se mantuvo unitario para todo entrenamiento. 

\begin{equation}
\begin{split}
\Delta w_{i,j} &= \eta x_i x_j \\
w_{{i,j}_n} &= w_{{i,j} _{n-1}} + \Delta w_{i,j}
\end{split}
\label{eq:regla_aprendizaje}
\end{equation}

\begin{equation}
\begin{split}
\Delta\mathbf{W} &= \eta \mathbf{x} \mathbf{x}^T \\
\mathbf{W}_n &= \mathbf{W}_{n-1} + \Delta\mathbf{W}
\end{split}
\label{eq:generalizacion_matricial}
\end{equation}

El proceso de aprendizaje equivale a aprender la correlación entre todas las imágenes. Imágenes muy parecidas (por algún criterio, ej.: distancia de Hamming) van a quedar ``cerca'' en la función energética subyacente. Esto trae a colación un aspecto importante para toda memoria, que es la capacidad. Como se va a demostrar más adelante, la capacidad es función de la cantidad de neuronas y correlación ente estados memorizados. 

Para'' ejecutar'' la red neuronal alcanza con calcular todos los estados siguientes de las neuronas. Esto se puede hacer de dos formas:
\begin{itemize}
\item Actualización sincrónica: se computa el siguiente estado de todas las neuronas a la vez.
\item Actualización asincrónica: las neuronas son aleatoriamente actualizadas, sin seguir un orden particular. La idea es que todas se actualicen al finalizar el ciclo. 
\end{itemize}

Para este trabajo se usó una actualización sincrónica, pero la actualización asincrónica tiene beneficios en lo relativo a convergencia de la red. 

Ahora bien, ¿Cómo se parte de una matriz de pesos y se termina con un sistema que recuerda estados? El libro ``\textit{Introduction To The Theory Of Neural Computation}'' provisto por el Campus de la materia explica que los estados memorizados son atractores en el espacio de todos los estados posibles. Una forma de pensarlo es como que los patrones o estados aprendidos son concavidades en una tela y el estado actual es una canica puesta sobre esta tela. Si se deja que la canica se mueva, va a caer en alguna de estas hendiduras de la tela. Como se va a tratar más adelante, existen otros atractores que no son los patrones aprendidos: los inversos de estos, las combinaciones lineales de un número impar de patrones (estados espurios de mezcla) y otros estados espurios que aparecen como consecuencia de la dinámica de la red.

\begin{center}
\rule{0.5\linewidth}{0.5pt} % línea de mitad de ancho y 0.5pt de grosor
\end{center}

Ahora que se tiene cierta noción sobre las redes de Hopfield se puede pasar al desarrollo de las consignas como tales. La primera pide verificar que la red entrenada haya aprendido las imágenes. Recordando, si los patrones fueron memorizados, serían atractores, y la red se mantendría en ellos aunque se itere, lo cual constituye una forma simple de verificar el aprendizaje. 

Por un tema de las dimensiones de las imágenes, los tres primeros incisos se realizaron con dos redes neuronales de tres imágenes cada una, para luego unificar los tamaños en el cuarto. Las figuras \ref{fig:ej1_a} y \ref{fig:ej1_b} muestran, para cada subfigura, la imagen enseñada (izquierda) y la recordada (derecha) de las dos redes neuronales. Ambas redes devolvieron correctamente las imágenes enseñadas. 

La segunda consigna pide evaluar la propiedad de la red de recordar contenido. La idea es iniciar con una imagen alterada, permitir que la red evolucione y verificar a que estado convergió. Idealmente la red va a poder devolver el estado deseado, que sería el inicial sin las alteraciones. Las figuras \ref{fig:ej2_a} \ref{fig:ej2_b} presentan visualmente los resultados de partir de una imagen alterada. Ambas redes neuronales fueron capaces de converger a los estados memorizados correctos, habiendo partido de los estados alterados. Esto se logró en 2 iteraciones, ya que se requieren dos para verificar la convergencia de la red (ausencia de cambios entre iteraciones).

\newpage 
El tercer ítem de las consignas exige evaluar la existencia de los estados espurios previamente mencionados. Para esto, se eligieron algunos estados espurios representativos que permiten demostrar su presencia. El único tipo de estado que no se pudo evidenciar es el tercer tipo, los verdaderamente espurios. Recordando y profundizando, los estados espurios serían:

\begin{itemize}
\item \textbf{Estados inversos de los memorizados:} si un patrón memorizado es $\mathbf{x}$, entonces $-\mathbf{x}$ también es un atractor de la red, ya que posee la misma energía.
\item \textbf{Combinaciones lineales de un número impar de patrones:} se generan estados de mezcla a partir de un conjunto de patrones memorizados $\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(k)}\}$ con $k$ impar, mediante
\[
\mathbf{s} = \mathrm{sign}\left(\sum_{i=1}^{k} \mathbf{x}^{(i)}\right),
\]
donde la función signo se aplica componente a componente. Estos estados son atractores espurios porque no corresponden a ningún patrón individual entrenado, pero emergen de la dinámica de la red.
\item \textbf{Estados verdaderamente espurios:} atractores que no están relacionados directamente con los patrones memorizados; su aparición es más rara y depende de la red y la configuración de los patrones.
\end{itemize}

La figura \ref{fig:ej1_3_1} muestra, a la izquierda, el estado entregado a la red (el inverso del panda) y, a la derecha, el estado al que convergió en una iteración, indicando que es un punto de estabilidad en la red 1.  

La figura \ref{fig:ej1_3_2} demuestra que el inverso del estado de ``v'' también constituye un atractor estable en la red 1.  

La figura \ref{fig:ej1_3_3} muestra un caso similar, partiendo del inverso del torero editado. La convergencia al estado inverso indica que el torero inverso es un atractor en la red 2.  

La figura \ref{fig:ej1_3_4} presenta como entrada una combinación lineal de los tres patrones usados para entrenar la red 1. La red no cambia de estado al iterar, lo que indica que esta combinación es estable.  

Finalmente, la figura \ref{fig:ej1_3_5} muestra como entrada una combinación lineal de los tres patrones usados para entrenar la red 2, incluyendo la paloma editada. La convergencia al estado de combinación lineal indica que esta combinación también es estable y actúa como atractor.


\clearpage
\begin{figure}[p]
    \centering
    \subfloat[Imagen 1]{\includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_1_1.png}}\\
    \subfloat[Imagen 2]{\includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_1_2.png}}\\
    \subfloat[Imagen 3]{\includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_1_3.png}}
    \caption{Imágenes del inciso 1 - Red 1.}
    \label{fig:ej1_a}
\end{figure}


\begin{figure}[p]
    \centering
    \subfloat[Imagen 4]{\includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_1_4.png}}\\
    \subfloat[Imagen 5]{\includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_1_5.png}}\\
    \subfloat[Imagen 6]{\includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_1_6.png}}
    \caption{Imágenes del inciso 1 - Red 2.}
    \label{fig:ej1_b}
\end{figure}
\clearpage
\newpage

\clearpage
\begin{figure}[p]
    \centering
    \subfloat[Imagen 1]{\includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_2_1.png}}\\
    \subfloat[Imagen 2]{\includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_2_2.png}}\\
    \subfloat[Imagen 3]{\includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_2_3.png}}
    \caption{Imágenes del inciso 2 - Red 1.}
    \label{fig:ej2_a}
\end{figure}


\begin{figure}[p]
    \centering
    \subfloat[Imagen 4]{\includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_2_4.png}}\\
    \subfloat[Imagen 5]{\includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_2_5.png}}\\
    \subfloat[Imagen 6]{\includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_2_6.png}}
    \caption{Imágenes del inciso 2 - Red 2.}
    \label{fig:ej2_b}
\end{figure}
\clearpage
\newpage

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_3_1.png}
    \caption{Figura 1: El estado inverso del panda es estable.}
    \label{fig:ej1_3_1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_3_2.png}
    \caption{Figura 2: El estado inverso del torero editado es un atractor.}
    \label{fig:ej1_3_2}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_3_3.png}
    \caption{Figura 3: La combinación lineal de los 3 estados enseñados a la red 1 es estable.}
    \label{fig:ej1_3_3}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_3_4.png}
    \caption{Figura 4: La combinación lineal con un estado alterado es un atractor.}
    \label{fig:ej1_3_4}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{C:/Users/q/Documents/GitHub/Redes-neuronales/latex tp1/imgs/ej1_3_5.png}
    \caption{Figura 5: descripción breve de la imagen.}
    \label{fig:ej1_3_5}
\end{figure}
\clearpage

\newpage




\clearpage
\newpage

\subsection{Análisis}

\section{Ejercicio 2}

\subsection{Introducción}

\subsection{Resultados}

\subsection{Análisis}

\section{Conclusiones}

Leí tus notebooks y te dejo una devolución organizada:
Lo que está bien en tu análisis

* **Implementación clara**: programaste la regla de Hebb y la dinámica de actualización de Hopfield de manera correcta y modular.
* **Experimentos variados**: probaste con diferentes números de patrones y neuronas, lo que te permitió mostrar la relación entre capacidad y error.
* **Visualización**: los gráficos permiten ver cómo evoluciona la red y en qué punto empieza a fallar la memoria.
* **Discusión inicial**: mencionás la capacidad límite (aprox. `0.14N`) y observás cómo se degrada el rendimiento al aumentar la carga.

Aspectos en los que podrías ahondar

1. **Profundizar en la teoría**

   * Explicar mejor por qué la capacidad máxima se aproxima a `0.14N` (derivación a partir de resultados de Amit, Gutfreund y Sompolinsky).
   * Diferenciar entre *memorizar patrones* y *recuperarlos con ruido* (estabilidad de atractores vs. basins of attraction).

2. **Dinámica de actualización**

   * Comparar **actualización síncrona vs. asíncrona** y sus consecuencias en la convergencia.
   * Mostrar ejemplos donde la red entra en **ciclos** o estados espurios.

3. **Ruido y robustez**

   * Evaluar qué pasa si los patrones iniciales tienen cierto porcentaje de bits cambiados.
   * Graficar probabilidad de recuperación exitosa vs. nivel de ruido inicial.

4. **Estados espurios**

   * Mencionar y, si podés, mostrar ejemplos de **estados espurios mixtos** (combinaciones lineales de patrones almacenados).
   * Discutir qué implican para la capacidad real de la red.

5. **Extensiones posibles**

   * Comentar variantes como Hopfield continuo (con funciones sigmoides), o usar matrices de pesos con aprendizaje estocástico.
   * Mencionar relación con máquinas de Boltzmann y redes modernas de memoria asociativa.

---

Para tu **documento en LaTeX** te convendría estructurarlo así:

1. Introducción breve (qué es una red de Hopfield y para qué sirve).
2. Regla de aprendizaje (con ecuación).
3. Dinámica y convergencia.
4. Experimentos y resultados (capacidad, ruido, errores).
5. Limitaciones y próximos pasos (espurios, generalización).

¿Querés que te arme un **esqueleto en LaTeX** con estas secciones, listo para que pegues tus resultados?

\end{document}
