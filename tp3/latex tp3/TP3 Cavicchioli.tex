% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the ``article'' class.
% See ``book'', ``report'', ``letter'' for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt


\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
% para los cuadritos en links
%\usepackage[linkbordercolor={0 0 1}, citebordercolor={0 1 0}, urlbordercolor={1 0 0}]{hyperref}
\usepackage[colorlinks=true, linkcolor=black, citecolor=green, urlcolor=red]{hyperref} % solo resalta
\usepackage[spanish]{babel}



%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The ``real'' document content comes below...

\title{Trabajo práctico 3\\ Redes Neuronales y Aprendizaje Profundo}
\author{Ignacio Ezequiel Cavicchioli\\Padrón 109428\\icavicchioli@fi.uba.ar}
\date{2/12/2025} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed



\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Introducción}

Este documento presenta el desarrollo de las consignas del trabajo práctico N°3 de la materia de \textbf{Redes Neuronales y Aprendizaje Profundo}.  El código correspondiente fue realizado en  \textit{Jupyter notebooks}, \textit{Python}, adjuntados a la entrega en formato PDF. Toda imagen o implementación requeridas para el análisis se explicitarán en el presente archivo, por lo que la lectura del código en sí queda a discreción del lector. La teoría relevante será presentada y discutida en la sección pertinente.



\section{Teoría sobre redes de Kohonen y SOM}

Las \textit{Self-Organizing Maps} (SOM), o \textit{redes de Kohonen}, son un tipo de red neuronal no supervisada diseñada para proyectar datos de alta dimensionalidad sobre una rejilla de menor dimensión, preservando en lo posible la topología del espacio original. Este proceso permite visualizar, agrupar y extraer estructuras latentes en los datos.

La palabra ``rejilla'' se refiere a la estructura informática/topológica donde viven las neuronas. Esta puede ser:
\begin{itemize}
	\item 1D: una línea (vector de neuronas) que se cierra o no en los extremos.
	\item 2D: una matriz, que se puede o no cerrar en sus lados.
	\item 3D o más: un tensor de orden mayor.
\end{itemize}


El mecanismo de entrenamiento está inspirado en el aprendizaje hebbiano: las neuronas que responden de manera similar a un mismo estímulo se refuerzan conjuntamente. Sin embargo, la SOM incorpora además un esquema de \textit{competitive learning}, donde sólo la neurona ganadora (y sus vecinas en la rejilla) actualizan sus pesos (o lo que corresponda según la función de vecindad). Esto genera la organización progresiva del mapa y produce un \textit{feature mapping} explícito, ordenando los patrones según similitud.

Las SOM actúan como un método de reducción de dimensionalidad no lineal, construyendo representaciones espaciales que capturan regularidades estadísticas de los datos sin requerir etiquetas. Esto mismo se va a ver en la tercer consigna de este trabajo, donde se pasa de un espacio dimensional de N dimensiones al espacio 2D de la rejilla de neuronas, y se usa la matriz U para ratificar la existencia de \textit{clusters}.

\newpage

\section{Ejercicio 1}

\subsection{Consignas}
Construya una red de Kohonen de 2 entradas que aprenda una distribución uniforme dentro del círculo unitario. Mostrar el mapa de preservación de topología. Probar con distribuciones uniformes dentro de otras figuras geométricas.

\subsection{Desarrollo}

Dado lo expuesto en la teoría sobre redes de Kohonen y lo visto en clase, se espera que las redes implementadas y entrenadas puedan aprender las distribuciones uniformes que se les van a dar en este inciso. Visualmente, esto debería notarse como que las neuronas están aproximadamente equidistantes las unas de las otras, siempre dentro de los límites de la distribución.

Ahora bien, para generar muestras aleatorias uniformes en un círculo hay que pensar que se quiere la misma probabilidad de caer en cualquier parte de su área. Como el área de esta figura depende del radio, sortear un $r$ y $\theta$ distribuidos uniformemente entre $(0,1)$ y $(0,360^{\circ})$ respectivamente generaría una distribución conjunta con mayor densidad de muestras en el centro, indicativo de mayor probabilidad.

Para balancear esto, se ajusta la distribución del radio sorteado a $r=R\sqrt{u}$ con $u \sim \mathcal{U}\{0,1\}$.

Con un $r$ y $theta$ correctamente generados, se procede a hacer una conversión a coordenadas cartesianas, que van a usar las neuronas.

Las otras figuras geométricas fueron generadas de forma similar, haciendo uso de cambios de coordenadas para mantener la uniformidad.

Para entrenar la red de Kohonen utilizada en este ejercicio, se implementó una versión vectorizada del algoritmo, que es más rápida que \textit{loopear} por todas las neuronas. La inicialización de los pesos se realizó tomando muestras aleatorias del propio conjunto de datos. Durante el entrenamiento, los pesos se modificaron mediante una función de vecindad gaussiana, de varianza linealmente decreciente con las iteraciones, que debería ayudar con la organización del mapa. La tasa de aprendizaje se mantuvo constante.

Para cada muestra, se determina la neurona ganadora (\textit{Best Matching Unit}, BMU) y se actualizan tanto ella como sus vecinas de acuerdo con la función de vecindad. Esto se hace una cantidad de iteraciones determinada por el usuario. Para este caso, el algoritmo se ejecutó hasta ver un resultado invariante.


las imágenes de a continuación muestran las figuras generadas con las neuronas en sus ubicaciones aprendidas. Las figuras tienen 1000 muestras (puntos de datos), y hay 25 neuronas distribuidas en una matriz de $5x5$, razón por la cual se forma una \textit{lattice} (rejilla) de polígonos de 4 lados; cada neurona se conecta a otras 4.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/circulo con neuronas}
	\caption{Distribución de datos con forma de círculo y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:circulo-con-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/dona con neuronas}
	\caption{Distribución de datos con forma de dona y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:dona-con-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/Ucon neuronas}
	\caption{Distribución de datos con forma de U y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:ucon-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/triangulo con neuronas}
	\caption{Distribución de datos con forma de triángulo y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:triangulo-con-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/cuadradocon neuronas}
	\caption{Distribución de datos con forma de cuadrado y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:cuadradocon-neuronas}
\end{figure}


\clearpage

\subsection{Análisis}

Se observa que la red implementada es capaz de aprender considerablemente bien las distribuciones de las figuras \ref{fig:circulo-con-neuronas}, \ref{fig:triangulo-con-neuronas} y \ref{fig:cuadradocon-neuronas}, que son el círculo, triángulo y cuadrado respectivamente. La posición de las neuronas es aproximadamente equidistante las unas de las otras, respondiendo a la distribución de datos subyacente. También se pudieron generar los mapas de preservación de topología.

En las formas U y dona (fig. \ref{fig:dona-con-neuronas} y \ref{fig:ucon-neuronas}), las neuronas aprendieron la distribución pero hay algunas unidades que quedaron por fuera de las figuras: en la U quedaron 2 neuronas mientras que en la dona, 1. Se considera que este efecto tiene su origen en que esas neuronas nunca ganan, y son movidas por todas las otras neuronas con una ``fuerza'' inversamente proporcional a la distancia. Por esto mismo hay neuronas en el centro de la dona (todas las neuronas dentro de la dona tiran con la misma fuerza por estar a la misma distancia), y en la U tenemos neuronas más arriba o abajo (las neuronas de los laterales tiran hacia si mismas y contrarrestan los efectos de las de la parte de abajo de la U).

En las otras geometrías esto no sucede.

Para entender esto se puede ver la imagen \ref{fig:dona-y-u-con-neuronas-vector}, que tiene algunos de los vectores de fuerza imaginaria en verde. En ella se va a notar con claridad lo enunciado más arriba.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{imgs/ej1/dona y u con neuronas vector}
	\caption{}
	\label{fig:dona-y-u-con-neuronas-vector}
\end{figure}

Otro detalle interesante es que, si se observa con detenimiento, las neuronas están sobre los puntos de mayor densidad de muestras, indicando que:
\begin{enumerate}
	\item El aprendizaje está funcionando porque las neuronas se acercan a donde ganan más, que coincide con las zonas donde hay mayor concentración de muestras.
	\item Las distribuciones generadas no fueron perfectamente uniformes: la presencia de cúmulos locales de mayor densidad hace que algunas neuronas se agrupen en esas regiones.
\end{enumerate}

\clearpage


\section{Ejercicio 2}

\subsection{Consignas}
Resuelva (aproximadamente) el “Traveling salesman problem” para 200 ciudades con una red de Kohonen.

\subsection{Desarrollo}

Para este ejercicio se usó un \textit{dataset} de 312 ciudades estadounidenses, a partir del cual se crearon sets de menos muestras para ir mostrando la resolución del problema del “Traveling salesman problem” de menor a mayor complejidad.

La

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/muestras}
	\caption{Todas las muestras de ciudades de USA del dataset}
	\label{fig:muestras}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/20 ciudades 20 neuronas camino}
	\caption{20 ciudades con 20 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:20-ciudades-20-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/20 ciudades 20 neuronas}
	\caption{20 ciudades con 20 neuronas - puede haber overfitting: neuronas sobrantes o neuronas que representan varias ciudades}
	\label{fig:20-ciudades-20-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/20 ciudades 40 neuronas camino}
	\caption{20 ciudades 40 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:20-ciudades-40-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/20 ciudades 40 neuronas}
	\caption{20 ciudades 40 neuronas}
	\label{fig:20-ciudades-40-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/50 ciudades 100 neuronas camino}
	\caption{50 ciudades 100 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:50-ciudades-100-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/50 ciudades 100 neuronas}
	\caption{50 ciudades 100 neuronas}
	\label{fig:50-ciudades-100-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/100 ciudades 200 neuronas camino}
	\caption{100 ciudades 200 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:100-ciudades-200-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/100 ciudades 200 neuronas}
	\caption{100 ciudades 200 neuronas}
	\label{fig:100-ciudades-200-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/200 ciudades 400 neuronas 2 datos 3 neuronas 1}
	\caption{200 ciudades 400 neuronas - dos ciudades representadas por tres neuronas, una queda en el medio}
	\label{fig:200-ciudades-400-neuronas-2-datos-3-neuronas-1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/200 ciudades 400 neuronas 2 datos 3 neuronas 2}
	\caption{200 ciudades 400 neuronas - se ve la neurona intermedia entre dos ciudades cercanas}
	\label{fig:200-ciudades-400-neuronas-2-datos-3-neuronas-2}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/200 ciudades 400 neuronas camino}
	\caption{200 ciudades 400 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:200-ciudades-400-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/200 ciudades 400 neuronas zoom}
	\caption{200 ciudades 400 neuronas - zoom para más detalle}
	\label{fig:200-ciudades-400-neuronas-zoom}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/200 ciudades 400 neuronas}
	\caption{200 ciudades 400 neuronas}
	\label{fig:200-ciudades-400-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/312 ciudades 624 neuronas camino}
	\caption{312 ciudades 624 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:312-ciudades-624-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/312 ciudades 624 neuronas}
	\caption{312 ciudades 624 neuronas}
	\label{fig:312-ciudades-624-neuronas}
\end{figure}



\subsection{Análisis}

\clearpage

\section{Ejercicio 3}

En el campus encontrará el archivo ``datos\_para\_clustering.mat'' que contiene una matriz de datos de 500 mediciones de una variable de 100 dimensiones.

a) Utilice una red de Kohonen para reducir la dimensionalidad de los datos.


b) Verifique la presencia de clusters, e indique cuantos puede visualizar, haciendo uso de la matriz U.

\subsection{Consignas}

\subsection{Desarrollo}

\subsection{Análisis}



\section{Conclusiones}

Cada consigna tiene su análisis, así que no es necesario en detalle sobre algún ejercicio particular. Considero que el trabajo sirvió como una introducción extremadamente completa los temas de redes neuronales basadas en perceptrones, métodos de optimización y redes convolucionales. A mi entender, las consignas fueron resueltas de forma satisfactoria, ahondando en algunos temas no requeridos pero que hacen al entendimiento de la materia.


\end{document}
