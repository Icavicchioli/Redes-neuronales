% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the ``article'' class.
% See ``book'', ``report'', ``letter'' for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt


\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{siunitx}
% para los cuadritos en links
%\usepackage[linkbordercolor={0 0 1}, citebordercolor={0 1 0}, urlbordercolor={1 0 0}]{hyperref}
\usepackage[colorlinks=true, linkcolor=black, citecolor=green, urlcolor=red]{hyperref} % solo resalta
\usepackage[spanish]{babel}



%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The ``real'' document content comes below...

\title{Trabajo práctico 3\\ Redes Neuronales y Aprendizaje Profundo}
\author{Ignacio Ezequiel Cavicchioli\\Padrón 109428\\icavicchioli@fi.uba.ar}
\date{2/12/2025} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed



\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Introducción}

Este documento presenta el desarrollo de las consignas del trabajo práctico N°3 de la materia de \textbf{Redes Neuronales y Aprendizaje Profundo}.  El código correspondiente fue realizado en  \textit{Jupyter notebooks}, \textit{Python}, adjuntados a la entrega en formato PDF. Toda imagen o implementación requeridas para el análisis se explicitarán en el presente archivo, por lo que la lectura del código en sí queda a discreción del lector. La teoría relevante será presentada y discutida en la sección pertinente.



\section{Teoría sobre redes de Kohonen y SOM}

Las \textit{Self-Organizing Maps} (SOM), o \textit{redes de Kohonen}, son un tipo de red neuronal no supervisada diseñada para proyectar datos de alta dimensionalidad sobre una rejilla de menor dimensión, preservando en lo posible la topología del espacio original. Este proceso permite visualizar, agrupar y extraer estructuras latentes en los datos.

La palabra ``rejilla'' se refiere a la estructura informática/topológica donde viven las neuronas. Esta puede ser:
\begin{itemize}
	\item 1D: una línea (vector de neuronas) que se cierra o no en los extremos.
	\item 2D: una matriz, que se puede o no cerrar en sus lados.
	\item 3D o más: un tensor de orden mayor.
\end{itemize}


El mecanismo de entrenamiento está inspirado en el aprendizaje hebbiano: las neuronas que responden de manera similar a un mismo estímulo se refuerzan conjuntamente, y sus pesos se ``acercan'' al estímulo. Sin embargo, además se incluye un esquema de aprendizaje competitivo, donde sólo la neurona ganadora (y sus vecinas en la rejilla) actualizan sus pesos (o lo que corresponda según la función de vecindad). Esto genera la organización progresiva del mapa, ordenando los patrones según similitud.

También, las SOM actúan como un método de reducción de dimensionalidad no lineal y no supervisado. Esto mismo se va a ver en la tercer consigna de este trabajo, donde se pasa de un espacio dimensional de N dimensiones al espacio 2D de la rejilla de neuronas, y se usa la matriz U para ratificar la existencia de \textit{clusters}.

\newpage

\section{Ejercicio 1}

\subsection{Consignas}
Construya una red de Kohonen de 2 entradas que aprenda una distribución uniforme dentro del círculo unitario. Mostrar el mapa de preservación de topología. Probar con distribuciones uniformes dentro de otras figuras geométricas.

\subsection{Desarrollo}

Dado lo expuesto en la teoría sobre redes de Kohonen y lo visto en clase, se espera que las redes implementadas y entrenadas puedan aprender las distribuciones uniformes que se les van a dar en este inciso. Visualmente, esto debería notarse como que las neuronas están aproximadamente equidistantes las unas de las otras, siempre dentro de los límites de la distribución.

Ahora bien, para generar muestras aleatorias uniformes en un círculo hay que pensar que se quiere la misma probabilidad de caer en cualquier parte de su área. Como el área de esta figura depende del radio, sortear un $r$ y $\theta$ distribuidos uniformemente entre $(0,1)$ y $(0,360^{\circ})$ respectivamente generaría una distribución conjunta con mayor densidad de muestras en el centro, indicativo de mayor probabilidad.

Para balancear esto, se ajusta la distribución del radio sorteado a $r=R\sqrt{u}$ con $u \sim \mathcal{U}\{0,1\}$.

Con un $r$ y $theta$ correctamente generados, se procede a hacer una conversión a coordenadas cartesianas, que van a usar las neuronas.

Las otras figuras geométricas fueron generadas de forma similar, haciendo uso de cambios de coordenadas para mantener la uniformidad.

Para entrenar la red de Kohonen utilizada en este ejercicio, se implementó una versión vectorizada del algoritmo, que es más rápida que \textit{loopear} por todas las neuronas. La inicialización de los pesos se realizó tomando muestras aleatorias del propio conjunto de datos. Durante el entrenamiento, los pesos se modificaron mediante una función de vecindad gaussiana, de varianza linealmente decreciente con las iteraciones, que debería ayudar con la organización del mapa. La tasa de aprendizaje se mantuvo constante.

Para cada muestra, se determina la neurona ganadora (\textit{Best Matching Unit}, BMU) y se actualizan tanto ella como sus vecinas de acuerdo con la función de vecindad. Esto se hace una cantidad de iteraciones determinada por el usuario. Para este caso, el algoritmo se ejecutó hasta ver un resultado invariante.


las imágenes de a continuación muestran las figuras generadas con las neuronas en sus ubicaciones aprendidas. Las figuras tienen 1000 muestras (puntos de datos), y hay 25 neuronas distribuidas en una matriz de $5x5$, razón por la cual se forma una \textit{lattice} (rejilla) de polígonos de 4 lados; cada neurona se conecta a otras 4.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/circulo con neuronas}
	\caption{Distribución de datos con forma de círculo y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:circulo-con-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/dona con neuronas}
	\caption{Distribución de datos con forma de dona y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:dona-con-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/Ucon neuronas}
	\caption{Distribución de datos con forma de U y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:ucon-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/triangulo con neuronas}
	\caption{Distribución de datos con forma de triángulo y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:triangulo-con-neuronas}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej1/cuadradocon neuronas}
	\caption{Distribución de datos con forma de cuadrado y posiciones finales de las neuronas de la red luego del entrenamiento.}
	\label{fig:cuadradocon-neuronas}
\end{figure}


\clearpage

\subsection{Análisis}

Se observa que la red implementada es capaz de aprender considerablemente bien las distribuciones de las figuras \ref{fig:circulo-con-neuronas}, \ref{fig:triangulo-con-neuronas} y \ref{fig:cuadradocon-neuronas}, que son el círculo, triángulo y cuadrado respectivamente. La posición de las neuronas es aproximadamente equidistante las unas de las otras, respondiendo a la distribución de datos subyacente. También se pudieron generar los mapas de preservación de topología.

En las formas U y dona (fig. \ref{fig:dona-con-neuronas} y \ref{fig:ucon-neuronas}), las neuronas aprendieron la distribución pero hay algunas unidades que quedaron por fuera de las figuras: en la U quedaron 2 neuronas mientras que en la dona, 1. Se considera que este efecto tiene su origen en que esas neuronas nunca ganan, y son movidas por todas las otras neuronas con una ``fuerza'' inversamente proporcional a la distancia. Por esto mismo hay neuronas en el centro de la dona (todas las neuronas dentro de la dona tiran con la misma fuerza por estar a la misma distancia), y en la U tenemos neuronas más arriba o abajo (las neuronas de los laterales tiran hacia si mismas y contrarrestan los efectos de las de la parte de abajo de la U).La imagen \ref{fig:dona-y-u-con-neuronas-vector} tiene algunos de los vectores de ``fuerza'' imaginaria en verde. En ella se va a notar con claridad lo enunciado antes.

En las otras geometrías no hay neuronas ``huérfanas'' debido a que no tienen regiones sin datos en las que puedan quedar varadas.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{imgs/ej1/dona y u con neuronas vector}
	\caption{}
	\label{fig:dona-y-u-con-neuronas-vector}
\end{figure}

Otro detalle interesante es que, si se observa con detenimiento cualquiera de las imágenes, las neuronas están sobre los puntos de mayor densidad de muestras, indicando que:
\begin{enumerate}
	\item El aprendizaje está funcionando porque las neuronas se acercan a donde ganan más, que coincide con las zonas donde hay mayor concentración de muestras.
	\item Las distribuciones generadas no fueron perfectamente uniformes: la presencia de cúmulos locales de mayor densidad hace que algunas neuronas se agrupen en esas regiones.
\end{enumerate}

\clearpage


\section{Ejercicio 2}

\subsection{Consignas}
Resuelva (aproximadamente) el “Traveling salesman problem” para 200 ciudades con una red de Kohonen.

\subsection{Desarrollo}

Para este ejercicio se usó un \textit{dataset} de coordenadas (X,Y) de 312 ciudades estadounidenses, a partir del cual se crearon sets de menos muestras para ir mostrando la resolución del problema del “Traveling salesman problem” en orden de menor a mayor complejidad.

Además, se modificó el código de entrenamiento de la red de Kohonen para que la rejilla de neuronas no fuera una grilla 2D sino una línea cuyos extremos están conectados entre sí. Es decir, la topología utilizada fue un anillo.
Esta configuración asegura que, la red final sea un camino cerrado, lo cual es consistente con la solución del problema del vendedor viajante, donde la ruta debe iniciar y finalizar en el mismo punto. Aparte, la naturaleza de como se organiza el mapa según cercanía debería resultar en una solución buena del problema (seguramente no óptima).

La Figura \ref{fig:muestras} muestra la distribución completa de las ciudades del dataset.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/muestras}
	\caption{Todas las muestras de ciudades de USA del dataset}
	\label{fig:muestras}
\end{figure}


Para los mapas de menos ciudades, se usaron 3000 iteraciones con una varianza de 2. Para los mapas más grandes, se usaron 1000 iteraciones y varianza de 8, pero todos con \textit{learning rate} de $0.5$. Este ajuste de parámetros fue empírico, y siempre se buscó la solución que pareciera mejor.

\newpage

Iniciamos con un conjunto de 20 ciudades y una red de 20 neuronas.
La Figura \ref{fig:20-ciudades-20-neuronas-camino} muestra únicamente el camino resultante (sin visualizar neuronas ni muestras).
Luego, en la Figura \ref{fig:20-ciudades-20-neuronas} puede verse una situación típicamente notada cuando la cantidad de neuronas coincide con la cantidad de ciudades: algunas neuronas quedan exactamente sobre las ciudades, pero otras quedan entre ellas, sin representar ninguna muestra en particular. Es análogo a lo observado en la consigna anterior, donde algunas neuronas quedan fuera de la distribución.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/20 ciudades 20 neuronas camino}
	\caption{20 ciudades con 20 neuronas - El camino sin las neuronas o ciudades}
	\label{fig:20-ciudades-20-neuronas-camino}
\end{figure}
\vspace{-0.5cm}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{imgs/ej2/20 ciudades 20 neuronas}
	\caption{20 ciudades con 20 neuronas - Pueden aparecer neuronas que quedan entre ciudades}
	\label{fig:20-ciudades-20-neuronas}
\end{figure}

En mi opinión, no se busca que el modelo aprenda a la perfección la distribución, pero estos casos de generalización (una unidad entre 2 muestras) no son deseables cuando el resultado final deseado es un trayecto que pase por todos los puntos. Para corregir este comportamiento, se aumentó la cantidad de neuronas al doble de ciudades, con la intención de que hayan más unidades, logrando más ajuste a los datos y caminos que si pasan por todos los puntos a visitar.

\clearpage


Ahora, con 40 neuronas para 20 ciudades (Figuras \ref{fig:20-ciudades-40-neuronas-camino} y \ref{fig:20-ciudades-40-neuronas}) se observa que, al haber neuronas sobrantes, siempre hay unidades representando correctamente las muestras y el trayecto final pasa por todos los puntos. Se notó que las unidades sobrantes se acomodan sobre los trayectos rectos del viaje, como se observó en la consigna anterior con la U o dona. No ``entorpecen'' (hasta se podrían remover) y, a priori, no parecerían empeorar la solución.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/20 ciudades 40 neuronas camino}
	\caption{20 ciudades con 40 neuronas - El camino sin las neuronas}
	\label{fig:20-ciudades-40-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/20 ciudades 40 neuronas}
	\caption{20 ciudades con 40 neuronas}
	\label{fig:20-ciudades-40-neuronas}
\end{figure}

\clearpage

Pasando a un conjunto mayor, con 50 ciudades y 100 neuronas (Figuras \ref{fig:50-ciudades-100-neuronas-camino} y \ref{fig:50-ciudades-100-neuronas}), se observa que las neuronas logran ajustarse a la distribución, aún si quedan algunos puntos intermedios que no corresponden a ninguna ciudad (se notó en el visualizador integrado en el \textit{notebook}). El trayecto resultante pasa por todas las ciudades.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/50 ciudades 100 neuronas camino}
	\caption{50 ciudades con 100 neuronas - El camino sin las neuronas o ciudades}
	\label{fig:50-ciudades-100-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/50 ciudades 100 neuronas}
	\caption{50 ciudades con 100 neuronas}
	\label{fig:50-ciudades-100-neuronas}
\end{figure}


\clearpage

Para 100 ciudades con 200 neuronas (Figuras \ref{fig:100-ciudades-200-neuronas-camino} y \ref{fig:100-ciudades-200-neuronas}), la red continúa manteniendo un comportamiento adecuado, ajustándose al contorno general del mapa y generando un camino continuo y ordenado.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/100 ciudades 200 neuronas camino}
	\caption{100 ciudades 200 neuronas - El camino sin las neuronas o ciudades}
	\label{fig:100-ciudades-200-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/100 ciudades 200 neuronas}
	\caption{100 ciudades 200 neuronas}
	\label{fig:100-ciudades-200-neuronas}
\end{figure}


\clearpage

Al aumentar a 200 ciudades y 400 neuronas, los fenómenos de 1 unidad entre 2 ciudades se vuelve más prolifero, como se ve en la figura \ref{fig:200-ciudades-400-neuronas-zoom}, Lo mismo sucede con el fenómeno de la neurona atrapada entre 2 ciudades, como muestran las figuras \ref{fig:200-ciudades-400-neuronas-2-datos-3-neuronas-1} y \ref{fig:200-ciudades-400-neuronas-2-datos-3-neuronas-2}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/200 ciudades 400 neuronas 2 datos 3 neuronas 1}
	\caption{200 ciudades 400 neuronas - Dos ciudades representadas por tres neuronas, una queda en el medio}
	\label{fig:200-ciudades-400-neuronas-2-datos-3-neuronas-1}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/200 ciudades 400 neuronas 2 datos 3 neuronas 2}
	\caption{200 ciudades 400 neuronas - Se observa la neurona intermedia entre dos ciudades cercanas}
	\label{fig:200-ciudades-400-neuronas-2-datos-3-neuronas-2}
\end{figure}

El camino completo para este caso se muestra en las Figuras \ref{fig:200-ciudades-400-neuronas-camino} y \ref{fig:200-ciudades-400-neuronas}. En la imagen \ref{fig:200-ciudades-400-neuronas-zoom} se hace \textit{zoom} al camino, y se ve que claramente la solución no pasa por encima de todas las ciudades, lo que técnicamente invalidaría este camino como solución. Sin embargo, como estas ciudades están muy cerca (razón por la cual sucede este fenómeno de que la neurona queda en el medio), la corrección del trayecto es trivial, solo se debe elegir una de las 2 combinaciones que minimice la distancia.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/200 ciudades 400 neuronas camino}
	\caption{200 ciudades 400 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:200-ciudades-400-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/200 ciudades 400 neuronas zoom}
	\caption{200 ciudades 400 neuronas - zoom para más detalle}
	\label{fig:200-ciudades-400-neuronas-zoom}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\linewidth]{imgs/ej2/200 ciudades 400 neuronas}
	\caption{200 ciudades 400 neuronas}
	\label{fig:200-ciudades-400-neuronas}
\end{figure}

\clearpage

Finalmente, se entrenó el modelo con las 312 ciudades del dataset completo y 624 neuronas.


Las Figuras \ref{fig:312-ciudades-624-neuronas-camino} y \ref{fig:312-ciudades-624-neuronas} muestran el resultado final, donde el mapa neuronal se ajusta al contorno general del país, produciendo una ruta continua que enlaza todas las ciudades.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/312 ciudades 624 neuronas camino}
	\caption{312 ciudades 624 neuronas - el camino sin las neuronas o ciudades}
	\label{fig:312-ciudades-624-neuronas-camino}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej2/312 ciudades 624 neuronas}
	\caption{312 ciudades 624 neuronas}
	\label{fig:312-ciudades-624-neuronas}
\end{figure}

\clearpage

\subsection{Análisis}

Ante todo, se destaca que se pudo resolver el problema del \textit{travelling salesman} de forma aproximada por medio de redes de Kohonen. El \textit{approach} de que las neuronas dupliquen la cantidad de ciudades ayudó a minimizar la aparición de neuronas que quedan ubicadas entre dos ciudades muy cercanas, a costas de tener más unidades innecesariamente ubicadas en el trayecto.

Estos problemas no invalidan la solución porque la corrección del trayecto es trivial; alcanza con elegir el orden que minimiza la distancia. Pero es importante remarcar que, estrictamente, la solución es aproximada, y, si se fuera a usar de verdad, requeriría de un ajuste manual y verificación.

Aunque el método no garantiza optimalidad, los trayectos generados parecen coherentes y de longitud visualmente razonable.

\clearpage

\section{Ejercicio 3}
\subsection{Consignas}
En el campus encontrará el archivo ``datos\_para\_clustering.mat'' que contiene una matriz de datos de 500 mediciones de una variable de 100 dimensiones.

\begin{enumerate}
\item[a)] Utilice una red de Kohonen para reducir la dimensionalidad de los datos.
\item[b)] Verifique la presencia de clusters, e indique cuantos puede visualizar, haciendo uso de la matriz U.
\end{enumerate}

\subsection{Desarrollo}

Para este inciso se usó una red de Kohonen grilla 2D de 60x60 neuronas, cada una con 100 pesos de entrada. La dimensión de entrada es de 100, y la de salida, 2. El proceso de entrenamiento tomó 10 minutos. El tamaño de 60x60 es relevante ya que aumenta el tiempo de procesamiento pero también la resolución de la matriz U, ya que son coincidentes en dimensiones.

La matriz U (Unified Distance Matrix) es una representación visual que permite analizar la topología y calidad del ordenamiento del SOM.

La matriz U muestra:

\begin{itemize}
	\item Zonas donde las neuronas son similares entre sí (distancias pequeñas).
	\item Zonas donde hay cambios bruscos entre neuronas (distancias grandes).
\end{itemize}

La definición formal es

$$
U(i,j) = \frac{1}{|\mathcal{N}_{i,j}|}
\sum_{(k,l)\in \mathcal{N}_{i,j}}
\left\| \mathbf{w}_{i,j} - \mathbf{w}_{k,l} \right\|
$$

Donde $\mathcal{N}_{i,j}$ es el conjunto de neuronas vecinas.  Basándonos en esto, los pasos para calcular la matriz U son:

\begin{enumerate}
	\item Elegir una neurona e identificar sus vecinos en la grilla 2D.
	\item Calcular la distancia euclídea entre el vector de pesos de la neurona y el de cada vecino.
	$$
	d_{(i,j),(k,l)} = \left\| \mathbf{w}_{i,j} - \mathbf{w}_{k,l} \right\|
	$$
	\item Promediar todas las distancias calculadas.
	$$
	U(i,j) =
	\frac{1}{|\mathcal{N}_{i,j}|}
	\sum_{(k,l)\in\mathcal{N}_{i,j}}
	d_{(i,j),(k,l)}
	$$
	\item Repetir el cálculo para todas las neuronas del mapa, generando una matriz 2D del mismo tamaño que el SOM.


Los valores bajos en la matriz indican regiones de neuronas similares (clústeres), mientras que valores elevados indican regiones de neuronas distantes, que serían posibles fronteras de clústeres.
\end{enumerate}


\newpage

Primero, se hizo una exploración de los datos por medio de un PCA de 2 dimensiones, que se puede ver en la figura \ref{fig:pca-datos}


\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\linewidth]{imgs/ej3/PCA datos}
	\caption{PCA de los datos}
	\label{fig:pca-datos}
\end{figure}

Se notan 4 clústeres y algunas muestras en el centro que no parecen pertenecer a ningún grupo en particular. la mezcla de colores en los datos indica que las muestras no siguen un orden en particular.

Hecho esto, se pasó a entrenar la red, obteniéndose una matriz de 60x60 neuronas, cada una con 100 pesos ajustados a estos datos recién mostrados.

Posteriormente, se ejecutó un análisis PCA sobre los pesos entrenados para saber que esperar con la matriz U. La figura \ref{fig:pca-colores} muestra un PCA de 2 dimensiones, donde los colores indican la cercanía de las neuronas en la grilla 2D. La imagen \ref{fig:pca3-colores} muestra un PCA 3D, cuya perspectiva no es muy cómoda pero muestra 4 regiones de mayor densidad de unidades\footnote{Para una mejor exploración recomiendo correr el \textit{notebook}.}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/ej3/PCA colores}
	\caption{PCA de 2 dimensiones}
	\label{fig:pca-colores}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/ej3/PCA3 colores}
	\caption{PCA de 3 dimensiones}
	\label{fig:pca3-colores}
\end{figure}

Ambos PCAs muestran 4 clústeres bien definidos, y se nota que las neuronas de los clústeres están coloreadas de forma similar, indicando que están cerca en la matriz de neuronas. Además, la observación se condice con el PCA de los datos originales. Se espera que la matriz U también muestre 4 clústeres.

\clearpage

La matriz U se calculó con el método indicado al inicio del desarrollo. Las figuras \ref{fig:u} y \ref{fig:u-bilineal} muestran el mismo mapa de calor de la matriz U con y sin una interpolación.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/ej3/U}
	\caption{Matriz U sin interpolación}
	\label{fig:u}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{imgs/ej3/U bilineal}
	\caption{Matriz U con interpolación bilineal}
	\label{fig:u-bilineal}
\end{figure}

Se ve que hay 4 regiones de un color amarillo intenso delimitadas por regiones de un violeta oscuro. Traducido a distancias, esto indica que hay 4 regiones donde la distancia promedio entre unidades es baja (están cerca), y están separadas por regiones donde la distancia promedio es elevada, apuntando a la existencia de fronteras.

Sería incorrecto decir que el \textit{kernel} de los datos existe en un subespacios de 2 dimensiones, pero 2 dimensiones parecerían alcanzar para hacer una caracterización gruesa de los datos.

\subsection{Análisis}

Antes que todo, se destaca que se logró el \textit{clustering} pedido en las consignas, y se validó por PCA, que dió una noción de como se agrupan los datos y pesos.

Se considera que la deficiencia principal del SOM para \textit{clustering} es que tarda mucho, es un algoritmo de alto costo computacional.

Se me ocurre que se podría hacer clasificación de la siguiente forma: Antes que nada, se deben encontrar los clúster como ya se hizo (de forma no supervisada), y luego asignarle una clase (si correspondiese). Luego, se puede hacer lo siguiente:
\begin{enumerate}
	\item Se toma el vector de entrada (la muestra a clasificar).
	\item Se busca la BMU - la neurona de pesos más cercanos a la muestra.
	\item La muestra es asignada el cluster de la BMU.
\end{enumerate}

De cierta manera, estaría operando como un extractor de \textit{features}.

Otro detalle es que el \textit{clustering} por matriz U no se podría hacer directamente con los datos porque no están topológicamente ordenados. Es la red de Kohonen la que aporta el orden necesario para poder aplicar el método usado.

\section{Conclusiones}

Las tres consignas permitieron profundizar en el funcionamiento y las capacidades de los mapas auto-organizados (SOM). A lo largo del trabajo se observó que este tipo de redes resulta especialmente útil en problemas donde la estructura geométrica de los datos es relevante, ya sea para realizar reducción de dimensionalidad, hacer \textit{clustering}, aproximar distribuciones o generar representaciones ordenadas de datos.

El primer ejercicio mostró la capacidad de generalizar de estas redes haciendo que aprendan distribuciones uniformes de varias geometrías. El segundo inciso mostró la capacidad de preservar distancias y auto-ordenamiento del algoritmo. El último punto ahondó en la capacidad de reducir la dimensionalidad de datos de alta dimensión para encontrar las estructuras subyacentes.

Si bien los entrenamientos de las redes más grandes pueden haber tardado mucho tiempo, los resultados indican que este es un método interesante de tener en mente para la resolución de ciertos problemas menos convencionales que requieran de las capacidades ya mencionadas.

Se considera que las consignas fueron resueltas de forma satisfactoria y permitieron obtener una comprensión sólida del uso práctico del SOM en distintos contextos.


\end{document}
